\newcommand{\doctitle}{Bachelor Project in Computer Science}
\newcommand{\docauthor}{Danny Nygård Hansen}

\documentclass[a4paper, 11pt, article, danish, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}
\usepackage[autostyle]{csquotes}

\usepackage[final]{microtype}
\frenchspacing
\raggedbottom

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[largesmallcaps]{kpfonts}
\linespread{1.06}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
\usepackage{inconsolata}

\usepackage{hyperref}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor={\docauthor},
    hidelinks,
}

\usepackage{theorems-changedot}
\usepackage{theorems-references}

\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}
\setlist{
	listparindent=\parindent,
	parsep=0pt,
}
\usepackage{array}

\title{\doctitle}
\author{\docauthor}

\newcommand{\overbar}[3]{\mkern #1mu\overline{\mkern-#1mu#3\mkern-#2mu}\mkern #2mu}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\extreals}{\overbar{1.5}{1.5}{\reals}}
\newcommand{\complex}{\mathbb{C}}


\usepackage{pgffor}

\newcommand{\rvar}[1]{\mathsf{#1}}

\foreach \x in {A,...,Z}{%
    \expandafter\xdef\csname cal\x\endcsname{\noexpand\mathcal{\x}}
    \expandafter\xdef\csname frak\x\endcsname{\noexpand\mathfrak{\x}}
    \expandafter\xdef\csname rand\x\endcsname{\noexpand\rvar{\x}}
}


\usepackage{etoolbox}
\newcommand{\blank}{\mathrel{\;\cdot\;}}
\newcommand{\blankifempty}[1]{\ifstrempty{#1}{\blank}{#1}}
\DeclarePairedDelimiter{\auxdelimlvert}{\lvert}{\rvert}
\DeclarePairedDelimiter{\auxdelimlVert}{\lVert}{\rVert}
\DeclarePairedDelimiterX{\auxdelimanglescomma}[2]{\langle}{\rangle}{#1,#2}
\newcommand{\abs}[1]{\auxdelimlvert{\blankifempty{#1}}}
\newcommand{\norm}[1]{\auxdelimlVert{\blankifempty{#1}}}
\newcommand{\inner}[2]{\auxdelimanglescomma{\blankifempty{#1}}{\blankifempty{#2}}}


\DeclarePairedDelimiter{\auxdelimparen}{(}{)}
\DeclarePairedDelimiterX{\auxdelimparencomma}[2]{(}{)}{#1,#2}
\DeclarePairedDelimiter{\auxdelimbracket}{[}{]}
\DeclarePairedDelimiterX{\auxdelimbracketcomma}[2]{[}{]}{#1,#2}
\newcommand{\powerset}[2][]{\calP\auxdelimparen[#1]{#2}}
\newcommand{\powersetcard}[3][]{\calP_{#2}\auxdelimparen[#1]{#3}}
\newcommand{\powersetfin}[2][]{\powersetcard[#1]{\omega}{#2}}
\newcommand{\borel}[2][]{\calB\auxdelimparen[#1]{#2}}
\newcommand{\meas}[2][]{\calM\auxdelimparen[#1]{#2}}
\newcommand{\measC}[2][]{\calM_\complex\auxdelimparen[#1]{#2}}
\newcommand{\measpos}[2][]{\meas[#1]{#2}^+}
\newcommand{\measbound}[2][]{\calM_b\auxdelimparen[#1]{#2}}
\newcommand{\measboundpos}[2][]{\measbound[#1]{#2}^+}


\newcommand{\extmeas}[2][]{\overbar{4.5}{0.5}{\calM}\auxdelimparen[#1]{#2}}
\newcommand{\extmeaspos}[2][]{\extmeas[#1]{#2}^+}
\newcommand{\simplemeas}[2][]{\calS\!\calM\auxdelimparen[#1]{#2}}
\newcommand{\simplemeaspos}[2][]{\simplemeas[#1]{#2}^+}
\newcommand{\sigmaalg}[2][]{\sigma\auxdelimparen[#1]{#2}}
\newcommand{\deltasys}[2][]{\delta\auxdelimparen[#1]{#2}}

\newcommand{\expval}[2][]{\mathbb{E}\auxdelimbracket[#1]{#2}}
\newcommand{\var}[2][]{\operatorname{Var}\auxdelimbracket[#1]{#2}}
\newcommand{\cov}[3][]{\operatorname{Cov}\auxdelimbracketcomma[#1]{#2}{#3}}


\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\id}{id}
\newcommand{\indicator}[1]{\mathbf{1}_{#1}}

% Lattice operations
\newcommand{\meet}{\land}
\newcommand{\join}{\lor}

\DeclareMathOperator*{\smallbigvee}{\textstyle\bigvee}
\DeclareMathOperator*{\bigjoin}{\mathchoice
    {\smallbigvee}%
    {\bigvee}%
    {\bigvee}%
    {\bigvee}%
}
\DeclareMathOperator*{\smallbigsqcup}{\textstyle\bigsqcup}
\DeclareMathOperator*{\bigdjoin}{\mathchoice
    {\smallbigsqcup}%
    {\bigsqcup}%
    {\bigsqcup}%
    {\bigsqcup}%
}
\DeclareMathOperator*{\smallbigwedge}{\textstyle\bigwedge}
\DeclareMathOperator*{\bigmeet}{\mathchoice
    {\smallbigwedge}%
    {\bigwedge}%
    {\bigwedge}%
    {\bigwedge}%
}



\newcommand*\union\cup
\newcommand*\intersect\cap

\DeclareMathOperator*{\smallbigcup}{\textstyle\bigcup}
\DeclareMathOperator*{\bigunion}{\mathchoice
    {\smallbigcup}%
    {\bigcup}%
    {\bigcup}%
    {\bigcup}%
}
\DeclareMathOperator*{\smallbigcap}{\textstyle\bigcap}
\DeclareMathOperator*{\bigintersect}{\mathchoice
    {\smallbigcap}%
    {\bigcap}%
    {\bigcap}%
    {\bigcap}%
}


\DeclarePairedDelimiterX{\set}[2]{\lbrace}{\rbrace}{#1\;\delimsize\vert\;#2}

\newcommand{\defeq}{\coloneqq}
\renewcommand{\phi}{\varphi}
\newcommand{\iu}{\mathrm{i}\mkern1mu}
\DeclareMathOperator{\e}{\mathrm{e}}

\newcommand{\ball}[3][]{%
    \ifstrempty{#1}%
        {%
            b\auxdelimparencomma{#2}{#3}%
        }{%
            b_{#1}\auxdelimparencomma{#2}{#3}%
        }%
}

\newcommand{\converges}[1]{\xrightarrow[#1]{}}
\DeclareMathOperator{\supp}{supp}
\let\oldvec\vec
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\Tr}[1][]{%
    \ifstrempty{#1}%
        {%
            \operatorname{Tr}%
        }{%
            \operatorname{Tr}_{#1}%
        }%
}


\usepackage{listofitems}
\setsepchar{,}

\makeatletter
\newcommand{\mat@dims}[1]{%
    \readlist*\@dims{#1}%
    \ifnum \@dimslen=1
        \def\@dimsout{\@dims[1]}%
    \else
        \def\@dimsout{\@dims[1], \@dims[2]}%
    \fi
    \@dimsout
}


\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\matGL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\trans}{^{\top}}
\newcommand{\mat}[2]{M_{\mat@dims{#1}}(#2)}

\makeatother

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\clSpan}{\overbar{0.5}{1.5}{span}}

\newcommand\inv{^{\langle-1\rangle}}
\newcommand{\preim}[2][]{^{-1}\auxdelimparen[#1]{#2}}
\newcommand{\image}[2][]{\auxdelimbracket[#1]{#2}}

\newcommand{\dsupp}[2][]{\mathrm{Sp}_d\auxdelimparen[#1]{#2}}

\usepackage{xcolor}
\usepackage{biolinum}
\usepackage{mathpartir}



% Chapter style section-like
\makeatletter
\makechapterstyle{articlestyle}{%
  \chapterstyle{default}
  \setlength{\beforechapskip}{3.5ex \@plus 1ex \@minus .2ex}
  \renewcommand*{\chapterheadstart}{\vspace{\beforechapskip}}
  \setlength{\afterchapskip}{2.3ex \@plus .2ex}
  \renewcommand{\printchaptername}{}
  \renewcommand{\chapternamenum}{}
  \renewcommand{\chaptitlefont}{\normalfont\scshape\Large\bfseries\color{white!20!black}}
  \renewcommand{\chapnumfont}{\chaptitlefont}
  \renewcommand{\printchapternum}{\chapnumfont \thechapter~~\ensuremath{\diamond}~~}%\quad}
  \renewcommand{\afterchapternum}{}}
\makeatother

\chapterstyle{articlestyle}


%   \setsecheadstyle{\normalfont\Large\bfseries\color{white!20!black}}
\setsecheadstyle{\normalfont\large\bfseries}
\setsubsecheadstyle{\normalfont\large\itshape}
%   \setsecnumformat{\csname the#1\endcsname~~\ensuremath{\diamond}~~}

\setsecnumformat{\csname gablin#1\endcsname}
% \newcommand{\gablinsection}{{\thesection~~\ensuremath{\diamond}~~}}
\newcommand{\gablinsection}{{\thesection~~{\normalsize\textbullet}~~}}
\newcommand{\gablinsubsection}{{\thesubsection.~~}}
\newcommand{\gablinparagraph}{{\normalfont\theparagraph}}

% Paragrahs
\maxsecnumdepth{paragraph}
\renewcommand{\theparagraph}{(\alph{paragraph})}
\crefname{paragraph}{paragraph}{paragraph}
\crefformat{paragraph}{#2§#1#3}
\crefformat{chapter}{#2§#1#3}
\crefformat{section}{#2§#1#3}
\makeatletter
\renewcommand{\p@paragraph}{\thesection} % https://tex.stackexchange.com/questions/531572/full-references-to-subordinated-sections-with-cref
\makeatother
\newcommand{\newpar}{\paragraph{}}
\setbeforeparaskip{.5\baselineskip}


\newcommand{\infrule}[1]{{\normalfont\textsc{#1}}}
\newcommand{\step}{\to}
\newcommand{\headstep}{\to_h}
\newcommand{\purestep}{\to_p}

\newcommand{\hole}{-}
\newcommand{\unitval}{()}
\newcommand{\STLC}{\lambda_{\rightarrow}}

% \newcommand{\keyword}[1]{\textbf{\textit{#1}}}
\newcommand{\pto}{\rightharpoonup}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\ran}{\operatorname{ran}}

\newcommand{\textlang}[1]{\textsf{#1}}
\newcommand{\langkw}[1]{\textlang{\color{objlangcolor} #1}}
\newcommand{\Fst}{\operatorname{\langkw{fst}}}
\newcommand{\Snd}{\operatorname{\langkw{snd}}}
\renewcommand{\prod}{\times}

\newcommand{\objlang}[1]{{\normalfont\textsf{\textcolor{objlangcolor}{#1}}}}
\definecolor{objlangcolor}{HTML}{A91616}

\newcommand{\objOp}[1]{\operatorname{\objlang{#1}}}
\newcommand{\objDelim}[1]{\objlang{(}#1\objlang{)}}

\newcommand{\objFst}[1]{\objOp{fst}#1}
\newcommand{\objSnd}[1]{\objOp{snd}#1}
\newcommand{\objInl}[1]{\objOp{inj}_{\objlang{1}}#1}
\newcommand{\objInr}[1]{\objOp{inj}_{\objlang{2}}#1}

\newcommand{\objPair}[2]{\objDelim{#1\mathpunct{\objlang{,}}#2}}
\newcommand{\objUnit}{\objlang{()}}
\newcommand{\objRec}[3]{\objOp{rec}#1\objDelim{#2} \mathrel{\textcolor{objlangcolor}{\ensuremath{\coloneqq}}} #3}
\newcommand{\objApp}[2]{#1\,#2}
\newcommand{\objAss}[2]{#1 \mathrel{\textcolor{objlangcolor}{\ensuremath{\coloneqq}}} #2}

\newcommand{\objMatch}[4]{\objlang{match} \;#1\, \objlang{with}\: \objInl{#2} \mathbin{\textcolor{objlangcolor}{\Rightarrow}} #3 \mathbin{\textcolor{objlangcolor}{\mid}} \objInr{#2} \mathbin{\textcolor{objlangcolor}{\Rightarrow}} #4 \,\objlang{end}} % TODO xcolor now has \mathcolor? Have to update TeXlive?

\newcommand{\objForall}[2]{\objApp{\textcolor{objlangcolor}{\Lambda}}{#2}}

\newcommand{\setVar}{\mathit{Var}}
\newcommand{\setTVar}{\mathit{TVar}}
\newcommand{\setLoc}{\mathit{Loc}}
\newcommand{\setSto}{\mathit{Sto}}
\newcommand{\setExp}{\mathit{Exp}}
\newcommand{\setVal}{\mathit{Val}}
\newcommand{\setType}{\mathit{Type}}
\newcommand{\setECtx}{\mathit{ECtx}}
\newcommand{\setTAnn}{\mathit{TAnn}}
\newcommand{\typeUnit}{{\normalfont\textsf{Unit}}}
\newcommand{\freevar}[1]{\mathit{FV}(#1)}
\newcommand{\freeTvar}[1]{\mathit{FTV}(#1)}
% \newcommand{\powersetcard}[2]{\calP_\omega(#1)}
\newcommand{\pmaps}[3][]{#2 \pto_{#1} #3}
\newcommand{\typeForall}[2]{\forall #1. #2}
\newcommand{\typeExists}[2]{\exists #1. #2}
\newcommand{\typeRef}[1]{{\normalfont\textsf{ref(}#1\textsf{)}}}
\newcommand{\objTapp}[2]{\objApp{#1}{\textcolor{objlangcolor}{\_}}}
\newcommand{\objPack}[1]{\objOp{pack}#1}
\newcommand{\objUnpack}[3]{\objlang{unpack}\,#1\,\objlang{as}\,#2\,\objlang{in}\,#3}
\newcommand{\objRef}[1]{\objOp{ref}#1}
\newcommand{\objLoad}[1]{\objOp{!}#1}


\begin{document}

\maketitle


\chapter{Preliminaries}

If $X$ is a set, then we denote by $\powerset{X}$ the power set of $X$, and for a cardinal $\kappa$ we furthermore write $\powersetcard{\kappa}{X}$ for the collection of subsets of $X$ with cardinality strictly less than $\kappa$. If $A \in \powersetcard{\kappa}{X}$, then we also write $A \subseteq_\kappa X$. We do not distinguish between the ordinal $\omega$ and the cardinal $\aleph_0$, so $\powersetfin{X}$ denotes the set of finite subsets of $X$. If $Y$ is another set, then we denote the set of partial maps from $X$ to $Y$ by $\pmaps{X}{Y}$. For $f \in \pmaps{X}{Y}$ we let $\dom f$ and $\ran f$ denote the domain and range of $f$, respectively. We also write $\pmaps[\kappa]{X}{Y}$ for the partial functions $f \in \pmaps{X}{Y}$ with $\dom f \subseteq_\kappa X$. For $x_0 \in X$ and $y_0 \in Y$ we denote by $f[x_0 \mapsto y_0]$ the partial map given by
%
\begin{equation*}
    f[x_0 \mapsto y_0](x) =
    \begin{cases}
        y_0, & x = x_0, \\
        f(x), & x \in \dom f \setminus \{x_0\}.
    \end{cases}
\end{equation*}
%
Hence $\dom f[x_0 \mapsto y_0] = \dom f \union \{x_0\}$, so that if $f \in \pmaps[\omega]{X}{Y}$, then also $f[x_0 \mapsto y_0] \in \pmaps[\omega]{X}{Y}$.

We let $\naturals$ denote the set of natural numbers, including $0$. The image of a set $A \subseteq X$ under a function $f \colon X \to Y$ is denoted $f\image{A}$.


\chapter{Induction and Inversion}

In this section we study inference rules and the structures that they give rise to, and we prove various theorems for such structures that will be important in the proof of type safety in TODO ref.

We first give an overview of the abstract theory, studying generating functions in complete lattices and in dcpo's. In a complete lattice Knaster--Tarski's fixed-point theorem yields a principle of induction, which when applied to inference rules gives a notion of rule induction. In dcpo's we study continuous functions in the context of Kleene's fixed-point theorem, which gives a finite description of elements in a structure defined by inference rules. We then study derivations of such elements and prove an inversion theorem.

For readers not interested in the technical details, we note that the main results that will be used in the sequel are \cref{thm:rule-induction} and \cref{cor:inversion}.


\section{Abstract theory}

\newpar

Let $(P,\leq)$ be a partially ordered set\footnote{That is, $\leq$ is a reflexive, transitive and anti-symmetric binary relation on $P$. We also call $P$ a \keyword{poset}.}. If $A \subseteq P$, then an element $x \in P$ is called an \keyword{upper bound} of $A$ if $a \leq x$ for all $a \in A$. If there is a least upper bound $x$ (i.e., such that if $y$ is any other upper bound, then $x \leq y$), then $x$ is called the \keyword{join} of $A$. The join of $A$ is clearly unique if it exists (by anti-symmetry), and we denote it by $\bigjoin A$. We similarly define the \keyword{meet} of $A$, denoted $\bigmeet A$, to be the greatest lower bound of $A$, if it exists. If every two-element subset of $P$ has a join and a meet, then $P$ is called a \keyword{lattice}, and we write $x \join y = \bigjoin \{x,y\}$ and $x \meet y = \bigmeet \{x,y\}$. If every subset of $P$ has a join and a meet, then $P$ is a \keyword{complete lattice}.

If $P$ is a partially ordered set, then a subset $D \subseteq P$ is said to be \keyword{directed} if every finite subset of $D$ has an upper bound (but not necessarily a \emph{least} upper bound, i.e. a join). By induction this is equivalent to the property that, for every \emph{pair} of elements $x,y \in D$, there exists a $z \in D$ with $x \leq z$ and $y \leq z$.\footnote{This is the usual definition of directedness. As an example of why directedness in interesting, recall that a union of a collection of subspaces of a vector space is not usually a subspace itself, but it is if the collection is directed (with respect to inclusion). Similarly for subgroups and other algebraic structures, but note that the same does \emph{not} hold for e.g. topologies or $\sigma$-algebras. If we substituted \enquote{countable} for \enquote{finite} in the definition of directness, $\sigma$-algebras would have this property as well, while we for topologies would need \enquote{arbitrary} subsets.} We further note that the image of a directed set under a monotone map is also directed. If $D$ is a directed set whose join exists, we often write $\bigdjoin D \defeq \bigjoin D$ instead. If $\bigdjoin D$ exists for every directed subset $D$ of $P$, then $P$ is called a \keyword{directly complete partial order}, or \keyword{dcpo} for short. If $P$ also has a least element, usually written $\bot$, then $P$ is called a \keyword{dcppo} (the extra \enquote{p} is for \enquote{pointed}). Notice that complete lattices are dcppo's.

Let $F \colon P \to P$ be a monotone\footnote{A map $f \colon P \to Q$ between posets is \keyword{monotone} if $x \leq y$ implies $f(x) \leq f(y)$ for all $x,y \in P$} map. We think of $F$ as a \keyword{generating function}. An element $x \in P$ is said to be \keyword{$F$-closed} if $F(x) \leq x$, \keyword{$F$-consistent} if $x \leq F(x)$, and a \keyword{fixed-point} of $F$ if $F(x) = x$. If $F$ has a least fixed-point, then this is usually denoted $\mu F$. Similarly, the greatest fixed-point, if it exists, is denoted $\nu F$.


\newpar

We begin by studying dcpo's. If $P$ and $Q$ are dcpo's, then a map $f \colon P \to Q$ is \keyword{continuous}\footnote{Also called \keyword{Scott-continuous} after Dana Scott.} if, for every directed $D \subseteq P$, the image $f\image{D}$ is directed and
%
\begin{equation*}
    f \bigl( \bigdjoin D \bigr)
        = \bigdjoin f\image{D}.
\end{equation*}
%
It is easy to show that continuous maps are monotone (notice that if $x \leq y$, then the set $\{x,y\}$ is directed). If conversely $f$ is monotone, then $f\image{D}$ is as mentioned also directed, and the inequality \enquote{$\geq$} always holds.

Now let $P$ be a dcppo and $F \colon P \to P$ a monotone function. We clearly have $\bot \leq F(\bot)$, and since $F$ is monotone we get the chain\footnote{A \keyword{chain} is a totally ordered set.}
%
\begin{equation*}
    \bot
        \leq F(\bot)
        \leq \cdots
        \leq F^n(\bot)
        \leq F^{n+1}(\bot)
        \leq \cdots
\end{equation*}
%
called the \keyword{ascending Kleene chain}. Since it is a chain it is also directed. The main theorem is the following:

\begin{theorem}[Kleene's fixed-point theorem]
    \label{thm:kleene-fixpoint-theorem}
    If $P$ is a dcppo and $F \colon P \to P$ is continuous, then $F$ has a least fixed-point $\mu F$ and
    %
    \begin{equation*}
        \mu F
            = \bigdjoin_{n \in \naturals} F^n(\bot).
    \end{equation*}
\end{theorem}

\begin{proof}
    First notice that, since $F$ is continuous,
    %
    \begin{equation*}
        F \Bigl( \bigdjoin_{n \in \naturals} F^n(\bot) \Bigr)
            = \bigdjoin_{n \in \naturals} F^{n+1}(\bot)
            = \bigdjoin_{n \in \naturals^+} F^n(\bot)
            = \bigdjoin_{n \in \naturals} F^n(\bot),
    \end{equation*}
    %
    where we use that $F^0(\bot) = \bot$. Hence $\bigdjoin_{n \in \naturals} F^n(\bot)$ is indeed a fixed-point of $F$. If $\beta$ is any fixed-point of $F$, then $\bot \leq \beta$, and hence $F^n(\bot) \leq F^n(\beta) = \beta$ since $F$ is monotone. Taking the join on the left-hand side yields $\bigdjoin_{n \in \naturals} F^n(\bot) \leq \beta$ as desired.
\end{proof}


\newpar

Next, let $L$ be a complete lattice, and let $F \colon L \to L$ be a monotone map. Even though $L$ is also a dcppo, if $F$ is not continuous then \cref{thm:kleene-fixpoint-theorem} does not apply. However, $F$ still has fixpoints, as the following theorem shows:

\begin{theorem}[Knaster--Tarski's fixed-point theorem]
    \label{thm:knaster-tarski}
    If $L$ is a complete lattice and $F \colon L \to L$ is monotone, then $F$ has a least and a greatest fixed-point, and these are given by
    %
    \begin{equation*}
        \mu F
            = \bigmeet \set{x \in L}{F(x) \leq x}
        \quad \text{and} \quad
        \nu F
            = \bigjoin \set{x \in L}{x \leq F(x)}.
    \end{equation*}
    %
    In particular, $\mu F$ is the smallest $F$-closed element and $\nu F$ is the greatest $F$-consistent element in $L$.
\end{theorem}

\begin{proof}
    Denote the meet above by $\alpha$. If $x$ is $F$-closed, then $\alpha \leq x$, so $F(\alpha) \leq F(x) \leq x$. Taking the meet of $x$ we get $F(\alpha) \leq \alpha$, so $\alpha$ is closed. It follows that $F(F(\alpha)) \leq F(\alpha)$, so $F(\alpha)$ is also closed, and so $\alpha \leq F(\alpha)$. Hence $\alpha$ is a fixed-point. Since every other fixed-point is in particular closed, $\alpha$ is the least fixed-point.
\end{proof}

\begin{corollarynoproof}[Principle of induction]
    \label{cor:induction-abstract}
    If $y \in L$ is $F$-closed, then $\mu F \leq y$.
\end{corollarynoproof}
%
We dually have a \keyword{principle of coinduction} -- if $y \in L$ is $F$-consistent, then $y \leq \nu F$ -- but we shall not need this in the sequel.


\section{In power sets}

\newpar

Specialising to the case where $L$ is a power set $\powerset{X}$, one way to define a generating function is using inference rules. An \keyword{inference rule} on $X$ is an expression on the form
%
\begin{equation*}
    \inferrule*[left=Rule]{
        x_1 \and x_2 \and \cdots \and x_k
    }{
        y
    }
\end{equation*}
%
where $x_1, \ldots, x_k$ and $y$ are elements of $X$, and $k \in \naturals$.\footnote{We could equivalently view an inference rule as an element of the product $X^k \prod X$.} We allow $k$ to be zero, in which case we call the rule an \keyword{axiom}. We have decorated the expression with the label \enquote{\infrule{Rule}} so that we may refer to it later. Let us call $x_1, \ldots, x_k$ the \keyword{antecedents} of the rule and $y$ the \keyword{consequent}. Given a (possibly infinite) collection of inference rules, we construct a generating function $F \colon \powerset{X} \to \powerset{X}$ by defining $F(A)$ for a subset $A \subseteq X$ as follows: For $y \in X$ we let $y \in F(A)$ if and only if there is an inference rule whose consequent is $y$ and whose antecedents lie in $A$. We say that $F$ is \keyword{represented by} this collection of inference rules.

Clearly $F$ is monotone if it is represented by a collection of inference rules, so it has a least fixed-point $\mu F$ and we get a principle of induction. However, it is useful to restate induction in terms of the inference rules, since we usually have explicit rules in mind when defining $F$. If $\calR$ is a collection of inference rules on $X$, then we say that a subset $A \subseteq X$ is \keyword{$\calR$-closed} if, given any rule $R \in \calR$ with antecedents lying in $A$, the consequent of $R$ also lies in $A$.

\begin{lemma}
    \label{lem:R-closed-F-closed}
    If $F$ is represented by a collection $\calR$ of inference rules, then a subset $A \subseteq X$ is $F$-closed if and only if it is $\calR$-closed.
\end{lemma}

\begin{proof}
    First assume that $A$ is $F$-closed so that $F(A) \subseteq A$, and consider a rule from $\calR$ with antecedents $x_1, \ldots, x_k \in A$ and consequent $y$. Since $F$ is represented by $\calR$, this implies that $y \in F(A) \subseteq A$, so $A$ is $\calR$-closed.

    If $A$ is $\calR$-closed, then let $y \in F(A)$. Hence there is a rule in $\calR$ with consequent $y$ and antecedents $x_1, \ldots, x_k \in A$. But since $A$ is $\calR$-closed, this implies that $y \in A$, and so $F(A) \subseteq A$.
\end{proof}


\begin{theorem}[Principle of rule induction]
    \label{thm:rule-induction}
    If $F$ is represented by $\calR$ and $A \subseteq X$, then to show that $\mu F \subseteq A$, it suffices to show the following condition: For every inference rule $R \in \calR$ with antecedents lying in $A$, the consequent of $R$ also lies in $A$.
\end{theorem}

\begin{proof}
    The condition says precisely that $A$ is $\calR$-closed, which implies that $A$ is $F$-closed by \cref{lem:R-closed-F-closed}. But then \cref{cor:induction-abstract} implies that $\mu F \subseteq A$.
\end{proof}


\newpar

\newcommand{\assum}[1]{\operatorname{asm}#1}
\newcommand{\concl}[1]{\operatorname{con}#1}
\newcommand{\derlen}[1]{l(#1)}


Sometimes it is necessary (or at least useful) to have an alternative characterisation of $\mu F$, one that makes use of the fact that the number of antecedents of an inference rule is finite. This property is captured by $F$ in the following way:

\begin{lemma}
    If $F$ is represented by a collection of inference rules, then $F$ is continuous.
\end{lemma}

\begin{proof}
    It suffices to show that if $\calD \subseteq \powerset{X}$ is directed, then $F(\bigdjoin \calD) \subseteq \bigdjoin F\image{\calD}$, so let $y \in F(\bigdjoin \calD)$. Say that $F$ is represented by a collection $\calR$. Then there is a rule in $\calR$ with antecedents $x_1, \ldots, x_k \in \bigdjoin \calD$ and consequent $y$. Since the (directed) join in $\powerset{X}$ is just union, there are sets $A_1, \ldots, A_k \in \calD$ such that $x_i \in A_i$. And since $\calD$ is directed there is a set $A \in \calD$ with $A_i \subseteq A$, so that $x_i \in A$ for all $i$. But then $y \in F(A) \subseteq \bigdjoin F\image{\calD}$ as desired.
\end{proof}
%
Hence, \cref{thm:kleene-fixpoint-theorem} implies that $\mu F = \bigdjoin_{n \in \naturals} F^n(\emptyset)$.

To make this more concrete, we study how one can use the inference rules to derive that some element of $X$ lies in $\mu F$. If $\calR$ is a collection of inference rules, then a \keyword{derivation} from $\calR$ is a finite sequence $D = (R_1, \ldots, R_n)$ of inference rules from $\calR$. If $y$ is a consequent of some $R_i$, then we say that $y$ is a \keyword{conclusion} of $D$. The consequent of $R_n$ is called the \keyword{final conclusion} of $D$. We say that $x \in X$ is an \keyword{assumption} of $D$ if $x$ is an antecedent of some $R_i$, and if none of the rules $R_1, \ldots, R_{i-1}$ has $x$ as its antecedent. That is, $x$ is an assumption if it is not implied by any of the previous rules in the derivation. The set of conclusion of $D$ is denoted $\concl{D}$, and the set of assumptions of $D$ is denoted $\assum{D}$. If $\assum{D} = \emptyset$, then we say that $D$ is \keyword{closed}.

\begin{remark}
    Let $D = (R_1, \ldots, R_n)$ be a derivation.
    %
    \begin{enumrem}
        \item If $D$ is closed, then $R_1$ must be an axiom.
    
        \item\label{enum:subderivation} For any $i \in \{1, \ldots, n\}$, $D' = (R_1, \ldots, R_i)$ is called a \keyword{subderivation} of $D$. If $i < n$, then $D'$ is called a \keyword{proper subderivation} of $D$ (so $D'$ is a proper subderivation when $D' \neq D$). It is clear that $\assum{D'} \subseteq \assum{D}$ and $\concl{D'} \subseteq \assum{D}$, so $D'$ is closed if $D$ is closed.
    
        \item\label{enum:derivation-antecedent} If $D$ is closed and $x$ is an antecedent of some $R_i$, then $D$ has a proper subderivation $D'$ whose final consequence is $x$: For $x$ must be the consequent of some $R_1, \ldots, R_{i-1}$, say $R_j$, and $(R_1, \ldots, R_j)$ is closed subderivation of $D$ by \subcref{enum:subderivation}.

        \item\label{enum:derivation-composition} If $E = (S_1, \ldots, S_m)$ is another derivation, then
        %
        \begin{equation*}
            D \circ E
                \defeq (R_1, \ldots, R_n, S_1, \ldots, S_m)
        \end{equation*}
        %
        is another derivation. It is clear that
        %
        \begin{equation*}
            \assum{(D \circ E)}
                = \assum{D} \union (\assum{E} \setminus \concl{D}),
        \end{equation*}
        %
        that
        %
        \begin{equation*}
            \concl{(D \circ E)}
                = \concl{D} \union \concl{E},
        \end{equation*}
        %
        and that the final conclusion of $D \circ E$ is the final conclusion of $E$. In particular, if $D$ is closed and $\assum{E} \subseteq \concl{D}$, then $D \circ E$ is also closed.

        \item The \keyword{length} of a derivation $D = (R_1, \ldots, R_n)$ is $n$, and this is denoted $\derlen{D}$. If $D'$ is a subderivation of $D$, then $\derlen{D'} \leq \derlen{D}$ with equality if and only if $D' = D$. If $E$ is another derivation, then $\derlen{D \circ E} = \derlen{D} + \derlen{E}$.
    \end{enumrem}
\end{remark}


\begin{proposition}
    \label{prop:fixpoint-iff-derivation}
    If $F$ is represented by $\calR$ and $y \in X$, then $y \in \mu F$ if and only if $y$ is the final conclusion of some closed derivation from $\calR$.
\end{proposition}

\begin{proof}
    First assume that $y \in \mu F$, and recall that $\mu F = \bigdjoin_{n \in \naturals} F^n(\emptyset) = \bigunion_{n \in \naturals} F^n(\emptyset)$. We prove by induction in $n$ that every element in $F^n(\emptyset)$ is the final conclusion of a closed derivation. The base case $y \in F^0(\emptyset) = \emptyset$ is vacuous, so let $n \in \naturals$ and assume that the claim holds for every element of $F^n(\emptyset)$. If $y \in F^{n+1}(\emptyset) = F(F^n(\emptyset))$, then there is an inference rule $R$ with consequent $y$ and antecedents $x_1, \ldots, x_k$ that lie in $F^n(\emptyset)$. By induction each $x_i$ is the final conclusion of some derivation $D_i$, and by \cref{enum:derivation-composition} the derivation $D_1 \circ \cdots \circ D_k \circ (R)$ is a closed derivation, and $y$ is its final conclusion.

    We prove the converse by (strong) induction on the length of a derivation. If $y$ is the final conclusion of a closed derivation $(R)$ of length $1$, then $R$ must be an axiom. But then $y \in F(\emptyset) \subseteq \mu F$ since $F$ is represented by $\calR$. Next, let $n \in \naturals$ and let $y$ be the final conclusion of a closed derivation $D = (R_1, \ldots, R_{n+1})$. If $x$ is an antecedent of $R_{n+1}$, then by \cref{enum:derivation-antecedent} $D$ must have a proper subderivation $D'$ whose final conclusion is $x$. By \cref{enum:subderivation} $D'$ is also closed, and so $x \in \mu F$ by induction. But then we must have $y \in F(\mu F) = \mu F$ as desired.
\end{proof}
%
It is of course standard to use derivation \emph{trees}, but all that matters is that there is a finite way to obtain elements in $\mu F$. Since it is easier to define and reason about linear derivations -- and since we will not have to do any actual derivations -- we have taken this approach here.\footnote{Compare the role of deductive calculi in first-order logic, where e.g. the compactness theorem can be obtained from the \emph{existence} of a deductive calculus (with finite derivations).} The main consequence we shall use is the following:

\begin{corollary}[Inversion]
    \label{cor:inversion}
    If $F$ is represented by $\calR$ and $y \in \mu F$, then there is a rule $R$ in $\calR$ with $y$ as consequent whose antecedents also lie in $\mu F$.
\end{corollary}

\begin{proof}
    By \cref{prop:fixpoint-iff-derivation} there is a closed derivation $D$ of $y$ from $\calR$, and we denote its last rule by $R$. If $x$ is an antecedent of $R$, then some subderivation of $D$ is a derivation of $x$ by \cref{enum:subderivation}. Another application of \cref{prop:fixpoint-iff-derivation} then implies that $x \in \mu F$.
\end{proof}






\newcommand{\hastype}[5]{%
    \ifstrempty{#1}%
        {%
            \ifstrempty{#2}%
            {%
                #3 \vdash #4 : #5%
            }{%
                #2 \mid #3 \vdash #4 : #5%
            }%
        }{%
            \ifstrempty{#2}%
            {%
                #1 \mid \emptyset \mid #3 \vdash #4 : #5%
            }{%
                #1 \mid #2 \mid #3 \vdash #4 : #5%
            }%
        }%
}

\newcommand{\stotype}[4]{%
    \ifstrempty{#1}%
        {%
            \ifstrempty{#2}%
            {%
                #3 \vdash #4%
            }{%
                #2 \mid #3 \vdash #4%
            }%
        }{%
            \ifstrempty{#2}%
            {%
                #1 \mid \emptyset \mid #3 \vdash #4%
            }{%
                #1 \mid #2 \mid #3 \vdash #4%
            }%
        }%
}



\chapter{General stuff about languages}

We describe the general framework in which we may describe various programming languages, introducing the concepts that will later be defined precisely in the concrete setting of System~F.

\section{Syntax}

We first fix countable sets $\setVar$ of variables and $\setLoc$ of locations. The \keyword{expressions} of the language will be a set $\setExp$ containing both $\setVar$ and $\setLoc$, and we designate some of these expressions to be \keyword{values}, collected in a set $\setVal$. We think of an expression as specifying the state of a program, and values are states in which the computation of the program has finished. All locations will also be values, and these are supposed to model memory addresses. The memory state of the program (i.e. the part of the memory that the program has access to) is modelled by a \keyword{store}\footnote{Sometimes called a \keyword{heap}, but this has nothing to do with the heap \emph{data structure}.}, which is an element of $\pmaps[\omega]{\setLoc}{\setExp}$. We simply write $\setSto$ for this set of partial maps.

For $e \in \setExp$ we define the set $\freevar{e} \subseteq \setVar$ of \keyword{free variables}. There are various ways of binding variables in expressions, and the notion of free variables is supposed to capture the idea that variables can be bound, e.g. by lambda abstraction, and hence also \emph{not} bound. If $\freevar{e} = \emptyset$, then we say that $e$ is \keyword{closed}. Complete programs do not have free variables, or if they do we specify their values before running the program, so we may assume that all programs are closed expressions. We will see the importance of this assumption when we prove a progress theorem for System F in [TODO ref].

When defining the set of expressions of a language, we are strictly speaking defining a concrete syntax for the language. However, while we write expressions as a linear sequence of characters, they are thought of as describing an abstract syntax. But since writing abstract syntax trees quickly becomes impractical, we instead express them using a more convenient linear shorthand. If $e_1$, $e_2$ and $e_3$ are expressions, an expression in the concrete syntax of the language could be $e_1 \oplus e_2 \otimes e_3$, but this might have two different derivations from the grammar and hence correspond to two different abstract syntax trees represented by $(e_1 \oplus e_2) \otimes e_3$ and $e_1 \oplus (e_2 \otimes e_3)$. To disambiguate the expression $e_1 \oplus e_2 \otimes e_3$ we must either introduce precedence and associativity rules (external to the grammar itself), or else rewrite the grammar to be unambiguous. Luckily we do not need to deal with these issues since we will not have to parse programs written in our version of System F. If the need arises, we simply use parentheses to make the structure of the abstract syntax tree clear. [TODO Mogensen?]


\section{Type system}

In order to reason statically about the correctness and safety of a program, we introduce \keyword{types}. Each sufficiently \enquote{well-formed} expression will be assigned a type, and if it is possible to assign a type to an expression, then we say that the expression is \keyword{well-typed}. We specify rules which determine which expressions can be typed based on the types of their subexpressions.

We thus define a set $\setType$ of types. This includes a countable set $\setTVar$ of type variables, any base types (e.g. unit, integer, and boolean types), as well as more complex types that can be constructed recursively from the base types such as function, product, or sum types. It also includes reference types, which are the types of locations. For $\tau \in \setType$ we define the set of \keyword{free type variables} $\freeTvar{\tau} \subseteq \setTVar$ in $\tau$. If $\freeTvar{\tau} = \emptyset$, then we also say that $\tau$ is \keyword{closed}. A pair $(e,\tau)$ of an expression and a type is usually written $e : \tau$. An expression may also have types as subexpressions, for instance if a lambda abstraction has a type annotation on its parameter, though this will not be the case for our version of System F.

If an expression $e$ has free variables, then to assign a type to $e$ it is (usually) necessary to first assign types to the free variables in $e$. This is done using a \keyword{type context}, which is a partial function $\pmaps[\omega]{\setVar}{\setType}$. If $e$ is to be well-typed in a type context $\Gamma$, then we (again usually) require that $\freevar{e} \subseteq \dom \Gamma$. That is, $\Gamma$ must in fact specify the types of the variables that occur free in $e$. Notice that it is possible for an expression to be well-typed in one context but not another. If for instance $e$ is the expression $x + 1$, then the typing rules will probably require the free variable $x$ to have some sort of numeric type in the given type context for $e$ to be well-typed.

Furthermore, if $e$ has a location as a subexpression, then we need to be able to look up the type of the expression stored at this location in order to specify the type of $e$. Hence the type of $e$ can also depend on the store in question. However, it not in general possible to deduce the type of $e$ just by knowing the contents of the store: If $\sigma$ is a store with $l_1,l_2 \in \dom \sigma$, and if $\sigma(l_1)$ references $l_2$ and $\sigma(l_2)$ references $l_1$, then it is impossible to deduce the type of $\sigma(l_1)$. Hence we introduce a \keyword{store typing}, which is an element of $\pmaps[\omega]{\setLoc}{\setType}$ that assigns a type to each location. We of course require that the store typing in question actually contains in its domain all locations referenced in $e$, and we furthermore require that all free variables of $e$ lie in the domain of the current type context.

Since a store $\sigma$ specifies the expressions at each location, and a store typing $\Sigma$ specifies the types of those locations, we of course require $\sigma(l)$ to be of type $\Sigma(l)$. In particular, for $\sigma$ to be well-typed we must have $\dom \sigma \subseteq \dom \Sigma$. [TODO but why equal? Refer to later proofs where we use this]

It may be that the type of $e$ or of the types of the variables in the type context $\Gamma$ or of the expressions in the store typing $\Sigma$ contain type variables. In order to keep track of these we collect these in a (finite) set $\Xi$ and require that the free type variables in $\Gamma$ and $\Sigma$, defined by
%
\begin{equation*}
    \freeTvar{\Gamma}
        = \bigunion_{\tau \in \ran \Gamma} \freeTvar{\tau}
    \quad \text{and} \quad
    \freeTvar{\Sigma}
        = \bigunion_{\tau \in \ran \Sigma} \freeTvar{\tau},
\end{equation*}
%
are contained in $\Xi$. A variable $x$ is called \keyword{fresh} for $\Gamma$ if $x \not\in \dom \Gamma$. The finitude of $\dom \Gamma$ ensures that there always exist fresh variables (recall that there are countably infinitely many variables). If $\Delta$ is another type context such that $\dom \Gamma \intersect \dom \Delta = \emptyset$, then instead of $\Gamma \union \Delta$ we simply write $\Gamma,\Delta$. Furthermore, if $\Delta = \{x_1 : \tau_1, \ldots, x_n : \tau_n\}$ for distinct $x_i$, then we omit the braces and write $\Gamma, x_1 : \tau_1, \ldots, x_n : \tau_n$. If $\Xi$ and $\Phi$ are finite disjoint subsets of $\setTVar$, then we similarly write $\Xi,\Phi$ for $\Xi \union \Phi$, and if $\Phi = \{X_1, \ldots, X_n\}$ for distinct $X_i$, then we also write $\Xi, X_1, \ldots, X_n$.

We are now ready to describe the semantics of the type system precisely. The semantics is captured by a subset of the set
%
\begin{equation*}
    \powersetfin{\setTVar}
        \prod (\pmaps[\omega]{\setVar}{\setType})
        \prod (\pmaps[\omega]{\setLoc}{\setType})
        \prod \setExp
        \prod \setType,
\end{equation*}
%
where an element $(\Xi,\Gamma,\Sigma,e,\tau)$ of this set is written $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and is called a \keyword{type derivation}. If $\Xi = \emptyset$, then we simply denote the above element by $\hastype{}{\Gamma}{\Sigma}{e}{\tau}$ [TODO do we need this?], and we furthermore write $\hastype{}{}{\Sigma}{e}{\tau}$ if also $\Gamma = \emptyset$.

Say that we have somehow settled on a semantics for the type system, i.e. a subset of the above product. We do not often refer to this set explicitly, but let us temporarily denote it $\calT$. Usually $\calT$ is defined recursively, by specifying a series of inference rules. Indeed, these inference rules represent a generating function, and $\calT$ will be the least fixed-point of this function.

If a type derivation $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ lies in $\calT$, then we say that $e$ is \keyword{well-typed} in $\Xi,\Gamma,\Sigma$ with type $\tau$.


\section{Dynamics}

The operational semantics of the language is specified in a small-step style. We describe this semantics in stages, beginning with the \keyword{pure head reductions}. These are evaluations that can be performed (1) on expressions that have no subexpressions that can be evaluated, (2) without reading or modifying the store. More precisely, we specify a binary relation $\purestep$ on $\setExp$, such that $e \purestep e'$ is supposed to mean that $e$ evaluates to or reduces to $e'$.

Going one level up we define the relation $\headstep$ on $\setSto \prod \setExp$ of (not necessarily pure) \keyword{head reductions}. These are reductions that may affect and be affected by the contents of the store. Of course, if $\sigma$ is a store and $e \purestep e'$, then we have $(\sigma,e) \headstep (\sigma,e')$, but we augment the pure head reductions with reductions that e.g. read from or write to the store.

Finally we need a way to evaluate complex expressions. One way of doing this is to specify the evaluation rules for all expressions immediately instead of going through head reductions (this is the approach taken by Pierce TODO). Another is to introduce \keyword{evaluation contexts}, which are (essentially) maps $\setExp \to \setExp$. If $K$ is an evaluation context and $e$ is an expression, then we write $K[e]$ for the value of $K$ at $e$. We then define the final reduction relation $\step$ on $\setSto \prod \setExp$ by letting $(\sigma, K[e]) \step (\sigma', K[e'])$ if $(\sigma,e) \headstep (\sigma',e')$.

One role of evaluation contexts is to specify the evaluation order of complex expressions, e.g. if the evaluation of function applications is call-by-value or call-by-name, or if we evaluate the arguments to functions left-to-right or right-to-left. The possibilities thus depend on the available evaluation contexts.

TODO multiple threads


\chapter{System F}

\newcommand{\derives}{\Rightarrow}
\newcommand{\derivesmany}{\Rightarrow^*}

The following grammar defines the syntax, the sets of values and types, and the evaluation contexts of System F:
%
\begin{alignat*}{2}
    && x &\in \setVar \\
    && l &\in \setLoc \\
    && X &\in \setTVar \\
    & \setExp \quad & e &\Coloneqq \objUnit \mid x \mid l \mid \objPair{e}{e} \mid \cdots \\
    & \setVal \quad & v &\Coloneqq \objUnit \mid l \mid \objPair{v}{v} \mid \objInl{v} \mid \cdots \\
    & \setType \quad & \tau &\Coloneqq \typeUnit \mid X \mid \typeRef{\tau} \mid \tau \prod \tau \mid \cdots \\
    & \setECtx \quad & K &\Coloneqq \hole \mid \objPair{K}{e} \mid \objPair{v}{K} \mid \cdots
\end{alignat*}
%
We first note some difficulties with this and similar definitions. Recall that a \keyword{grammar} is a tuple $G = (V,\Sigma,P,S)$, where $V$ is a finite set of variables or non-terminal symbols, $\Sigma$ is a finite set of terminal symbols that is disjoint from $V$, $P$ is a finite rewriting system\footnote{A \keyword{rewriting system} $P$ on a set $A$ is a binary relation on $A^*$, i.e. a subset of $A^* \prod A^*$. An element of $P$ is called a \keyword{production}. If $(\alpha,\beta) \in P$ and $\gamma,\delta \in A^*$, then we write $\gamma \alpha \delta \derives \gamma \beta \delta$. This defines another binary relation $\derives$ on $A^*$, the reflexive and transitive closure of which is denoted $\derivesmany$.} on $V \union \Sigma$, and $S \in V$ is the start symbol. The language generated by $G$ is the language $L(G) = \set{\alpha \in \Sigma^*}{S \derivesmany \alpha}$. We say that $G$ is \keyword{context-free} if every production in $P$ is on the form $A \derives \alpha$, where $A \in V$.

Notice especially that a grammar must only contain \emph{finitely} many productions, but also that the above \enquote{grammar} defining System F\footnote{We put the word \enquote{grammar} in scare quotes since it is not actually a grammar.} as written has infinitely many productions: For instance, there is a production $(e,x)$ for each of the infinitely many variables $x \in \setVar$. While this poses no problems for the abstract study of System F that we are undertaking, we note that it is (or at least in practice it will be) possible to rewrite the \enquote{grammar} so that it has only finitely many productions. We may for instance define the countably many variables recursively by the grammar with productions
%
\begin{equation*}
    x \Coloneqq \objlang{x} \mid x\objlang{'},
\end{equation*}
%
yielding the countably many variables $\objlang{x}$, $\objlang{x'}$, $\objlang{x''}$, and so on. Including these productions instead of the postulate \enquote{$x \in \setVar$} would solve at least this problem. We can do similarly for locations and type variables.

Next we notice that the \enquote{grammar} has no obvious start symbol. Indeed, we may take any of $e$, $v$, $\tau$ or $K$ to be the start symbol, depending on the type of string we wish to construct.

With these issues dealt with, we now relate grammars and the languages they generate to the results from TODO ref, and in particular to inference rules. In fact, since the \enquote{grammar} defining System F is not a proper grammar, we consider more broadly \keyword{infinite grammars}. An infinite grammar is just a grammar $G = (V,\Sigma,P,S)$, except that we allow $V$, $\Sigma$ and $P$ to be infinite. Given $G$ we construct a collection of inference rules as follows: First add the axiom
%
\begin{equation*}
    \inferrule*{
    }{
        S
    }
\end{equation*}
%
saying that we can always derive the start symbol $S$. Next, for every production $(\alpha,\beta) \in P$ and strings $\gamma,\delta \in (V \union \Sigma)^*$ we add the rule
%
\begin{equation*}
    \inferrule*{
        \gamma\alpha\delta
    }{
        \gamma\beta\delta
    }
\end{equation*}
%
Denote the resulting collection of inference rules $\calR_G$.

\begin{proposition}
    $\mu \calR_G = \set{\alpha \in (V \union \Sigma)^*}{S \derivesmany \alpha}$. In particular, TODO no variables % TODO: Define \mu \calR
\end{proposition}

\begin{proof}
    Denote the set $\set{\alpha \in (V \union \Sigma)^*}{S \derivesmany \alpha}$ by $\tilde{L}(G)$. We prove the inclusion \enquote{$\subseteq$} by rule induction, cf. \cref{thm:rule-induction}.
\end{proof}

TODO if all rules have one antecedent, maybe let that define a relation (axioms don't give an element of the relation) + show about transitive closure?



\begin{lemma}
    $\setVal \subseteq \setExp$.
\end{lemma}

\begin{proof}
    
\end{proof}

Notice that $\setLoc \subseteq \setVal \subseteq \setExp$ as we required in TODO ref. If $K$ is an evaluation context and $e$ is an expression, then we define $K[e]$ recursively by
%
\begin{align*}
    \hole[e] &= e \\
    \objPair{K}{e'}[e] &= \objPair{K[e]}{e'} \\
    \objPair{v}{K}[e] &= \objPair{v}{K[e]} \\
    & etc.
\end{align*}
%
It is easy to prove (by induction in $K$) that $K[e]$ is an expression, so every evaluation context $K$ can indeed be thought of as a map $\setExp \to \setExp$ given by $e \mapsto K[e]$.

If $e$ is an expression, then the set $\freevar{e}$ of free variables in $e$ is defined recursively as follows:
%
\begin{align*}
    \freevar{\objUnit} &= \emptyset \\
    \freevar{x} &= \{x\} \\
    \freevar{\objPair{e_1}{e_2}} &= \freevar{e_1} \union \freevar{e_2} \\
    & etc.
\end{align*}
%
Similarly, if $\tau$ is a type, then we define the set $\freeTvar{\tau}$ of free type variables in $\tau$ as follows:
%
\begin{align*}
    \freeTvar{\typeUnit} &= \emptyset \\
    \freeTvar{X} &= \{X\} \\
    \freeTvar{\tau_1 \prod \tau_2} &= \freeTvar{\tau_1} \union \freeTvar{\tau_2} \\
    & etc.
\end{align*}

We are now in a position to define the typing relation. This is the smallest relation on the set
%
\begin{equation*}
    \powersetfin{\setTVar}
        \prod (\pmaps[\omega]{\setVar}{\setType})
        \prod (\pmaps[\omega]{\setLoc}{\setType})
        \prod \setExp
        \prod \setType,
\end{equation*}
%
satisfying the following inference rules:

\begin{equation*}
    \inferrule*[right=T-var]{
        \freeTvar{\Gamma} \subseteq \Xi
        \and
        \freeTvar{\Sigma} \subseteq \Xi
        \and
        (x : \tau) \in \Gamma
    }{
        \hastype{\Xi}{\Gamma}{\Sigma}{x}{\tau}
    }
\end{equation*}


\begin{equation*}
    \inferrule*[right=T-loc]{
        \freeTvar{\Gamma} \subseteq \Xi
        \and
        \freeTvar{\Sigma} \subseteq \Xi
        \and
        l \in \dom \Sigma
    }{
        \hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\Sigma(l)}}
    }
\end{equation*}


Lemma: If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, then $\freeTvar{\Gamma} \subseteq \Xi$ and $\freevar{e} \subseteq \dom \Gamma$. In particular, if $\Xi = \emptyset$ then $\tau$ is closed, and if $\Gamma = \emptyset$ then $e$ is closed. TODO

TODO all type variables in tau also in Xi? All locations in e also in Sigma?

TODO lemma weakening?


\section{Lemmas}

\begin{lemma}[Inversion]
    \label{lem:inversion}
    Assume that $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$.
    %
    \begin{enumlem}
        \item\label{enum:inversion-variable} If $e = x$ is a variable, then $(x : \tau) \in \Gamma$.
        
        \item\label{enum:inversion-unit} If $e = \objUnit$, then $\tau = \typeUnit$.

        \item\label{enum:inversion-pair} If $e = \objPair{e_1}{e_2}$, then $\tau = \tau_1 \prod \tau_1$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau_2}$.

        \item\label{enum:inversion-fst} If $e = \objFst{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau \prod \tau_2}$.
        
        \item\label{enum:inversion-snd} If $e = \objSnd{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_1 \prod \tau}$.
        
        \item\label{enum:inversion-inl} If $e = \objInl{e'}$, then $\tau = \tau_1 + \tau_2$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_1}$.
        
        \item\label{enum:inversion-inr} If $e = \objInr{e'}$, then $\tau = \tau_1 + \tau_2$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_2}$.

        \item\label{enum:inversion-match} If $e = \objMatch{e_1}{x}{e_2}{e_3}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1 + \tau_2}$ and $\hastype{\Xi}{\Gamma, x : \tau_1}{\Sigma}{e_2}{\tau}$ and $\hastype{\Xi}{\Gamma, x : \tau_2}{\Sigma}{e_3}{\tau}$.

        \item\label{enum:inversion-rec} If $e = \objRec{f}{x}{e'}$, then $\tau = \tau_1 \to \tau_2$ and $\hastype{\Xi}{\Gamma, f : \tau_1 \to \tau_2, x : \tau_1}{\Sigma}{e'}{\tau_2}$.

        \item\label{enum:inversion-app} If $e = \objApp{e_1}{e_2}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1 \to \tau}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau}$.

        \item\label{enum:inversion-forall} If $e = \objForall{X}{e'}$, then $\tau = \typeForall{X}{\tau'}$ and $\hastype{\Xi,X}{\Gamma}{\Sigma}{e'}{\tau'}$.
        
        \item\label{enum:inversion-tapp} If $e = \objTapp{e'}{X}$, then $\tau = \tau'[\tau''/X]$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\typeForall{X}{\tau'}}$.

        \item\label{enum:inversion-pack} If $e = \objPack{e'}$, then $\tau = \typeExists{X}{\tau'}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau'[\tau''/X]}$.

        \item\label{enum:inversion-unpack} If $e = \objUnpack{e_1}{x}{e_2}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\typeExists{X}{\tau'}}$ and $\hastype{\Xi,X}{\Gamma, x \colon \tau'}{\Sigma}{e_2}{\tau}$.

        \item\label{enum:inversion-fold} fold TODO

        \item\label{enum:inversion-unfold} unfold TODO

        \item\label{enum:inversion-location} If $e = l$ is a location, then $l \in \dom \Sigma$ and $\tau = \typeRef{\Sigma(l)}$.

        \item\label{enum:inversion-ref} If $e = \objRef{e'}$, then $\tau = \typeRef{\tau'}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau'}$.

        \item\label{enum:inversion-ass} If $e = \objAss{e_1}{e_2}$, then $\tau = \typeUnit$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\typeRef{\tau'}}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau'}$.

        \item\label{enum:inversion-load} If $e = \objLoad{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\typeRef{\tau}}$.
    \end{enumlem}
\end{lemma}

\begin{proof}
    Notice that since the conclusions of different inference rules are distinct, there is a unique rule that was applied last in the derivation of $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ [TODO ref what that means]. This means that if $e$ has any of the above forms, then it must have the type as assigned by the relevant inference rule, and the assumptions of that rule must also hold.

    For instance, if there exist expressions $e_1$ and $e_2$ such that $e = \objPair{e_1}{e_2}$, then the last rule applied must be \infrule{T-pair}. But then $\tau$ must be on the form $\tau_1 \prod \tau_2$ for types $\tau_1$ and $\tau_2$. And furthermore, the assumptions must also hold, implying that $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau_2}$. The other cases are proved in the same way.
\end{proof}

Notice the significance of the inversion lemma: While an expression can generally have many different types (if nothing else then due to substitution into type variables), it seems natural to believe that e.g. pairs cannot be of function type. The inversion lemma says precisely this, that if a pair has any type, then that type must be a product type. Note that the lemma does \emph{not} just say that a well-typed pair is of product type, it rather says that a well-typed pair is \emph{only} of product type.


\begin{lemma}[Canonical forms]
    \label{lem:canonical}
    Assume that $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau}$ where $v$ is a value.
    %
    \begin{enumlem}
        \item\label{enum:canonical-unit} If $\tau = \typeUnit$, then $v = \objUnit$.

        \item\label{enum:canonical-product} If $\tau = \tau_1 \prod \tau_2$, then $v = (v_1,v_2)$.
        
        \item\label{enum:canonical-sum} If $\tau = \tau_1 + \tau_2$, then either $v = \objInl{v'}$ or $v = \objInr{v'}$.

        \item\label{enum:canonical-function} If $\tau = \tau_1 \to \tau_2$, then $v = \objRec{f}{x}{e}$.

        \item\label{enum:canonical-forall} If $\tau = \typeForall{X}{\tau'}$, then $v = \objForall{X}{e}$.

        \item\label{enum:canonical-exists} If $\tau = \typeExists{X}{\tau'}$, then $v = \objPack{e}$.

        \item\label{enum:canonical-recursive} TODO fold

        \item\label{enum:canonical-ref} If $\tau = \typeRef{\tau'}$, then $v$ is a location.
    \end{enumlem}
\end{lemma}

\begin{proof}
    We assume that $\tau = \tau_1 \prod \tau_2$ for concreteness; the other cases are identical. In this case we simply check for each production of the grammar with non-terminal $v$ whether the relevant value can have type $\tau_1 \prod \tau_2$. For instance, \cref{enum:inversion-inl} implies that a value $\objInl{v'}$ can only be of sum type, and hence $v$ cannot be of this form. The only possibility is that $v$ is in fact a pair.
\end{proof}


--- TODO substitution lemma


\begin{lemma}
    If $\hastype{\Xi,X}{\Gamma}{\Sigma}{e}{\tau}$, then $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{e}{\tau[\tau'/X]}$.
\end{lemma}

\begin{proof}
\begin{proofsec*}
    \item[\infrule{T-var}]
    Assume that $\hastype{\Xi,X}{\Gamma}{\Sigma}{x}{\tau}$. By \cref{enum:inversion-variable} we thus have $(x : \tau) \in \Gamma$, so $(x : \tau[\tau'/X]) \in \Gamma[\tau'/X]$ by definition of substitution. But then \infrule{T-var} implies that $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{x}{\tau[\tau'/X]}$ (where we use that $X$ does not occur in the context or type, so it doesn't need to appear in $\Xi$).

    \item[\infrule{T-rec}]
    Assume that $\hastype{\Xi,X}{\Gamma}{\Sigma}{\objRec{f}{x}{e}}{\tau_1 \to \tau_2}$. By the inversion lemma [TODO ref -- also can't we just do induction??] we have $\hastype{\Xi,X}{\Gamma, f : \tau_1 \to \tau_2, x : \tau_1}{\Sigma}{e}{\tau_2}$, so by induction it follows that $\hastype{\Xi}{\Gamma[\tau'/X], f : \tau_1[\tau'/X] \to \tau_2[\tau'/X], x : \tau_1[\tau'/X]}{\Sigma}{e}{\tau_2[\tau'/X]}$ [TODO by substitution on envs, function types etc.]. Applying \infrule{T-rec} we obtain the desired claim.
\end{proofsec*}

TODO rest

% Antag Ξ, X | Γ ⊢ rec f(x) := e : τ₁ → τ₂ og vis Ξ | Γ[τ'/X] ⊢ rec f(x) := e : (τ₁ → τ₂)[τ'\X]
% Per inversion i antagelsen ved vi, at Ξ, X | Γ, f : τ₁ → τ₂, x : τ₁ ⊢ e : τ₂. Fra induktionshypotesen konkludrer vi så Ξ | Γ[τ'\X], f : τ₁[τ'\X] → τ₂[τ'\X], x : τ₁[τ'\X] ⊢ e : τ₂[τ'\X] idet (τ₁ → τ₂)[τ'\X] = τ₁[τ'\X] → τ₂[τ'\X] og da definitionen af substitution på miljøjer giver os, at (Γ, x : τ)[τ'\X] = Γ[τ'\X], x : τ[τ'\X]. Dernæst følger udsagnet, som vi vil vise direkte af T-rec.

% De interessante tilfælde er (surprise, surprise) derimod for T-Tlam og T-Ttapp.

% T-Tlam 
% Antag Ξ, X | Γ ⊢ Λ e : ∀ Y . τ og vis at Ξ | Γ[τ'\X] ⊢ Λ e : (∀ Y . τ)[τ'/X]
% Vær spids på, at jeg her udnytter, at vi ræsonnerer modulo "alpha-ækvivalens", dvs. at vi kan omnavngive (type)variable frit. Det gør jeg ved at gøre brug en frisk variabel Y og ræsonere med en implicit antagelse om at X ≠ Y.
% Per inversion i antagelsen får vi, at Ξ, X, Y | Γ ⊢ e : τ. Husk, at typemiljøer er mængder, dvs Ξ, X, Y = Ξ, Y, X, og dermed gælder (Ξ, Y), X | Γ ⊢ e : τ også. Brug nu induktionshypotesen med (Ξ, Y) som miljø, og vi får Ξ, Y | Γ[τ'\X] ⊢ e : τ[τ'\X]. Vi kan nu konkludere vha. T-Tlam og da (∀ Y . τ)[τ'/X] = ∀ Y . τ[τ'\X].

% T-Ttapp
% Antag Ξ, X | Γ ⊢ e _ : τ[τ''\Y] og vis at Ξ | Γ[τ'\X] ⊢ e _ : τ[τ''\Y][τ'\X]. Igen, vær opmærksom på, at jeg er meget påpasselig med variabelnavnene!
% Per inversion i antagelsen får vi, at Ξ, X | Γ ⊢ e : ∀ Y . τ. Fra induktionhypotesen følger at Ξ | Γ[τ'\X] ⊢ e : (∀ Y . τ)[τ'\X]. Vi bruger nu igen, at (∀ Y . τ)[τ'/X] = ∀ Y . τ[τ'\X] og konkludrerer, at Ξ | Γ[τ'\X] ⊢ e : ∀ Y . τ[τ'\X]. Fra T-Tapp følger nu, at Ξ | Γ[τ'\X] ⊢ e _ : τ[τ'\X][τ''\Y]. Vi er færdige antaget τ[τ'\X][τ''\Y] = τ[τ'\Y][τ''\X], hvilket er tilfældet da X ≠ Y.

% Som I kan se er det altså forholdvist ligetil at vise, givet et par egenskaber omkring substitution, som afhænger lidt af, hvordan vi helt præcist formaliserer denne (som diskuteret i TAPL). Men det er helt fint at ræsonnere, som jeg gør ovenfor.

% For at opresumere er planen for næste gang dermed:
% - Færdiggør preservation for System F (vha typesubstitutionslemmaet ovenfor)
% - Vis progress + preservation for System F + rekursive typer (dette burde være rimeligt ligetil vha. ovenstående)
% - Kig på referencer og kom så langt I kan med progress + preservation for denne udvidelse.
\end{proof}


\section{Progress}

\begin{theorem}[Progress]
    If $\hastype{}{}{\Sigma}{e}{\tau}$, then either $e$ is a value or else, for any store $\sigma$ with $\stotype{}{}{\Sigma}{\sigma}$, there exists an expression $e'$ and a store $\sigma'$ such that $(\sigma,e) \step (\sigma',e')$.
\end{theorem}

\begin{proof}
The proof is by induction on the typing relation $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, but the claim to be proved is augmented by \textquote{or either $\Xi$ or $\Gamma$ is non-empty}.\footnote{This ensures that we can perform the induction on the entire $4$-ary relation, which is important since this relation is the one that is defined by the inference rules, \emph{not} the corresponding binary relation obtained by restricting the ternary relation to the subset where $\Xi$ and $\Gamma$ are empty.} Notice that the induction step for each inference rule is trivial if $\Xi$ or $\Gamma$ is non-empty, so we need only prove each case when $\Xi$ and $\Gamma$ are empty. Furthermore, since the store is relevant for only a few reductions, we suppress it from the notation in most of the cases below, simply taking about expressions reducing to other expressions.
%
\begin{proofsec}
    \item[\infrule{T-unit}]
    Since $\objUnit$ is a value, this follows.

    \item[\infrule{T-var}]
    Since we may assume that $\Gamma$ is empty, this implication is vacuously true.\footnote{In the formalism of TODO ref \infrule{T-var} would not be an inference rule, it would instead be an axiom requiring the relation to include all quadruples $(\Xi, \Gamma, e, \tau)$ such that $(e : \tau) \in \Gamma$ and all type variables in $\tau$ occur in $\Xi$. This is of course also vacuously true when $\Gamma$ is empty.}

    \item[\infrule{T-pair}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1}$ and $\hastype{}{}{\Sigma}{e_2}{\tau_2}$. If both $e_1$ and $e_2$ are values, then $\objPair{e_1}{e_2}$ is also a value, so assume that only $e_1 = v_1$ is a value and that $e_2 \step e_2'$. Since the only rule that generates instances of the one-step relation $\to$ is \infrule{head-step-step}, it follows that $e_2$ is on the form $K[d_2]$ and $e_2'$ is on the form $K[d_2']$, where $K$ is an evaluation context and $d_2$ and $d_2'$ are subexpressions of $e_2$ and $e_2'$ respectively such that $d_2 \headstep d_2'$. Letting $K' = \objPair{v_1}{K}$ it follows that $\objPair{v_1}{e_2} = K'[d_2]$ and $\objPair{v_1}{e_2'} = K'[d_2']$, and so
    %
    \begin{equation*}
        \objPair{v_1}{e_2}
            = K'[d_2]
            \step K'[d_2']
            = \objPair{v_1}{e_2'}
    \end{equation*}
    %
    by \infrule{head-step-step}. If instead $e_1$ is not a value, then the same argument (using the evaluation context $\objPair{K}{e_2}$) yields the same result.

    \item[\infrule{T-fst}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau_1 \prod \tau_2}$. If $e$ is a value, then it is on the form $\objPair{v_1}{v_2}$ by \cref{enum:canonical-product}, where $v_1$ and $v_2$ are values. Hence $\objFst{e} = \objFst{\objPair{v_1}{v_2}}$, and this reduces via a head-step to $v_1$. Choosing the evaluation context $K = \hole$, \infrule{head-step-step} implies that $\objFst{\objPair{v_1}{v_2}} \step v_1$. If instead $e$ is not a value, then by induction there is some $e'$ such that $e \to e'$. Hence there are subexpressions $d$ and $d'$ and an evaluation context $K$ such that $e = K[d]$, $e' = K[d']$ and $d \headstep d'$. Letting $K' = \objFst{K}$ we have
    %
    \begin{equation*}
        \objFst{e}
            = K'[d]
            \step K'[d']
            = \objFst{e'},
    \end{equation*}
    %
    as desired.

    \item[\infrule{T-snd}]
    Similar to \infrule{T-fst}.

    \item[\infrule{T-inj1}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau_1}$. If $e$ is a value $v$, then so is $\objInl{v}$. If instead $e \step e'$, then as before $e = K[d]$, $e' = K[d']$ and $d \headstep d'$. Letting $K' = \objInl{K}$ we get $\objInl{e} = K'[d]$ and $\objInl{e'} = K'[d']$, so $\objInl{e} \step \objInl{e'}$.

    \item[\infrule{T-inj2}]
    Similar to \infrule{T-inj1}.

    \item[\infrule{T-match}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1 + \tau_2}$. If $e_1$ is a value, then by \cref{enum:canonical-sum} it must be on the form $\objInl{v}$ or $\objInr{v}$ for a value $v$. Hence the expression $\objMatch{e_1}{x}{e_2}{e_3}$ can reduce via a head step by either \infrule{E-match-inj1} or \infrule{E-match-inj2}, so it reduces by \infrule{head-step-step} (using the evaluation context $K = \hole$). If instead $e_1 \step e_1'$, then by the same argument as in previous cases with $K' = \objMatch{K}{x}{e_2}{e_3}$, it follows that $\objMatch{e_1}{x}{e_2}{e_3}$ reduces.

    \item[\infrule{T-rec}]
    This is obvious since $\objRec{f}{x}{e}$ is a value.

    \item[\infrule{T-app}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1 \to \tau_2}$ and $\hastype{}{}{\Sigma}{e_2}{\tau_1}$. If $e_1$ is a value, then by \cref{enum:canonical-function} it must be on the form $\objRec{f}{x}{e}$. If also $e_2$ is a value, then the claim follows by \infrule{E-rec-app}. If $e_1 = v$ is a value but $e_2$ is not, then $e_2 \to e_2'$. The same argument as in previous cases with $K' = \objApp{v}{K}$ shows that $\objApp{v}{e_2}$ reduces. Finally, if $e_1$ is not a value, then $e_1 \to e_1'$, and choosing $K' = \objApp{K}{e_2}$ proves the claim.

    \item[\infrule{T-Tlam}]
    This is obvious since $\objForall{X}{e}$ is a value.

    \item[\infrule{T-Tapp}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\typeForall{X}{\tau}}$. If $e$ is a value, then by \cref{enum:canonical-forall} it must be on the form $\objForall{X}{e'}$, so the claim follows from \infrule{E-tapp-tlam} (via \infrule{head-step-step} using the evaluation context $K = \hole$). If $e$ is not a value, then $e \to e'$ for some expression $e'$ by induction. These expressions then have subexpressions $d$ and $d'$ respectively such that $d \headstep d'$, and such that $e = K[d]$ and $e' = K[d']$ for some evaluation context $K$. Letting $K' = \objTapp{K}{\tau'}$ we thus have $\objTapp{e}{\tau'} = K'[d]$ and $\objTapp{e'}{\tau'} = K'[d']$, proving the claim.

    \item[\infrule{T-pack}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau[\tau'/X]}$. If $e$ is a value, then so is $\objPack{e}$. Otherwise $e \step e'$ for some expression $e'$. The same argument as before using the evaluation context $\objPack{K}$ for an appropriate $K$ yields the claim.

    \item[\infrule{T-unpack}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\typeExists{X}{\tau}}$. If $e_1$ is a value, then it must be on the form $\objPack{v}$ by \cref{enum:canonical-exists}, so an application of \infrule{E-unpack-pack} yields the claim. Otherwise $e \step e'$ for some $e'$, and we use the evaluation context $\objUnpack{K}{x}{e_2}$ for an appropriate $K$.

    \item[\infrule{T-fold}]
    TODO

    \item[\infrule{T-unfold}]
    TODO

    \item[\infrule{T-loc}]
    Locations are values, so this is obvious.

    \item[\infrule{T-alloc}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau}$. If $e$ is a value, then the claim follows by applying \infrule{E-alloc}, noting that there always exists a location $l \not\in \dom \sigma$. Otherwise there is an expression $e'$ and a store $\sigma'$ such that $(\sigma,e) \step (\sigma',e')$. But then there is some evaluation context $K$ and subexpressions $d$ and $d'$ of $e$ and $e'$ respectively, such that $e = K[d]$, $e' = K[d']$ and $(\sigma,d) \headstep (\sigma',d')$. Letting $K' = \objRef{K}$ we have $\objRef{e} = K'[d]$ and $\objRef{e'} = K'[d']$, so \infrule{head-step-step} implies that $(\sigma,\objRef{e}) \step (\sigma',\objRef{e'})$.

    \item[\infrule{T-store}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\typeRef{\tau}}$ and $\hastype{}{}{\Sigma}{e_2}{\tau}$, and let $\sigma$ be a store with $\stotype{}{}{\Sigma}{\sigma}$. If $e_1$ is a value, then by \cref{enum:canonical-ref} it is a location $l$, and by \cref{enum:inversion-location} we have $l \in \dom \Sigma = \dom \sigma$. If $e_2$ is also a value, then the claim follows from \infrule{E-store}. If $e_1 = l$ is a value but $e_2$ is not, then there is some $e_2'$ and $\sigma'$ such that $(\sigma,e_2) \step (\sigma',e_2')$. Again writing $e_2 = K[d_2]$ and $e_2' = K[d_2']$ with $(\sigma,d) \headstep (\sigma',d')$, we use the evaluation context $\objAss{l}{K}$. Finally, if $e_1$ is not a value, then the same argument using instead $\objAss{K}{e_2}$ yields the claim.

    \item[\infrule{T-load}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\typeRef{\tau}}$, and let $\sigma$ be a store with $\stotype{}{}{\Sigma}{\sigma}$. If $e$ is a value, then as before it is a location $l$, and $l \in \dom \sigma$. It then follows from \infrule{E-load} that $(\sigma, \objLoad{l}) \headstep (\sigma,v)$, where $v = \sigma(l)$. If instead $(\sigma,e) \step (\sigma',e')$ for an expression $e'$ and a store $\sigma'$, then we simply use the evaluation context $\objLoad{K}$ for an appropriate $K$.
\end{proofsec}
\end{proof}


\section{Preservation}

If $\Xi \subseteq_\omega \setTVar$, $\Gamma$ is a type context and $\Sigma$ is a store typing, then we say that a store $\sigma$ is \keyword{well-typed} with respect to $\Xi$, $\Gamma$ and $\Sigma$ if $\dom \sigma = \dom \Sigma$ and $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$ for all $l \in \dom \sigma$. In this case we write $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$, and if $\Xi$ and $\Gamma$ are both empty we simply write $\stotype{}{}{\Sigma}{\sigma}$.

\begin{theorem}[Preservation]
    If
    %
    \begin{equation*}
        \hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau},
        \quad
        \stotype{\Xi}{\Gamma}{\Sigma}{\sigma}
        \quad \text{and} \quad
        (\sigma,e) \step (\sigma',e'),
    \end{equation*}
    %
    then there exists some store typing $\Sigma'$ with $\Sigma \subseteq \Sigma'$ such that
    %
    \begin{equation*}
        \hastype{\Xi}{\Gamma}{\Sigma'}{e'}{\tau}
        \quad \text{and} \quad
        \stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}.
    \end{equation*}
\end{theorem}

\begin{proof}
    By definition of the one-step relation, there exist an evaluation context $K$ and subexpressions $d$ of $e$ and $d'$ of $e'$ such that $e = K[d]$, $e' = K[d']$, and $(\sigma,d) \headstep (\sigma',d')$. By \cref{lem:subexp-well-typed} there is some type $\rho$ such that $\hastype{\Xi}{\Gamma}{\Sigma}{d}{\rho}$. Next it follows from \cref{lem:preservation-head-steps} that $\hastype{\Xi}{\Gamma}{\Sigma'}{d'}{\rho}$ for some store typing $\Sigma'$ with $\Sigma \subseteq \Sigma'$ and $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$. By \cref{lem:store-typing-weakening} we also have $\hastype{\Xi}{\Gamma}{\Sigma'}{d}{\rho}$, so it follows from \cref{lem:evaluation-contexts-respect-types} that $\hastype{\Xi}{\Gamma}{\Sigma'}{K[d']}{\tau}$ as desired.
\end{proof}


\begin{lemma}
    \label{lem:subexp-well-typed}
    If $K$ is an evaluation context, $e$ is an expression and $\hastype{\Xi}{\Gamma}{\Sigma}{K[e]}{\tau}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.
\end{lemma}

\begin{proof}
Every evaluation context is obtained from the hole \enquote{$\hole$} by finitely many applications of the productions in the grammar [TODO prove this?]. We prove the claim by induction on the length of such a sequence of productions. If $K = \hole$, then the claim is obvious, since then $K[e] = e$. Hence we assume that $K$ is obtained from some evaluation context $K'$ by some application of a production, so that the induction hypothesis holds for $K'$.
%
\begin{proofsec}
    \item[$K = \objPair{K'}{e'}$]
    Then $K[e] = \objPair{K'[e]}{e'}$, and since this is well-typed with type $\tau$, \cref{enum:inversion-pair} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1}$ for some type $\tau_1$. By induction applied to $K'$ we have $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.

    \item[$K = \objPair{v}{K'}$]
    Similar to the above.

    \item[$K = \objFst{K'}$]
    Then $K[e] = \objFst{K'[e]$}, so \cref{enum:inversion-fst} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau \prod \tau_2}$ for some $\tau_2$. By induction we have $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.

    \item[$K \in \{\objSnd{K'}, \objInl{K'}, \objInr{K'}\}$]
    Similar to the above.

    \item[$K = \objMatch{K'}{x}{e_1}{e_2}$]
    Here \cref{enum:inversion-match} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1 + \tau_2}$, so the claim follows by induction.

    \item[$K = \objApp{K'}{e'}$]
    Then $K[e] = \objApp{K'[e]}{e'}$, so \cref{enum:inversion-app} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1 \to \tau}$ for some type $\tau_1$. The claim follows by induction as before.

    \item[$K = \objApp{v}{K'}$]
    Similar to the above.

    \item[$K = \objTapp{K'}{X}$]
    \Cref{enum:inversion-tapp} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1}$ for some type $\tau_1$, so the claim follows by induction.

    \item[$K = $]
    TODO

    \item[$K = \objRef{K'}$]
    Then $K[e] = \objRef{K'[e]}$, so the inversion lemma implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau'}$. The claim follows by induction.

    \item[$K = \objAss{K'}{e'}$]
    Then $K[e] = \objAss{K'[e]}{e'}$, so the inversion lemma implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau'}$ ... TODO
\end{proofsec}

TODO rest -- but they are all the same, so maybe just do one?
\end{proof}


\begin{lemma}[Weakening]
    \label{lem:store-typing-weakening}
    If $\Sigma$ and $\Sigma'$ are store typings with $\Sigma \subseteq \Sigma'$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, then $\hastype{\Xi}{\Gamma}{\Sigma'}{e}{\tau}$.
\end{lemma}

\begin{proof}
    This is a straightforward induction on type derivations, in that we notice that in all inference rules, the store typing is the same in the conclusion as it is in the hypotheses. Furthermore, if the axiom \infrule{T-loc} holds for $\Sigma$, then it clearly holds for $\Sigma'$.
\end{proof}


\begin{lemma}
    \label{lem:evaluation-contexts-respect-types}
    If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau}$ for the same type $\tau$, then $\hastype{\Xi}{\Gamma}{\Sigma}{K[e]}{\rho}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{K[e']}{\rho}$ for the same type $\rho$.
\end{lemma}

\begin{proof}
The proof is by induction on $K$. If $K = \hole$, then this is obvious.
%
\begin{proofsec}
    \item[$K = \objPair{K'}{e''}$]
    Then $K'[e]$ and $K'[e']$ have the same type, so by \infrule{T-pair}, so do $K[e]$ and $K[e']$.

    \item[$K = \objTapp{K'}{\tau'}$]
    Then $K'[e]$ and $K'[e']$ have the same type by induction, and so do $K[e] = \objTapp{K'[e]}{\tau'}$ and $K[e'] = \objTapp{K'[e']}{\tau'}$ by \infrule{T-Tapp}.\footnote{TODO since we just apply it to underscore, it actually has a lot of different types. But we can find one type that works for both.} [TODO need lemma saying that $\tau[\tau'/X]$ is a type!]
\end{proofsec}

TODO rest
\end{proof}


\begin{lemma}[Preservation for head-steps]
    \label{lem:preservation-head-steps}
    If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$ and $(\sigma,e) \headstep (\sigma',e')$, then there exists a store typing $\Sigma'$ such that $\Sigma \subseteq \Sigma'$, $\hastype{\Xi}{\Gamma}{\Sigma'}{e'}{\tau}$, and $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$.
\end{lemma}

\begin{proof}
We simply check all cases. [TODO mention pure cases]
%
\begin{proofsec}
    \item[\infrule{E-fst}]
    In this case $e = \objFst{\objPair{v_1}{v_2}}$ and $e' = v_1$ for values $v_1,v_2$. Then $e$ is a value, so [TODO canonical forms] implies first that $\hastype{\Xi}{\Gamma}{\Sigma}{\objPair{v_1}{v_2}}{\tau \prod \tau'}$ for some type $\tau'$, and then that $\hastype{\Xi}{\Gamma}{\Sigma}{v_1}{\tau}$.

    \item[\infrule{E-tapp-tlam}]
    Write the type of $\objTapp{\objForall{X}{e}}{\tau'}$ as $\tau[\tau'/X]$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{\objForall{X}{e}}{\typeForall{X}{\tau}}$, and again by inversion this implies that $\hastype{\Xi,X}{\Gamma}{\Sigma}{e}{\tau}$. But then it follows from [TODO lemma 0] that $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{e}{\tau[\tau'/X]}$, and since $\Gamma$ does not contain $X$ (since $\Xi$ does not) we have $\Gamma[\tau'/X] = \Gamma$, so the claim follows.

    \item[\infrule{E-alloc}]
    In this case $e = \objRef{v}$, $e' = l$, $\sigma' = \sigma[l \mapsto v]$, and $l \not\in \dom \sigma$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{\objRef{v}}{\typeRef{\tau'}}$ for some $\tau'$, and we further have $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau'}$. Now letting $\Sigma' = \Sigma[l \mapsto \tau']$, it follows from \infrule{T-loc} that $\hastype{\Xi}{\Gamma}{\Sigma'}{l}{\typeRef{\Sigma'(l)}}$, so we have both $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$ and $\hastype{\Xi}{\Gamma}{\Sigma'}{l}{\typeRef{\tau'}}$.

    \item[\infrule{E-store}]
    Here $e = \objAss{l}{v}$, $e' = \objUnit$ and $\sigma' = \sigma[l \mapsto v]$ with $l \in \dom \sigma$. Notice first that $\objAss{l}{v}$ and $\objUnit$ both have type $\typeUnit$ by inversion, so that $\hastype{\Xi}{\Gamma}{\Sigma}{\objUnit}{\typeUnit}$ as required.

    By inversion we also have $\hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\tau'}}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau'}$ for some type $\tau'$, and another application of inversion (TODO via \infrule{T-loc}, or canonical forms?) implies that $\tau' = \Sigma(l)$. Furthermore, since $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$, we have $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$. Since $v = \sigma'(l)$ it thus follows that $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma'(l)}{\Sigma(l)}$, so $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma'}$.

    \item[\infrule{E-load}]
    Here $e = \objLoad{l}$, $e' = v$ and $\sigma = \sigma'$ with $\sigma(l) = v$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\tau}}$, so $\tau = \Sigma(l)$ [TODO again]. But since $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$ we have $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$, or in other words, $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau}$.
\end{proofsec}

TODO rest
\end{proof}


\chapter{Misc}

\section{Lambda calculus}

The syntax of the untyped lambda calculus consists only of variables, abstractions and applications:
%
%
\begin{alignat*}{2}
    && x &\in \setVar \\
    & \setExp \quad & e &\Coloneqq x \mid \lambda x.e \mid e\,e \\
    & \setVal \quad & v &\Coloneqq \lambda x.e
\end{alignat*}
%
This means that there in particular are expressions on the form $e_1\,e_2\,e_3$, and we use the convention that application is left-associative, i.e. that the above expression is to be read $(e_1\,e_2)\,e_3$. [TODO build into the grammar, Mogensen]

The small-step reduction relation $\step$ on expressions formalises how to reduce expressions. For instance, the rule
%
\begin{equation*}
    \inferrule*[right=E-app-abs]{ }{
        (\lambda x . e_1) \, e_2 \step e_1[x \mapsto e_2]
    }
\end{equation*}
%
says that we can apply abstractions to other expressions. Such a rule is called a \keyword{computation rule}, and an expression $(\lambda x . e_1) \, e_2$ is called a \keyword{redex}. Rewriting a redex according to the above rule is called \keyword{$\beta$-reduction}.

A more complex expression might not itself be a redex but instead have a redex as a subexpression. In this case we need other rules, so-called \keyword{congruence rules}, which tell us how to reduce complex expressions by reducing subexpressions. For instance, in the expression $e_1\,e_2$, do we evaluate $e_1$ before $e_2$ or vice versa? That is, does evaluation happen left-to-right or right-to-left, or do we allow this to be determined arbitrarily? This is of course especially important in languages with side-effects. Formally we impose an evaluation order by having either (or both) of the congruence rules
%
\begin{equation*}
    \inferrule*[right=E-app1 \quad {\normalfont and} \quad]{
        e_1 \step e_1'
    }{
        e_1 \, e_2 \step e_1' \, e_2
    }
    % \quad \text{and} \quad
    \inferrule*[right=E-app2.]{
        e_2 \step e_2'
    }{
        e_1 \, e_2 \step e_1 \, e_2'
    }
\end{equation*}
%
If we desire right-to-left evaluation order, then we choose \infrule{E-app2}, but we also need a restricted form of \infrule{E-app1}, namely
%
\begin{equation*}
    \inferrule*[right=E-app1'.]{
        e \step e'
    }{
        e \, v \step e' \, v
    }
\end{equation*}
%
That is, only when the right expression has been reduced to a value $v$ can we evaluate the left expression.

We may also formalise the evaluation order by defining the reduction relation in terms of head reductions $\headstep$, and using evaluation contexts to impose an evaluation order. For instance,
%
\begin{equation*}
    K \Coloneqq \hole \mid K \, e \mid v \, K
    \quad \text{and} \quad
    K \Coloneqq \hole \mid e \, K \mid K \, v
\end{equation*}
%
define evaluation contexts for left-to-right and right-to-left evaluation, respectively. The symbol \enquote{$\hole$} is called the \keyword{hole}, and we think of the hole as the place into which we substitute the expression $e$ when writing $K[e]$.

Computation and congruence rules together might also allow for different evaluation strategies, for instance:
%
\begin{enumerate}
    \item Full $\beta$-reduction: We may reduce \emph{any} redex contained in an expression.

    \item Normal order reduction: We must reduce the leftmost, outermost redex first.

    \item Call-by-name: The subexpression $e_2$ of a redex $(\lambda x . e_1) \, e_2$ cannot be reduced. Instead, we must perform $\beta$-reduction without reducing $e_2$.

    \item Call-by-value: Instead, $e_2$ \emph{must} be reduced to a value before $\beta$-reduction can take place.
\end{enumerate}
%
For instance, in call-by-value we might have a restricted form of the rule \infrule{E-app-abs}, namely
%
\begin{equation*}
    \inferrule*[right=E-app-abs'.]{ }{
        (\lambda x . e) \, v \step e[x \mapsto v]
    }
\end{equation*}
%
That is, the argument must be a value $v$ for the reduction to take place. If we use evaluation contexts, this computation rule would be a rule concerning head reductions $\headstep$.


Since the untyped lambda calculus does not allow abstractions to be named, it is not obvious how to define recursive functions. TODO


\section{Recursion}

Call by name: $Y = \lambda f . (\lambda x . f (xx)) (\lambda x . f (xx))$

\begin{align*}
    Y g
        &= \lambda f . (\lambda x . f (xx)) (\lambda x . f (xx)) g \\
        &\step (\lambda x . g (xx)) (\lambda x . g (xx)) \\
        &\step g ((\lambda x . g (xx))(\lambda x . g (xx)))
\end{align*}
%
On the other hand we also have
%
\begin{align*}
    g (Y g)
        &= g (\lambda f . (\lambda x . f (xx)) (\lambda x . f (xx)) g) \\
        &\step g ((\lambda x . g (xx))(\lambda x . g (xx))).
\end{align*}
%
That is, $Y g$ and $g (Y g)$ reduce to the same expression.

Call by value: $Z = \lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y))$

\begin{align*}
    Z g
        &= \lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y)) g \\
        &\step (\lambda x . g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)) \\
        &\step g (\lambda y. (\lambda x. g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)) y) \\
        &\step g (  )
\end{align*}

\begin{align*}
    g (Z g)
        &= g (\lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y)) g) \\
        &\step g ((\lambda x . g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)))
\end{align*}


\end{document}