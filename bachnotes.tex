\newcommand{\doctitle}{Bachelor Project in Computer Science}
\newcommand{\docauthor}{Danny Nygård Hansen}

\documentclass[a4paper, 11pt, article, danish, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}
\usepackage[autostyle]{csquotes}

\usepackage[final]{microtype}
\frenchspacing
\raggedbottom

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[largesmallcaps]{kpfonts}
\linespread{1.06}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
\usepackage{inconsolata}

\usepackage{hyperref}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor={\docauthor},
    hidelinks,
}

\usepackage{theorems-changedot}
\usepackage{theorems-references}


% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}
\DeclareNumChars*{+} % https://tex.stackexchange.com/questions/16904/xiv-pages-in-page-counts


\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}
\setlist{
	listparindent=\parindent,
	parsep=0pt,
}
\usepackage{array}

\title{\doctitle}
\author{\docauthor}

\newcommand{\overbar}[3]{\mkern #1mu\overline{\mkern-#1mu#3\mkern-#2mu}\mkern #2mu}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\extreals}{\overbar{1.5}{1.5}{\reals}}
\newcommand{\complex}{\mathbb{C}}

\usepackage{bm}

\usepackage{pgffor}

\newcommand{\rvar}[1]{\mathsf{#1}}

\foreach \x in {A,...,Z}{%
    \expandafter\xdef\csname cal\x\endcsname{\noexpand\mathcal{\x}}
    \expandafter\xdef\csname frak\x\endcsname{\noexpand\mathfrak{\x}}
    \expandafter\xdef\csname rand\x\endcsname{\noexpand\rvar{\x}}
}


\usepackage{etoolbox}
\newcommand{\blank}{\mathrel{\;\cdot\;}}
\newcommand{\blankifempty}[1]{\ifstrempty{#1}{\blank}{#1}}
\DeclarePairedDelimiter{\auxdelimlvert}{\lvert}{\rvert}
\DeclarePairedDelimiter{\auxdelimlVert}{\lVert}{\rVert}
\DeclarePairedDelimiterX{\auxdelimanglescomma}[2]{\langle}{\rangle}{#1,#2}
\newcommand{\abs}[1]{\auxdelimlvert{\blankifempty{#1}}}
\newcommand{\norm}[1]{\auxdelimlVert{\blankifempty{#1}}}
\newcommand{\inner}[2]{\auxdelimanglescomma{\blankifempty{#1}}{\blankifempty{#2}}}


\DeclarePairedDelimiter{\auxdelimparen}{(}{)}
\DeclarePairedDelimiterX{\auxdelimparencomma}[2]{(}{)}{#1,#2}
\DeclarePairedDelimiter{\auxdelimbracket}{[}{]}
\DeclarePairedDelimiterX{\auxdelimbracketcomma}[2]{[}{]}{#1,#2}
\newcommand{\powerset}[2][]{\calP\auxdelimparen[#1]{#2}}
\newcommand{\powersetcard}[3][]{\calP_{#2}\auxdelimparen[#1]{#3}}
\newcommand{\powersetfin}[2][]{\powersetcard[#1]{\omega}{#2}}
\newcommand{\borel}[2][]{\calB\auxdelimparen[#1]{#2}}
\newcommand{\meas}[2][]{\calM\auxdelimparen[#1]{#2}}
\newcommand{\measC}[2][]{\calM_\complex\auxdelimparen[#1]{#2}}
\newcommand{\measpos}[2][]{\meas[#1]{#2}^+}
\newcommand{\measbound}[2][]{\calM_b\auxdelimparen[#1]{#2}}
\newcommand{\measboundpos}[2][]{\measbound[#1]{#2}^+}


\newcommand{\extmeas}[2][]{\overbar{4.5}{0.5}{\calM}\auxdelimparen[#1]{#2}}
\newcommand{\extmeaspos}[2][]{\extmeas[#1]{#2}^+}
\newcommand{\simplemeas}[2][]{\calS\!\calM\auxdelimparen[#1]{#2}}
\newcommand{\simplemeaspos}[2][]{\simplemeas[#1]{#2}^+}
\newcommand{\sigmaalg}[2][]{\sigma\auxdelimparen[#1]{#2}}
\newcommand{\deltasys}[2][]{\delta\auxdelimparen[#1]{#2}}

\newcommand{\expval}[2][]{\mathbb{E}\auxdelimbracket[#1]{#2}}
\newcommand{\var}[2][]{\operatorname{Var}\auxdelimbracket[#1]{#2}}
\newcommand{\cov}[3][]{\operatorname{Cov}\auxdelimbracketcomma[#1]{#2}{#3}}


\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\id}{id}
\newcommand{\indicator}[1]{\mathbf{1}_{#1}}

% Lattice operations
\newcommand{\meet}{\land}
\newcommand{\join}{\lor}

\DeclareMathOperator*{\smallbigvee}{\textstyle\bigvee}
\DeclareMathOperator*{\bigjoin}{\mathchoice
    {\smallbigvee}%
    {\bigvee}%
    {\bigvee}%
    {\bigvee}%
}
\DeclareMathOperator*{\smallbigsqcup}{\textstyle\bigsqcup}
\DeclareMathOperator*{\bigdjoin}{\mathchoice
    {\smallbigsqcup}%
    {\bigsqcup}%
    {\bigsqcup}%
    {\bigsqcup}%
}
\DeclareMathOperator*{\smallbigwedge}{\textstyle\bigwedge}
\DeclareMathOperator*{\bigmeet}{\mathchoice
    {\smallbigwedge}%
    {\bigwedge}%
    {\bigwedge}%
    {\bigwedge}%
}



\newcommand*\union\cup
\newcommand*\intersect\cap

\DeclareMathOperator*{\smallbigcup}{\textstyle\bigcup}
\DeclareMathOperator*{\bigunion}{\mathchoice
    {\smallbigcup}%
    {\bigcup}%
    {\bigcup}%
    {\bigcup}%
}
\DeclareMathOperator*{\smallbigcap}{\textstyle\bigcap}
\DeclareMathOperator*{\bigintersect}{\mathchoice
    {\smallbigcap}%
    {\bigcap}%
    {\bigcap}%
    {\bigcap}%
}


\DeclarePairedDelimiterX{\set}[2]{\lbrace}{\rbrace}{#1\;\delimsize\vert\;#2}

\newcommand{\defeq}{\coloneqq}
\renewcommand{\phi}{\varphi}
\newcommand{\iu}{\mathrm{i}\mkern1mu}
\DeclareMathOperator{\e}{\mathrm{e}}

\newcommand{\ball}[3][]{%
    \ifstrempty{#1}%
        {%
            b\auxdelimparencomma{#2}{#3}%
        }{%
            b_{#1}\auxdelimparencomma{#2}{#3}%
        }%
}

\newcommand{\converges}[1]{\xrightarrow[#1]{}}
\DeclareMathOperator{\supp}{supp}
% \let\oldvec\vec
% \renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\Tr}[1][]{%
    \ifstrempty{#1}%
        {%
            \operatorname{Tr}%
        }{%
            \operatorname{Tr}_{#1}%
        }%
}


\usepackage{listofitems}
\setsepchar{,}

\makeatletter
\newcommand{\mat@dims}[1]{%
    \readlist*\@dims{#1}%
    \ifnum \@dimslen=1
        \def\@dimsout{\@dims[1]}%
    \else
        \def\@dimsout{\@dims[1], \@dims[2]}%
    \fi
    \@dimsout
}


\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\matGL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\trans}{^{\top}}
\newcommand{\mat}[2]{M_{\mat@dims{#1}}(#2)}

\makeatother

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\clSpan}{\overbar{0.5}{1.5}{span}}

\newcommand\inv{^{-1}}
\newcommand{\preim}[2][]{^{-1}\auxdelimbracket[#1]{#2}}
\newcommand{\image}[2][]{\auxdelimbracket[#1]{#2}}

\newcommand{\dsupp}[2][]{\mathrm{Sp}_d\auxdelimparen[#1]{#2}}

\usepackage{xcolor}
\usepackage{biolinum}
\usepackage{mathpartir}



% Chapter style section-like
\makeatletter
\makechapterstyle{articlestyle}{%
  \chapterstyle{default}
  \setlength{\beforechapskip}{3.5ex \@plus 1ex \@minus .2ex}
  \renewcommand*{\chapterheadstart}{\vspace{\beforechapskip}}
  \setlength{\afterchapskip}{2.3ex \@plus .2ex}
  \renewcommand{\printchaptername}{}
  \renewcommand{\chapternamenum}{}
  \renewcommand{\chaptitlefont}{\normalfont\scshape\Large\bfseries\color{white!20!black}}
  \renewcommand{\chapnumfont}{\chaptitlefont}
  \renewcommand{\printchapternum}{\chapnumfont \thechapter~~\ensuremath{\diamond}~~}%\quad}
  \renewcommand{\afterchapternum}{}}
\makeatother

\chapterstyle{articlestyle}


%   \setsecheadstyle{\normalfont\Large\bfseries\color{white!20!black}}
\setsecheadstyle{\normalfont\large\bfseries}
\setsubsecheadstyle{\normalfont\large\itshape}
%   \setsecnumformat{\csname the#1\endcsname~~\ensuremath{\diamond}~~}

\setsecnumformat{\csname gablin#1\endcsname}
% \newcommand{\gablinsection}{{\thesection~~\ensuremath{\diamond}~~}}
\newcommand{\gablinsection}{{\thesection~~{\normalsize\textbullet}~~}}
\newcommand{\gablinsubsection}{{\thesubsection.~~}}
\newcommand{\gablinparagraph}{{\normalfont\theparagraph}}

% Paragrahs
\maxsecnumdepth{paragraph}
\renewcommand{\theparagraph}{(\alph{paragraph})}
\crefname{paragraph}{paragraph}{paragraph}
\crefformat{paragraph}{#2§#1#3}
\crefformat{chapter}{#2§#1#3}
\crefformat{section}{#2§#1#3}
\makeatletter
\renewcommand{\p@paragraph}{\thesection} % https://tex.stackexchange.com/questions/531572/full-references-to-subordinated-sections-with-cref
\makeatother
\newcommand{\newpar}{\paragraph{}}
\setbeforeparaskip{.5\baselineskip}


\newcommand{\infrule}[1]{{\normalfont\textsc{#1}}}
\newcommand{\step}{\to}
\newcommand{\headstep}{\to_h}
\newcommand{\purestep}{\to_p}

\newcommand{\hole}{-}
\newcommand{\unitval}{()}
\newcommand{\STLC}{\lambda_{\rightarrow}}

% \newcommand{\keyword}[1]{\textbf{\textit{#1}}}
\newcommand{\pto}{\rightharpoonup}
\newcommand{\injto}{\hookrightarrow}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\ran}{\operatorname{ran}}

\newcommand{\textlang}[1]{\textsf{#1}}
\newcommand{\langkw}[1]{\textlang{\color{objlangcolor} #1}}
\newcommand{\Fst}{\operatorname{\langkw{fst}}}
\newcommand{\Snd}{\operatorname{\langkw{snd}}}
\let\oldprod\prod
\renewcommand{\prod}{\times}
\newcommand{\bigprod}{\oldprod}

\newcommand{\objlang}[1]{{\normalfont\textsf{\textcolor{objlangcolor}{#1}}}}
\definecolor{objlangcolor}{HTML}{A91616}

\newcommand{\objOp}[1]{\operatorname{\objlang{#1}}}
\newcommand{\objDelim}[1]{\objlang{(}#1\objlang{)}}

\newcommand{\objFst}[1]{\objOp{fst}#1}
\newcommand{\objSnd}[1]{\objOp{snd}#1}
\newcommand{\objInl}[1]{\objOp{inj}_{\objlang{1}}#1}
\newcommand{\objInr}[1]{\objOp{inj}_{\objlang{2}}#1}

\newcommand{\objPair}[2]{\objDelim{#1\mathpunct{\objlang{,}}#2}}
\newcommand{\objUnit}{\objlang{()}}
\newcommand{\objRec}[3]{\objOp{rec}#1\objDelim{#2} \mathrel{\textcolor{objlangcolor}{\ensuremath{\coloneqq}}} #3}
\newcommand{\objApp}[2]{#1\,#2}
\newcommand{\objAss}[2]{#1 \mathrel{\textcolor{objlangcolor}{\ensuremath{\coloneqq}}} #2}

\newcommand{\objMatch}[4]{\objlang{match} \;#1\, \objlang{with}\: \objInl{#2} \mathbin{\textcolor{objlangcolor}{\Rightarrow}} #3 \mathbin{\textcolor{objlangcolor}{\mid}} \objInr{#2} \mathbin{\textcolor{objlangcolor}{\Rightarrow}} #4 \,\objlang{end}} % TODO xcolor now has \mathcolor? Have to update TeXlive?

\newcommand{\objForall}[2]{\objApp{\textcolor{objlangcolor}{\Lambda}}{#2}}

\newcommand{\setVar}{\calV}
\newcommand{\setTVar}{\mathit{TVar}}
\newcommand{\setLoc}{\mathit{Loc}}
\newcommand{\setSto}{\mathit{Sto}}
\newcommand{\setExp}{\mathit{Exp}}
\newcommand{\setVal}{\mathit{Val}}
\newcommand{\setType}{\mathit{Type}}
\newcommand{\setECtx}{\mathit{ECtx}}
\newcommand{\setTAnn}{\mathit{TAnn}}
\newcommand{\typeUnit}{{\normalfont\textsf{Unit}}}
\newcommand{\freevar}[2][]{\mathit{FV}\auxdelimparen[#1]{#2}}
\newcommand{\freevarsort}[3][]{\mathit{FV}_{#3}\auxdelimparen[#1]{#2}}
\newcommand{\boundvar}[2][]{\mathit{BV}\auxdelimparen[#1]{#2}}
\newcommand{\allvar}[2][]{\mathit{V}\auxdelimparen[#1]{#2}}
\newcommand{\freeTvar}[1]{\mathit{FTV}(#1)}
% \newcommand{\powersetcard}[2]{\calP_\omega(#1)}

% \newcommand{\pmaps}[3][]{P_{#1}(#2,#3)}
\newcommand{\pmaps}[3][]{%
    \ifstrempty{#1}%
        {%
            P(#2,#3)
        }{%
            P_{#1}(#2,#3)
        }%
}

\newcommand{\typeForall}[2]{\forall #1. #2}
\newcommand{\typeExists}[2]{\exists #1. #2}
\newcommand{\typeRef}[1]{{\normalfont\textsf{ref(}#1\textsf{)}}}
\newcommand{\objTapp}[2]{\objApp{#1}{\textcolor{objlangcolor}{\_}}}
\newcommand{\objPack}[1]{\objOp{pack}#1}
\newcommand{\objUnpack}[3]{\objlang{unpack}\,#1\,\objlang{as}\,#2\,\objlang{in}\,#3}
\newcommand{\objRef}[1]{\objOp{ref}#1}
\newcommand{\objLoad}[1]{\objOp{!}#1}


\begin{document}

\maketitle


\chapter{Preliminaries}

If $X$ is a set, then we denote by $\powerset{X}$ the power set of $X$, and for a cardinal $\kappa$ we furthermore write $\powersetcard{\kappa}{X}$ for the collection of subsets of $X$ with cardinality strictly less than $\kappa$. If $A \in \powersetcard{\kappa}{X}$, then we also write $A \subseteq_\kappa X$. We do not distinguish between the ordinal $\omega$ and the cardinal $\aleph_0$, so $\powersetfin{X}$ denotes the set of finite subsets of $X$. If $Y$ is another set, then we denote the set of partial maps from $X$ to $Y$ by $\pmaps{X}{Y}$. For $f \in \pmaps{X}{Y}$ we let $\dom f$ and $\ran f$ denote the domain and range of $f$, respectively. We also write $\pmaps[\kappa]{X}{Y}$ for the partial functions $f \in \pmaps{X}{Y}$ with $\dom f \subseteq_\kappa X$. For $x_0 \in X$ and $y_0 \in Y$ we denote by $f[x_0 \mapsto y_0]$ the partial map given by
%
\begin{equation*}
    f[x_0 \mapsto y_0](x) =
    \begin{cases}
        y_0, & x = x_0, \\
        f(x), & x \in \dom f \setminus \{x_0\}.
    \end{cases}
\end{equation*}
%
Hence $\dom f[x_0 \mapsto y_0] = \dom f \union \{x_0\}$, so that if $f \in \pmaps[\omega]{X}{Y}$, then also $f[x_0 \mapsto y_0] \in \pmaps[\omega]{X}{Y}$.

We let $\naturals$ denote the set of natural numbers, including $0$. The image of a set $A \subseteq X$ under a function $f \colon X \to Y$ is denoted $f\image{A}$.

% \newcommand{\seq}[1]{\vec{#1}}
\newcommand{\seq}[1]{\bm{#1}}
\newcommand{\setOp}{\calO}
\newcommand{\setSort}{\calS}
\newcommand{\sortExp}{\mathit{Exp}}
\newcommand{\sortType}{\mathit{Type}}
\newcommand{\finseq}[1]{#1^*}
\newcommand{\disj}{\mathbin{\#}}

If $X$ is a set, then we denote by $\finseq{X}$ the set of finite sequences of elements in $X$, including the empty sequence. We often denote such a sequence by a boldface symbol e.g. $\seq{x}$. If $\seq{x} \in \finseq{X}$ and $x \in X$, then we write $x \in \seq{x}$ if $x$ is among the entries in $\seq{x}$, and $x \not\in \seq{x}$ otherwise. We use the convention that the entries in $\seq{x}$ are written $x_1, \ldots, x_k$.

If $A$ and $B$ are disjoint sets, then we write $A \disj B$.


\chapter{Induction and Inversion}

In this section we study inference rules and the structures that they give rise to, and we prove various theorems for such structures that will be important in the proof of type safety in TODO ref.

We first give an overview of the abstract theory, studying generating functions in complete lattices and in dcpo's. In a complete lattice, Knaster--Tarski's fixed-point theorem yields a principle of induction, which when applied to inference rules gives a notion of rule induction. In dcpo's we study continuous functions in the context of Kleene's fixed-point theorem, which gives a finite description of elements in a structure defined by inference rules. We then study derivations of such elements and prove an inversion theorem. We finally use Kleene's fixed-point theorem to prove that we can define functions recursively over inference rules.

For readers not interested in the technical details, we note that the main results that will be used in the sequel are \cref{thm:rule-induction}, \cref{cor:inversion} and \cref{thm:recursive-definitions}.


\section{Abstract theory}

\newpar

Let $(P,\leq)$ be a partially ordered set\footnote{That is, $\leq$ is a reflexive, transitive and anti-symmetric binary relation on $P$. We also call $P$ a \keyword{poset}.}. If $A \subseteq P$, then an element $x \in P$ is called an \keyword{upper bound} of $A$ if $a \leq x$ for all $a \in A$. If there is a least upper bound $x$ (i.e., such that if $y$ is any other upper bound, then $x \leq y$), then $x$ is called the \keyword{join} of $A$. The join of $A$ is clearly unique if it exists (by anti-symmetry), and we denote it by $\bigjoin A$. We similarly define the \keyword{meet} of $A$, denoted $\bigmeet A$, to be the greatest lower bound of $A$, if it exists. If every two-element subset of $P$ has a join and a meet, then $P$ is called a \keyword{lattice}, and we write $x \join y = \bigjoin \{x,y\}$ and $x \meet y = \bigmeet \{x,y\}$. If every subset of $P$ has a join and a meet, then $P$ is a \keyword{complete lattice}.

If $P$ is a partially ordered set, then a nonempty [TODO Vickers 7.2.1] subset $D \subseteq P$ is said to be \keyword{directed} if every finite subset of $D$ has an upper bound (but not necessarily a \emph{least} upper bound, i.e. a join). By induction this is equivalent to the property that, for every \emph{pair} of elements $x,y \in D$, there exists a $z \in D$ with $x \leq z$ and $y \leq z$.\footnote{This is the usual definition of directedness. As an example of why directedness in interesting, recall that a union of a collection of subspaces of a vector space is not usually a subspace itself, but it is if the collection is directed (with respect to inclusion). Similarly for subgroups and other algebraic structures, but note that the same does \emph{not} hold for e.g. topologies or $\sigma$-algebras. If we substituted \enquote{countable} for \enquote{finite} in the definition of directedness, $\sigma$-algebras would have this property as well, while we for topologies would need \enquote{arbitrary} subsets.} We further note that the image of a directed set under a monotone map\footnote{A function $f \colon P \to Q$ between posets is \keyword{monotone} if $x \leq y$ implies $f(x) \leq f(y)$ for all $x,y \in P$.} is also directed [TODO embedding]. If $D$ is a directed set whose join exists, we often write $\bigdjoin D \defeq \bigjoin D$ instead. If $\bigdjoin D$ exists for every directed subset $D$ of $P$, then $P$ is called a \keyword{directly complete partial order}, or \keyword{dcpo} for short. If $P$ also has a least element, usually written $\bot$, then $P$ is called a \keyword{dcppo} (the extra \enquote{p} is for \enquote{pointed}). Notice that complete lattices are dcppo's.

If $P$ and $Q$ are dcpo's, then a map $f \colon P \to Q$ is \keyword{continuous}\footnote{Also called \keyword{Scott-continuous} after Dana Scott.} if, for every directed $D \subseteq P$, the image $f\image{D}$ is directed and
%
\begin{equation*}
    f \bigl( \bigdjoin D \bigr)
        = \bigdjoin f\image{D}.
\end{equation*}
%
It is easy to show that continuous maps are monotone (notice that if $x \leq y$, then the set $\{x,y\}$ is directed). If conversely $f$ is monotone, then $f\image{D}$ is as mentioned also directed, and the inequality \enquote{$\geq$} always holds.

\newcommand{\graph}[2][]{\calG\auxdelimparen[#1]{#2}}


\begin{examplebreak}[Products of dcpo's]
    Let $(P_i)_{i \in I}$ be a collection dcpo's, and equip the product $P \defeq \bigprod_{i \in I} P_i$ with the product order\footnote{The \keyword{product order} on $P = \bigprod_{i \in I} P_i$ is defined as follows: For $(x_i)_{i \in I}, (y_i)_{i \in I} \in P$ we say that $(x_i)_{i \in I} \leq (y_i)_{i \in I}$ if $x_i \leq y_i$ for all $i \in I$. The projections $\pi_i \colon P \to P_i$ are thus monotone, and, as an aside, we mention that $P$ is a categorical product in the category of posets and monotone maps.}. We claim that $P$ is also a dcpo. If $D \subseteq P$ is directed, then each $\pi_i\image{D}$ is also directed and thus has a join $x_i$. Letting $x = (x_i)_{i \in I}$ it is easy to see that $x = \bigdjoin D$ in $P$: It is clear that $x$ is an upper bound of $D$, and furthermore $x \leq y$ if and only if $x_i \leq \pi_i(y)$ for all $y \in P$ and $i \in I$.

    In particular,
    %
    \begin{equation*}
        \pi_i \bigl( \bigdjoin D \bigr)
            = \pi_i(x)
            = x_i
            = \bigdjoin \pi_i\image{D},
    \end{equation*}
    %
    so $\pi_i$ is continuous.\footnote{It follows that $P$ is a product in the category of dcpo's and continuous maps.}
\end{examplebreak}


\begin{examplebreak}[Partial functions]
    If $X$ and $Y$ are sets, then we order the set $\pmaps{X}{Y}$ of partial functions as follows: For $f,g \colon X \pto Y$ we let $f \leq g$ if $\dom f \subseteq \dom g$ and $f(x) = g(x)$ for all $x \in \dom f$. The \keyword{graph} of a partial function $f$ is the set
    %
    \begin{equation*}
        \graph{f} = \set{(x,y) \in X \prod Y}{x \in \dom f \text{ and } f(x) = y}.
    \end{equation*}
    %
    This induces a map $\calG \colon \pmaps{X}{Y} \to \powerset{X \prod Y}$ given by $f \mapsto \graph{f}$, which is clearly an order-embedding. If $D \subseteq \pmaps{X}{Y}$ is a directed set of partial functions, then it is clear that the set $\bigdjoin \calG\image{D} = \bigunion_{d \in D} \graph{d}$ is the graph of a partial function. This function is then the join of $D$ in $\pmaps{X}{Y}$, which is therefore a dcpo, and the map $\calG$ is thus continuous. In fact, $\pmaps{X}{Y}$ is a dcppo since the empty map is the least element.

    Denoting the projection maps by $\pi_X \colon X \prod Y \to X$ and $\pi_Y \colon X \prod Y \to Y$, notice that $\dom f = \pi_X\image{\graph{f}}$ and $\ran f = \pi_Y\image{\graph{f}}$. Since images preserve unions, and joins in power sets are just unions, if $D \subseteq \pmaps{X}{Y}$ is directed then
    %
    \begin{equation*}
        \dom \bigdjoin D
            = \pi_X\image[\big]{\graph[\big]{\bigdjoin D}}
            = \pi_X\image[\big]{\bigdjoin \calG\image{D}}
            = \bigdjoin \pi_X\image[\big]{\calG\image{D}}
            = \bigdjoin \dom\image{D},
    \end{equation*}
    %
    and we similarly have $\ran \bigdjoin D = \bigdjoin \ran\image{D}$.
\end{examplebreak}


Let $F \colon P \to P$ be a monotone map. We think of $F$ as a \keyword{generating function}. An element $x \in P$ is said to be \keyword{$F$-closed} if $F(x) \leq x$, \keyword{$F$-consistent} if $x \leq F(x)$, and a \keyword{fixed-point} of $F$ if $F(x) = x$. If $F$ has a least fixed-point, then this is usually denoted $\mu F$. Similarly, the greatest fixed-point, if it exists, is denoted $\nu F$.


\newpar

Let $P$ be a dcppo and $F \colon P \to P$ a monotone function. We clearly have $\bot \leq F(\bot)$, and since $F$ is monotone we get the chain\footnote{A \keyword{chain} is a totally ordered set.}
%
\begin{equation*}
    \bot
        \leq F(\bot)
        \leq \cdots
        \leq F^n(\bot)
        \leq F^{n+1}(\bot)
        \leq \cdots
\end{equation*}
%
called the \keyword{ascending Kleene chain}. Since it is a chain it is also directed. The main theorem is the following:

\begin{theorem}[Kleene's fixed-point theorem]
    \label{thm:kleene-fixpoint-theorem}
    If $P$ is a dcppo and $F \colon P \to P$ is continuous, then $F$ has a least fixed-point $\mu F$ and
    %
    \begin{equation*}
        \mu F
            = \bigdjoin_{n \in \naturals} F^n(\bot).
    \end{equation*}
\end{theorem}

\newenvironment{prooffootnote}[1]{\begin{proofof}[Proof\,\protect\footnotemark]\footnotetext{#1}}{\end{proofof}}

\begin{prooffootnote}{This proof is based on \textcite[Theorem~8.15]{davey-priestley-order}.}
    First notice that, since $F$ is continuous,
    %
    \begin{equation*}
        F \Bigl( \bigdjoin_{n \in \naturals} F^n(\bot) \Bigr)
            = \bigdjoin_{n \in \naturals} F^{n+1}(\bot)
            = \bigdjoin_{n \in \naturals^+} F^n(\bot)
            = \bigdjoin_{n \in \naturals} F^n(\bot),
    \end{equation*}
    %
    where we use that $F^0(\bot) = \bot$. Hence $\bigdjoin_{n \in \naturals} F^n(\bot)$ is indeed a fixed-point of $F$. If $\beta$ is any fixed-point of $F$, then $\bot \leq \beta$, and hence $F^n(\bot) \leq F^n(\beta) = \beta$ since $F$ is monotone. Taking the join on the left-hand side yields $\bigdjoin_{n \in \naturals} F^n(\bot) \leq \beta$ as desired.
\end{prooffootnote}
%
The proof of \cref{thm:kleene-fixpoint-theorem} betrays that it is not necessary to work with directed sets: Indeed, the proof only uses the fact that an increasing sequence -- also called an \keyword{$\omega$-chain} -- in $P$ has a join, and that $F$ preserves such joins. The theory would work out in the same way if we instead worked in partial orders in which every $\omega$-chain has a join, called \keyword{$\omega$-complete partial orders} -- for short \keyword{$\omega$-cpo's}, or \keyword{$\omega$-cppo's} if they have a least element. Our reasons for nonetheless working in dcpo's are (at least) twofold: Firstly, replacing directed sets with $\omega$-chains does not simplify the arguments in any way. Secondly, since we intend to apply this theory to power sets anyway, there is no reason to make the weaker assumption that $P$ is an $\omega$-cppo. The present choice yields the stronger conclusion that the map $F$ in \cref{lem:inference-rules-continuous} is continuous and hence preserves all \emph{directed} joins and not just joins of $\omega$-chains. [TODO also useful in domain theory maybe??]


\newpar

Next, let $L$ be a complete lattice, and let $F \colon L \to L$ be a monotone map. Even though $L$ is also a dcppo, if $F$ is not continuous then \cref{thm:kleene-fixpoint-theorem} does not apply. However, $F$ still has fixpoints, as the following theorem shows:

\begin{theorem}[Knaster--Tarski's fixed-point theorem]
    \label{thm:knaster-tarski}
    If $L$ is a complete lattice and $F \colon L \to L$ is monotone, then $F$ has a least and a greatest fixed-point, and these are given by
    %
    \begin{equation*}
        \mu F
            = \bigmeet \set{x \in L}{F(x) \leq x}
        \quad \text{and} \quad
        \nu F
            = \bigjoin \set{x \in L}{x \leq F(x)}.
    \end{equation*}
    %
    In particular, $\mu F$ is the smallest $F$-closed element and $\nu F$ is the greatest $F$-consistent element in $L$.
\end{theorem}

\begin{prooffootnote}{This proof is based on \textcite[Theorem~2.35]{davey-priestley-order}.}
    Denote the meet above by $\alpha$. If $x$ is $F$-closed, then $\alpha \leq x$, so $F(\alpha) \leq F(x) \leq x$. Taking the meet of $x$ we get $F(\alpha) \leq \alpha$, so $\alpha$ is closed. It follows that $F(F(\alpha)) \leq F(\alpha)$, so $F(\alpha)$ is also closed, and so $\alpha \leq F(\alpha)$. Hence $\alpha$ is a fixed-point. Since every other fixed-point is in particular closed, $\alpha$ is the least fixed-point.
\end{prooffootnote}

\begin{corollarynoproof}[Principle of induction]
    \label{cor:induction-abstract}
    If $y \in L$ is $F$-closed, then $\mu F \leq y$.
\end{corollarynoproof}
%
We dually have a \keyword{principle of coinduction} -- if $y \in L$ is $F$-consistent, then $y \leq \nu F$ -- but we shall not need this in the sequel.


\section{In power sets}

% \newcommand{\myinfrule}[3]{#1 \colon #2 \vdash #3}

\newcommand{\myinfrule}[3]{%
    \ifstrempty{#1}%
        {%
            #2 \vdash #3
        }{%
            #1 \colon #2 \vdash #3
        }%
}

\newpar

Specialising to the case where $L$ is a power set $\powerset{X}$, one way to define a generating function is using inference rules. An \keyword{inference rule} on $X$ is an expression on the form
%
\begin{equation*}
    \inferrule*[left=Rule]{
        x_1 \and x_2 \and \cdots \and x_k
    }{
        y
    }
\end{equation*}
%
where $x_1, \ldots, x_k$ and $y$ are elements of $X$, and $k \in \naturals$.\footnote{We could equivalently view an inference rule as an element of the product $X^k \prod X$.} We allow $k$ to be zero, in which case we call the rule an \keyword{axiom}. We have decorated the expression with the label \enquote{\infrule{Rule}} so that we may refer to it later. Let us call $x_1, \ldots, x_k$ the \keyword{antecedents} of the rule and $y$ the \keyword{consequent}. If $R$ is a rule with antecedents $x_1, \ldots, x_k$ and consequent $y$, we also write $\myinfrule{R}{x_1, \ldots, x_k}{y}$, or $\myinfrule{R}{\seq{x}}{y}$ for short. We often suppress $R$ from this notation and simply write $\myinfrule{}{\seq{x}}{y}$. The number of antecedents of a rule $R$ is called its \keyword{arity} and is denoted $\rho(R)$.

Given a (possibly infinite) collection $\calR$ of inference rules, we construct a generating function $F \colon \powerset{X} \to \powerset{X}$ by defining $F(A)$ for a subset $A \subseteq X$ as follows: For $y \in X$ we let $y \in F(A)$ if and only if there is a rule $\myinfrule{}{\seq{x}}{y}$ in $\calR$ with $\seq{x} \subseteq A$. We say that $F$ is \keyword{represented by} $\calR$. In this case $F$ is clearly monotone, so it has a least and a greatest fixed-point $\mu F$ and $\nu F$, respectively. Since it is often unnecessary to explicitly talk about $F$, we also write $\mu \calR$ and $\nu \calR$ for these fixed-points.

By \cref{cor:induction-abstract} we also get a principle of induction for $F$. However, as for fixed-points it is useful to restate induction in terms of the inference rules, since we usually have explicit rules in mind when defining $F$. If $\calR$ is a collection of inference rules on $X$, then we say that a subset $A \subseteq X$ is \keyword{$\calR$-closed} if, given any rule $\myinfrule{}{\seq{x}}{y}$ in $\calR$, $\seq{x} \subseteq A$ implies that $y \in A$.

\begin{lemma}
    \label{lem:R-closed-F-closed}
    If $F$ is represented by a collection $\calR$ of inference rules, then a subset $A \subseteq X$ is $F$-closed if and only if it is $\calR$-closed.
\end{lemma}

\begin{proof}
    First assume that $A$ is $F$-closed so that $F(A) \subseteq A$, and consider a rule $\myinfrule{}{\seq{x}}{y}$ from $\calR$ with $\seq{x} \subseteq A$. Since $F$ is represented by $\calR$, this implies that $y \in F(A) \subseteq A$, so $A$ is $\calR$-closed.

    If $A$ is $\calR$-closed, then let $y \in F(A)$. Hence there is a rule $\myinfrule{}{\seq{x}}{y}$ in $\calR$ with $\seq{x} \subseteq A$. But since $A$ is $\calR$-closed, this implies that $y \in A$, and so $F(A) \subseteq A$.
\end{proof}


\begin{theorem}[Principle of rule induction]
    \label{thm:rule-induction}
    If $\calR$ is a set of inference rules on $X$ and $A \subseteq X$, then $\mu \calR \subseteq A$ if the following condition holds: For every inference rule $\myinfrule{}{\seq{x}}{y}$ in $\calR$, $\seq{x} \subseteq A$ implies $y \in A$.
\end{theorem}

\begin{proof}
    Let $F$ be the function represented by $\calR$. The condition says precisely that $A$ is $\calR$-closed, which implies that $A$ is $F$-closed by \cref{lem:R-closed-F-closed}. But then \cref{cor:induction-abstract} implies that $\mu \calR = \mu F \subseteq A$.
\end{proof}


\newpar

\newcommand{\assum}[1]{\operatorname{asm}#1}
\newcommand{\concl}[1]{\operatorname{con}#1}
\newcommand{\derlen}[1]{l(#1)}


Sometimes it is necessary (or at least useful) to have an alternative characterisation of $\mu \calR$, one that makes use of the fact that the number of antecedents of an inference rule is finite. This property is reflected by the represented function $F$ in the following way:

\begin{lemma}
    \label{lem:inference-rules-continuous}
    If $F$ is represented by a collection of inference rules, then $F$ is continuous.
\end{lemma}

\begin{proof}
    It suffices to show that if $\calD \subseteq \powerset{X}$ is directed, then $F(\bigdjoin \calD) \subseteq \bigdjoin F\image{\calD}$, so let $y \in F(\bigdjoin \calD)$. Say that $F$ is represented by a collection $\calR$. Then there is a rule $\myinfrule{}{\seq{x}}{y}$ in $\calR$ with $\seq{x} \subseteq \bigdjoin \calD$. Since the (directed) join in $\powerset{X}$ is just union, there are sets $A_1, \ldots, A_k \in \calD$ such that $x_i \in A_i$. And since $\calD$ is directed there is a set $A \in \calD$ with $A_i \subseteq A$, so that $x_i \in A$ for all $i$. But then $y \in F(A) \subseteq \bigdjoin F\image{\calD}$ as desired.
\end{proof}
%
Hence, \cref{thm:kleene-fixpoint-theorem} implies that $\mu F = \bigdjoin_{n \in \naturals} F^n(\emptyset)$.

To make this more concrete, we study how one can use the inference rules to derive that some element of $X$ lies in $\mu F$. If $\calR$ is a collection of inference rules, then a \keyword{derivation} from $\calR$ is a finite sequence $D = (R_1, \ldots, R_n)$ of inference rules from $\calR$. If $y$ is a consequent of some $R_i$, then we say that $y$ is a \keyword{conclusion} of $D$. The consequent of $R_n$ is called the \keyword{final conclusion} of $D$. We say that $x \in X$ is an \keyword{assumption} of $D$ if $x$ is an antecedent of some $R_i$, and if none of the rules $R_1, \ldots, R_{i-1}$ has $x$ as its consequent. That is, $x$ is an assumption if it is not implied by any of the previous rules in the derivation. The set of conclusion of $D$ is denoted $\concl{D}$, and the set of assumptions of $D$ is denoted $\assum{D}$. If $\assum{D} = \emptyset$, then we say that $D$ is \keyword{closed}.

\begin{remark}
    Let $D = (R_1, \ldots, R_n)$ be a derivation.
    %
    \begin{enumrem}
        \item If $D$ is closed, then $R_1$ must be an axiom.
    
        \item\label{enum:subderivation} For any $i \in \{1, \ldots, n\}$, $D' = (R_1, \ldots, R_i)$ is called a \keyword{subderivation} of $D$. If $i < n$, then $D'$ is called a \keyword{proper subderivation} of $D$ (so $D'$ is a proper subderivation when $D' \neq D$). It is clear that $\assum{D'} \subseteq \assum{D}$ and $\concl{D'} \subseteq \assum{D}$, so $D'$ is closed if $D$ is closed.
    
        \item\label{enum:derivation-antecedent} If $D$ is closed and $x$ is an antecedent of some $R_i$, then $D$ has a proper subderivation $D'$ whose final consequence is $x$: For $x$ must be the consequent of some $R_1, \ldots, R_{i-1}$, say $R_j$, and $(R_1, \ldots, R_j)$ is closed subderivation of $D$ by \subcref{enum:subderivation}.

        \item\label{enum:derivation-composition} If $E = (S_1, \ldots, S_m)$ is another derivation, then
        %
        \begin{equation*}
            D \circ E
                \defeq (R_1, \ldots, R_n, S_1, \ldots, S_m)
        \end{equation*}
        %
        is another derivation. It is clear that
        %
        \begin{equation*}
            \assum{(D \circ E)}
                = \assum{D} \union (\assum{E} \setminus \concl{D}),
        \end{equation*}
        %
        that
        %
        \begin{equation*}
            \concl{(D \circ E)}
                = \concl{D} \union \concl{E},
        \end{equation*}
        %
        and that the final conclusion of $D \circ E$ is the final conclusion of $E$. In particular, if $D$ is closed and $\assum{E} \subseteq \concl{D}$, then $D \circ E$ is also closed.

        \item The \keyword{length} of a derivation $D = (R_1, \ldots, R_n)$ is $n$, and this is denoted $\derlen{D}$. If $D'$ is a subderivation of $D$, then $\derlen{D'} \leq \derlen{D}$ with equality if and only if $D' = D$. If $E$ is another derivation, then $\derlen{D \circ E} = \derlen{D} + \derlen{E}$.
    \end{enumrem}
\end{remark}


\begin{proposition}
    \label{prop:fixpoint-iff-derivation}
    If $\calR$ is a collection of inference rules and $y \in X$, then $y \in \mu \calR$ if and only if $y$ is the final conclusion of some closed derivation from $\calR$.
\end{proposition}

\begin{proof}
    Let $F$ be the function represented by $\calR$. First assume that $y \in \mu F$, and recall that $\mu F = \bigdjoin_{n \in \naturals} F^n(\emptyset) = \bigunion_{n \in \naturals} F^n(\emptyset)$. We prove by induction in $n$ that every element in $F^n(\emptyset)$ is the final conclusion of a closed derivation. The base case $y \in F^0(\emptyset) = \emptyset$ is vacuous, so let $n \in \naturals$ and assume that the claim holds for every element of $F^n(\emptyset)$. If $y \in F^{n+1}(\emptyset) = F(F^n(\emptyset))$, then there is an inference rule $\myinfrule{R}{\seq{x}}{y}$ with $\seq{x} \subseteq F^n(\emptyset)$. By induction each $x_i$ is the final conclusion of some derivation $D_i$, and by \cref{enum:derivation-composition} the derivation $D_1 \circ \cdots \circ D_k \circ (R)$ is a closed derivation, and $y$ is its final conclusion.

    We prove the converse by (strong) induction on the length of a derivation. If $y$ is the final conclusion of a closed derivation $(R)$ of length $1$, then $R$ must be an axiom. But then $y \in F(\emptyset) \subseteq \mu F$ since $F$ is represented by $\calR$. Next, let $n \in \naturals$ and let $y$ be the final conclusion of a closed derivation $D = (R_1, \ldots, R_{n+1})$. If $x$ is an antecedent of $R_{n+1}$, then by \cref{enum:derivation-antecedent} $D$ must have a proper subderivation $D'$ whose final conclusion is $x$. By \cref{enum:subderivation} $D'$ is also closed, and so $x \in \mu F$ by induction. But then we must have $y \in F(\mu F) = \mu F$ as desired.
\end{proof}
%
It is of course standard to use derivation \emph{trees}, but all that matters is that there is a finite way to obtain elements in $\mu \calR$. Since it is easier to define and reason about linear derivations -- and since we will not have to do any actual derivations -- we have taken this approach here.\footnote{Compare the role of deductive calculi in first-order logic, where e.g. the compactness theorem can be obtained from the mere \emph{existence} of a deductive calculus (with finite derivations), cf. \textcite[Theorem~3.3.1]{leary-kristiansen-logic}.} The main consequence we shall use is the following:

\begin{corollary}[Inversion]
    \label{cor:inversion}
    If $\calR$ is a collection of inference rules and $y \in \mu \calR$, then there is a rule $\myinfrule{}{\seq{x}}{y}$ in $\calR$ such that $\seq{x} \subseteq \mu \calR$.
\end{corollary}

\begin{proof}
    By \cref{prop:fixpoint-iff-derivation} there is a closed derivation $D$ of $y$ from $\calR$, and let its last rule be $\myinfrule{}{\seq{x}}{y}$. For each $x_i$, some subderivation of $D$ is a derivation of $x_i$ by \cref{enum:subderivation}. Another application of \cref{prop:fixpoint-iff-derivation} then implies that $x_i \in \mu \calR$.
\end{proof}


\newpar

The final construction we need is definition by recursion.\footnote{The statement of this result is based on \textcite[Theorem~3.2]{goldrei-set-theory}, while the proof is inspired by \textcite[§8.18]{davey-priestley-order}.}

\begin{theorem}[Definition by recursion]
    \label{thm:recursive-definitions}
    Let $\calR$ be a set of inference rules such that each $y \in \mu\calR$ is the consequent of a unique rule, and let $Z$ be any set. For each $R \in \calR$ let $h_R \colon \mu \calR \prod Z^{\rho(R)} \to Z$ be a function. Then there is a unique function $f \colon \mu \calR \to Z$ such that if $\myinfrule{R}{\seq{x}}{y}$, then
    %
    \begin{equation*}
        f(y)
            = h_R \bigl( y, f(x_1), \ldots, f(x_k) \bigr).
    \end{equation*}
\end{theorem}

\begin{proof}
    Define a function $G \colon \pmaps{\mu\calR}{Z} \to \pmaps{\mu\calR}{Z}$ as follows: For $f \colon \mu\calR \pto Z$ let $\dom G(f)$ be the set of elements $y \in \mu\calR$ such that if $\myinfrule{R}{\seq{x}}{y}$ is the unique rule with consequent $y$, $\seq{x} \subseteq \dom f$. For $y \in \dom G(f)$ we then let
    %
    \begin{equation*}
        G(f)(y)
            = h_R \bigl( y, f(x_1), \ldots, f(x_k) \bigr).
    \end{equation*}
    %
    If $F$ is the function represented by $\calR$, notice that $\dom G(f) = F(\dom f)$. In particular, $\dom G(\bot) = F(\emptyset)$, so it follows by induction that $\dom G^n(\bot) = F^n(\emptyset)$ for $n \in \naturals$.

    We next show that $G$ is continuous, so let $D \subseteq \pmaps{\mu\calR}{Z}$ be directed. First notice that, since $F$ is continuous by [TODO ref], we get from [TODO ref ex] that
    %
    \begin{align*}
        \dom G \bigl( \bigdjoin D \bigr)
            &= F \bigl( \dom \bigdjoin D \bigr)
             = F \bigl( \bigdjoin \dom \image{D} \bigr) \\
            &= \bigdjoin F\image[\big]{\dom \image{D}}
             = \bigdjoin \dom \image[\big]{G\image{D}} \\
            &= \dom \bigdjoin G\image{D}.
    \end{align*}
    %
    For $y$ in this common domain there is a $d \in D$ such that $y \in \dom G(d)$. If $\myinfrule{R}{\seq{x}}{y}$ then $\seq{x} \subseteq \dom d$, and so
    %
    \begin{align*}
        G \bigl( \bigdjoin D \bigr) (y)
            &= h_R \bigl( y, (\bigdjoin D)(x_1), \ldots, (\bigdjoin D)(x_k) \bigr) \\
            &= h_R \bigl( y, d(x_1), \ldots, d(x_k) \bigr) \\
            &= G(d)(y) \\
            &= \bigl( \bigdjoin G\image{D} \bigr) (y).
    \end{align*}
    %
    Thus $G(\bigdjoin D) = \bigdjoin G\image{D}$, so $G$ is continuous.
    
    It now follows from \cref{thm:kleene-fixpoint-theorem} that
    %
    \begin{equation*}
        \dom \mu G
            = \dom \bigdjoin_{n \in \naturals} G^n(\bot)
            = \bigdjoin_{n \in \naturals} \dom G^n(\bot)
            = \bigdjoin_{n \in \naturals} F^n(\emptyset)
            = \mu F.
    \end{equation*}
    %
    Hence $\mu G$ is in fact a total function solving the equation [TODO ref], which proves the theorem.
\end{proof}

We also sometimes wish to define \emph{partial} functions recursively. This is no issue, for as the following shows, there is a correspondence between partial functions and certain total functions.

\newcommand{\cat}{\mathbf}
\newcommand{\catPfn}{\cat{Pfn}}
\newcommand{\catSet}{\cat{Set}}
\newcommand{\catSetP}{\cat{Set}_*}

\begin{example}
    Consider a partial function $f \colon X \pto Y$, let $*$ be some element not in $Y$, and let $Y_* = Y \union \{*\}$. We then define a total function $f_* \colon X \to Y_*$ by letting $f_*(x) = f(x)$ if $x \in \dom f$, and $f_*(x) = *$ otherwise.

    Conversely, given a total function $f_* \colon X \to Y_*$ we obtain the corresponding partial function $f \colon X \pto Y$ by restricting $f_*$ to the set $\set{x \in X}{f_*(x) \neq *}$.\footnote{This correspondence between partial and total functions indicates that there is some correspondence between the category $\catPfn$ of sets and partial functions and the category $\catSetP \cong 1/\catSet$ of pointed sets. Indeed, while these are not isomorphic, they are \emph{equivalent} (see e.g. \cite[Theorems~156 and 158]{smith-categories-2}).}
\end{example}
%
Hence, to define a partial function $f$ recursively, we first extend the indented codomain by some new element such as $*$, and define a total function $f_*$ recursively while letting $f_*(x) = *$ whenever $f$ is supposed to be undefined at $x$. Finally restrict $f_*$ to obtain $f$.


[TODO example? + nonexample for sigma-algebras/topologies?]


\section{Topology and logic}

\begin{definition}[Frames]
    A poset $P$ is a \keyword{frame} if
    %
    \begin{enumdef}
        \item every subset of $P$ has a join,
        \item every finite subset of $P$ has a meet, and
        \item $P$ satisfies the \keyword{join-infinite distributive law},
        %
        \begin{equation*}
            x \meet \bigjoin Y
                = \bigjoin \set{x \meet y}{y \in Y}
        \end{equation*}
        %
        for all $x \in P$ and $Y \subseteq P$.
    \end{enumdef}
\end{definition}
%
Since all joins exist, so do all meets. In particular, a frame is complete distributive lattice.

\newcommand{\inpoint}{\prec}
\newcommand{\opens}[1]{\calU_{#1}}
\newcommand{\specs}{\sqsubseteq}

\begin{definition}[Topological systems]
    A \keyword{topological system} is a pair $(X,\calT)$, where $X$ is a set and $\calT$ is a frame, equipped with a relation ${\inpoint} \subseteq X \prod \calT$ with the following properties:
    %
    \begin{enumdef}
        \item For all $\calS \subseteq_\omega \calT$ and $x \in X$,
        %
        \begin{equation*}
            x \inpoint \bigmeet \calS
                \quad \text{if and only if} \quad
                \forall U \in \calS \colon a \inpoint U.
        \end{equation*}

        \item For all $\calS \subseteq \calT$ and $x \in X$,
        %
        \begin{equation*}
            x \inpoint \bigjoin \calS
                \quad \text{if and only if} \quad
                \exists U \in \calS \colon a \inpoint U.
        \end{equation*}
    \end{enumdef}
\end{definition}
%
The elements in $X$ are called \keyword{points}, and the elements of $\calT$ are alled \keyword{opens}. If $x \inpoint U$, then we say that $x$ is an \keyword{inner point} of $U$. The set of opens of which $x$ is an inner point is denoted $\opens{x}$. For $x,y \in X$ we say that $y$ \keyword{specialises} $x$, written $x \specs y$, if $\opens{x} \subseteq \opens{y}$. We call $\specs$ the \keyword{specialisation preorder} on $X$, and it is indeed a preorder.

In trying to equip $X$ with a logic, we obviously wish to define disjunctions on $X$. For $A \subseteq X$, we say that $x \in X$ is a \keyword{disjunction} of $A$, written $\bigjoin A$, if it satisfies
%
\begin{equation*}
    \bigjoin A \inpoint U
        \quad \text{if and only if} \quad
        \exists a \in A \colon a \inpoint U,
\end{equation*}
%
for all $U \in \calT$. We are careful to distinguish this from a join $\bigdjoin A$ of $A$ with respect to $\specs$.\footnote{Note that since $\specs$ is not (generally) antisymmetric, the join of $A$ is not necessarily unique if it exists, but it is easy to show that if $x$ and $y$ are both joins of $A$, then $x \specs y$ and $y \specs x$.}

\begin{proposition}
    Any disjunction is a join, but not vice-versa.
\end{proposition}

\begin{proof}
    Let $\bigjoin A$ be a disjuction. If $a \in A$ and $a \inpoint U$, then we also have $\bigjoin A \inpoint U$, so $\bigjoin A$ is an upper bound of $A$. Next let $x$ be any upper bound of $A$. If $\bigjoin A \inpoint U$, then $a \inpoint U$ for some $a \in A$, and since $a \specs x$ we also have $x \inpoint U$. Hence $\bigjoin A \specs x$.
\end{proof}

However, arbitrary subsets of $X$ do not have disjunctions, or at least do not have disjunctions that respect the logic on $\calT$. For instance, if $x,y \in X$ are incomparable with respect to the specialisation preorder, then there are opens $U \in \opens{x} \setminus \opens{y}$ and $V \in \opens{Y} \setminus \opens{X}$. If the disjunction $x \join y$ existed, then we would have $x \join y \inpoint U$ and $x \join y \inpoint V$, but not $x \join y \inpoint U \meet V$, in contradiction with the assumption that $(X,\calT)$ is a topological system.

In order to mitigate this problem, we must find 



\section{Abstract syntax}

\newpar

Let $\setSort$ be a set. We think of $\setSort$ as a set of \keyword{sorts}, for instance expressions or types. A \keyword{valence} is an expression on the form $s_1,\ldots,s_k.s$, or $\seq{s}.s$ for short, where $s_1,\ldots,s_k,s$ are sorts for $k \in \naturals$. [TODO formalise expression] If $k = 0$, then instead of \enquote{$.s$} we omit the period and simply write \enquote{$s$}. Each argument of an operator is supposed to have a valence which describes both the sort of that argument as well as the sorts of the variables that are bound in that argument (if any). An \keyword{arity} is then an expression on the form $(v_1, \ldots, v_n)s$, where $v_1, \ldots, v_n$ are valences and $s$ is a sort, for $n \in \naturals$. [TODO formalise expression] Operators will be assigned arities, and $v_i$ is then the valence of the $i$th argument to an operator with the above arity, and $s$ is the sort of the return value of the operator. For each arity $\alpha$ we also fix a set $\setOp_\alpha$ which we think of as a set of \keyword{operators}. Finally we fix for each sort $s \in \calS$ a set $\setVar_s$ of variables so that we for instance can have both ordinary variables (of expression sort) and type variables (of type sort). Note that we do not [TODO contrary to Harper] require the sets $\setOp_\alpha$ to be disjoint, so an operator can have multiple arities. [TODO variables of multiple sorts?] We let $\setOp$ be the set of all operators of any arity, and $\calX$ the set of all variables of any sort.

For example, if $\sortExp$ is a sort of expressions and $\sortType$ a sort of types, an operator corresponding to lambda abstraction with type annotation might have arity $(\sortType, \sortExp.\sortExp)\sortExp$. This indicates that its arguments are a type (the type of the argument) along with an expression in which a variable of expression sort is bound, and the entire abstraction is also of expression sort. If we also allow the abstraction to be given a name (in order to easily allow for recursive functions), then the arity of the operator might instead be $(\sortType; \sortExp,\sortExp.\sortExp)\sortExp$. We here separate the valences with a semicolon since the valence of the second argument itself contains a comma.


\newpar

We first define the set of \keyword{unsorted abstract syntax trees}, or simply \keyword{UASTs}. [TODO universal set] We define this set using inference rules. For each $x \in \setVar$ we have the axiom
%
\begin{equation}
    \label{eq:UAST-var}
    \inferrule*{ }{
        x
    }
\end{equation}
%
saying that $x$ itself is always a UAST. For each $\phi \in \setOp$ taking $n$ arguments and $\seq{x}_1, \ldots, \seq{x}_n \in \finseq{\setVar}$ we also have the rule
%
\begin{equation}
    \label{eq:UAST-op}
    \inferrule*{
        a_1 \and \cdots \and a_n
    }{
        \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)
    }
\end{equation}
%
[TODO formalise expression] We denote the set of UASTs by $\calA$. The expression $\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)$ is supposed to denote the application of the operator $\phi$ on the UASTs $a_1, \ldots, a_n$ in which the variables $\seq{x}_1, \ldots, \seq{x}_n$ have been bound.

\newcommand{\hassort}[2]{#1 : #2}

Using unsorted ASTs we also define \keyword{(well-sorted) abstract syntax trees}, or \keyword{ASTs} for short, as following. We recursively define a relation on $\calA \prod \setSort$, where we write an element $(a,s)$ by $\hassort{a}{s}$: For each sort $s \in \setSort$ and variable $x \in \setVar_s$ of sort $s$ we have the rule
%
\begin{equation}
    \inferrule*{ }{
        \hassort{x}{s}
    }
\end{equation}
%
Next, consider an operator $\phi \in \setOp_\alpha$ where $\alpha = (v_1 ; \ldots ; v_n)s$ and $v_i = (s_{i1}, \ldots, s_{ik_i})s_i$. If we apply $\phi$ to ASTs of the proper sorts and bind in each argument the correct number of variables of the right sorts, then the resulting UAST should also be well-sorted with sort $s$: Hence, if $\seq{x}_i$ is a sequence of variables $(x_{i1}, \ldots, x_{ik_i})$ where $x_{ij}$ has sort $s_{ij}$, then we have the rule
%
\begin{equation}
    \inferrule*{
        \hassort{a_1}{s_1} \and \cdots \and \hassort{a_n}{s_n}
    }{
        \hassort{\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)}{s}
    }
\end{equation}
%
If $\hassort{a}{s}$, then we say that $a$ is \keyword{well-sorted} with sort $s$.

We next define the set of \keyword{free variables}. This will be a function $\freevar{} \colon \calA \to \powerset{\setVar}$ defined recursively using \cref{thm:recursive-definitions}. According to this theorem, we must specify for each inference rule $R$ a function $h_R \colon \calA \prod \powerset{\setVar}^{\rho(R)} \to \powerset{\setVar}$. If $R$ is a rule as in \cref{eq:UAST-var}, then $\rho(R) = 0$ and we let $h_R(a) = \{a\}$. If instead $R$ is as in \cref{eq:UAST-op}, then we let
%
\begin{equation*}
    h_R \bigl( \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n), A_1, \ldots, A_n \bigr)
        = \bigunion_{i=1}^n (A_i \setminus \seq{x}_i).
\end{equation*}
%
(If the first argument to $h_R$ is instead a variable, then we let the value of $h_R$ be an arbitrary element of $\powerset{\setVar}$.) This implies the existence of a function $\freevar{} \colon \calA \to \powerset{\setVar}$ with the properties
%
\begin{align*}
    \freevar{x}
        &= \{x\}, \\
    \freevar[\big]{\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)}
        &= \bigunion_{i=1}^n (\freevar{a_i} \setminus \seq{x}_i).
\end{align*}
%
The function $\freevar{}$ is usually \enquote{defined} simply by writing down the above two equations, but the precise justification goes through \cref{thm:recursive-definitions} and the functions $h_R$ (or a similar argument). We similarly define the set of \keyword{bound variables} in a UAST by
%
\begin{align*}
    \boundvar{x}
        &= \emptyset, \\
    \boundvar[\big]{\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)}
        &= \bigunion_{i=1}^n \bigl( \seq{x}_i \union \boundvar{a_i} \bigr).
\end{align*}
%
The set of all variables occurring in a UAST $a$ is then $\allvar{a} \defeq \freevar{a} \union \boundvar{a}$. If $\freevar{a} = \emptyset$, then we say that $a$ is \keyword{closed}.


\newpar

For $y,z \in \setVar$ we define the \keyword{renaming map} $a \mapsto a^{y \to z}$ on $\calA$ as follows:
%
\begin{align*}
    x^{y \to z}
        &= \begin{cases}
            z, & x = y, \\
            x, & x \neq y,
        \end{cases} \\
    \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)^{y \to z}
        &= \phi(\seq{x}_1.a_1' ; \ldots ; \seq{x}_n.a_n'),
\end{align*}
%
where $a_i' = a_i$ if $y \in \seq{x}$ and $a_i' = a_i^{y \to z}$ otherwise. We think of this map as renaming all the \emph{free} occurrences of $y$ in a UAST. This way of informally defining the renaming map makes it seem like there is an issue of well-definition: That is, if $a \in \calA$ do we indeed have $a^{y \to z} \in \calA$? But this \enquote{problem} disappears if we formulate the definition in the precise manner prescribed by \cref{thm:recursive-definitions}. For a sequence of variables we also define $(x_1, \ldots, x_n)^{y \to z} = (x_1', \ldots, x_n')$, where $x_i' = z$ if $x_i = y$ and $x_i' = x_i$ if $x_i \neq y$.

If $a$ is well-sorted and we rename a variable in $a$ to a variable of the same sort, then the result should certainly be well-sorted of the same sort, and this is indeed the case:

\begin{lemma}
    \label{lem:renaming-sorts}
    Let $\hassort{a}{s}$, and let $y,z$ be variables of the same sort. Then $\hassort{a^{y \to z}}{s}$.
\end{lemma}

\begin{proof}
    Induction on the definition of UASTs. If $a = x$ is a variable such that $x = y$, then $x$ and $z$ have the same sort, so $x^{y \to z} = z$ has the same sort as $x$. [TODO need that variables have only one sort!] If instead $x \neq y$, then $x^{y \to z} = x$ certainly has the same sort as $x$.

    Assume that $a = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)$ and that the claim holds for $a_1, \ldots, a_n$. Since $a$ is assumed well-sorted with some sort $s$, inversion [TODO ref] on the definition of ASTs implies that $\hassort{a_i}{s_i}$ for appropriate sorts $s_i$. Now consider
    %
    \begin{equation*}
        a^{y \to z}
            = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)^{y \to z}
            = \phi(\seq{x}_1.a_1' ; \ldots ; \seq{x}_n.a_n'),
    \end{equation*}
    %
    where $a_i'$ are as in the definition of renaming. If $a_i' = a_i$, then there is nothing to prove. If instead $a_i' = a_i^{y \to z}$, then by induction $a_i'$ has the same sort as $a_i$. But then [TODO rule AST] implies that $a^{y \to z}$ has the same sort as $a$.
\end{proof}


While the renaming map allows us to rename \emph{free} occurrences of variables in UASTs, this is not usually something we wish to do, first of all since the ASTs we are ultimately interested in will be closed anyway, and secondly because renaming a free variable makes the UAST behave differently with respect to e.g. substitution. Nonetheless, the renaming map allows to define renaming of \emph{bound} occurrences of variables, and the names of bound variables are of course not supposed to be significant. We usually say that two UASTs (or similar syntactic objects, such as terms in the $\lambda$-calculus) are \keyword{$\alpha$-equivalent} if one is obtained from the other by renaming of bound variables. We define the $\alpha$-equivalence relation $=_\alpha$ by the following inference rules:
%
\begin{equation*}
    \inferrule*{
        a_i =_\alpha b
    }{
        \phi(\ldots ; \seq{x}_i.a_i ; \ldots)
            =_\alpha \phi( \ldots ; \seq{x}_{i-1}.a_{i-1} ; \seq{x}_i.b ; \seq{x}_{i+1}.a_{i+1} ; \ldots )
    }
\end{equation*}

\begin{equation*}
    \inferrule*{ }{
        \phi(\ldots ; \seq{x}_i.a_i ; \ldots)
            =_\alpha \phi( \ldots ; \seq{x}_{i-1}.a_{i-1} ; \seq{x}_i^{y \to z}.a_i^{y \to z} ; \seq{x}_{i+1}.a_{i+1} ; \ldots )
    }
\end{equation*}
%
if either $y = z$ [TODO is this necessary??], or else $y \in \seq{x}_i$, $z \not\in \seq{x}_i$ and $z \not\in \allvar{a_i}$. We also require $=_\alpha$ to be an equivalence relation as described in [TODO ref + write it! use old stuff about fixpoints of joins of functions.]


\subsection{Substitution}

\newpar

We also want to be able to substitute one UAST $b$ into another UAST $a$ by replacing a (free) variable $x$ in $a$ with $b$. The resulting UAST is denoted $a[b/x]$, and it is fairly difficult to define precisely, even if the concept is natural. If $a$ is a variable $z$, then this is easy:
%
\begin{equation*}
    z[b/x]
        = \begin{cases}
            b, & z = x, \\
            z, & z \neq x.
        \end{cases}
\end{equation*}
%
However, if $a$ is a more complex UAST, then potential bindings inside $a$ might \enquote{capture} the free variables in $b$, so we must take care that does not happen. On the other hand, if $x$ is not free in $a$ then there is no issue, since then (as it turns out, cf. \cref{lem:substitution-not-free}) $b$ is not substituted into $a$ at all. There are various solutions to this capture problem, and the one we have chosen is to simply not define substitution when it does not make sense. We will see (cf. \cref{lem:substitution-alpha-defined}) that even if we cannot substitute $b$ into $a$, then we can find another UAST $a'$ that is $\alpha$-equivalent to $a$ into which it is possible to substitute $b$. It also turns out (cf. \cref{prop:substitution-alpha-respects}) that if both $a[b/x]$ and $a'[b/x]$ are defined and $a =_\alpha a'$, then we also have $a[b/x] =_\alpha a'[b/x]$. Hence the \emph{partial} substitution function on UASTs induces a \emph{total} substitution function on $\alpha$-equivalence classes of UASTs.

Returning to the definition of substitution, we now define $\phi(\ldots ; \seq{x}_i.a_i ; \ldots)[b/x]$ to be either undefined or $\phi(\ldots ; \seq{x}_i.a_i' ; \ldots)$, where $a_i'$ is to be defined below. For each $i \in \{1, \ldots, n\}$ we do the following:
%
\begin{enumerate}
    \item If $x \in \seq{x}_i$, then $a_i' = a_i$.

    \item Otherwise, if $a_i[b/x]$ is undefined, then $\phi(\ldots ; \seq{x}_i.a_i ; \ldots)[b/x]$ is also undefined.

    \item Otherwise, if both $x \in \freevar{a_i}$ and $\seq{x}_i \intersect \freevar{b} \neq \emptyset$, then $\phi(\ldots ; \seq{x}_i.a_i ; \ldots)[b/x]$ is undefined.

    \item Otherwise it follows that $a_i[b/x]$ is defined, and that either $x \not\in \freevar{a_i}$ or $\seq{x}_i \disj \freevar{b}$. In this case we let $a_i' = a_i[b/x]$.
\end{enumerate}
%
The idea is as follows: If it is impossible to substitute $b$ into any of the arguments to $\phi$, then the entire substitution is undefined. If $x$ is among the variables bound by $\phi$ in the $i$th argument $\seq{x}_i.a_i$, then there is never any issue, since in this case we do not substitute $b$ into $a_i$ at all. Next, if $x \not\in \freevar{a_i}$ then we can also substitute $b$ into $a_i$, since no actual substitution will happen: During the recursion we never reach a free occurrence of $x$. Finally, if $\seq{x}_i \disj \freevar{b}$, then there is no possibility of capture, and we may substitute $b$ into $a_i$.

As mentioned, even if substitution is undefined for some UAST, we can always find an $\alpha$-equivalent UAST into which it is possible to perform substitution.

\begin{lemma}
    \label{lem:substitution-alpha-defined}
    If $a[b/x]$ is undefined, then there is an $a' \in \calA$ with $a =_\alpha a'$ such that $a'[b/x]$ is defined. If $\hassort{a}{s}$, then we may also choose $\hassort{a'}{s}$.
\end{lemma}

\begin{proof}
    We prove this by rule induction on the definition of $\calA$. If $a = x$ is a variable, then we may simply choose $a' = x$. Next assume that the claim holds for UASTs $a_1, \ldots, a_n$, such that $a_i =_\alpha a_i'$ and $a_i'[b/x]$ is defined. Considering the UAST $\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)$, successive applications of [TODO rule] (and transitivity of $=_\alpha$) implies that
    %
    \begin{equation*}
        \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)
            =_\alpha \phi(\seq{x}_1.a_1' ; \ldots ; \seq{x}_n.a_n').
    \end{equation*}
    %
    For each $i \in \{1, \ldots, n\}$, if $x \in \seq{x}_i$ then do nothing for that $i$. Otherwise successively apply [TODO rule] to obtain $\seq{x}_i'$ and $a_i''$ such that
    %
    \begin{equation*}
        \phi( \ldots ; \seq{x}_{i-1}.a_{i-1}' ; \seq{x}_i.a_i' ; \seq{x}_{i+1}.a_{i+1}' ; \ldots )
            =_\alpha \phi( \ldots ; \seq{x}_{i-1}.a_{i-1}' ; \seq{x}_i'.a_i'' ; \seq{x}_{i+1}.a_{i+1}' ; \ldots ),
    \end{equation*}
    %
    and such that $\seq{x}_i' \disj \freevar{b}$. Now substitution is possible.

    For the final claim we note how to modify the above proof to obtain the stronger conclusion. This is first of all clear for variables. Then choose $a_i'$ to have the same sort as $a_i$. Finally choose the new variables in $\seq{x}_i'$ and $a_i''$ to have the same sorts as the old variables, in which case \cref{lem:renaming-sorts} implies that $a_i''$ has the same sort as $a_i'$. Applying [TODO rule] yields the claim.
\end{proof}


\newpar

We next prove some lemmas on renaming and substitution. First, renaming free variables in $a$ changes the set $\freevar{a}$ in a predictable way:

\begin{lemma}
    \label{lem:renaming-freevars}
    $\freevar{a^{y \to z}} \subseteq (\freevar{a} \setminus \{y\}) \union \{z\}$. In particular, if $y \neq z$ then $y \not\in \freevar{a^{y \to z}}$.
\end{lemma}

\begin{proof}
    We prove the claim by rule induction in $a$. If $a = x$ is a variable and $x = y$, then
    %
    \begin{equation*}
        \freevar{x^{y \to z}}
            = \freevar{z}
            = \{z\}
            \subseteq (\freevar{x} \setminus \{y\}) \union \{z\}.
    \end{equation*}
    %
    If instead $x \neq y$, then
    %
    \begin{equation*}
        \freevar{x^{y \to z}}
            = \freevar{x}
            \subseteq (\freevar{x} \setminus \{y\}) \union \{z\},
    \end{equation*}
    %
    since $y \not\in \{x\} = \freevar{x}$. Finally we have
    %
    \begin{equation*}
        \freevar[\big]{\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)^{y \to z}}
            = \freevar[\big]{\phi(\seq{x}_1.a_1' ; \ldots ; \seq{x}_n.a_n')}
            = \bigunion_{i=1}^n (\freevar{a_i'} \setminus \seq{x}_i).
    \end{equation*}
    %
    For each $i$ we either have $y \in \seq{x}_i$ and $\freevar{a_i'} = \freevar{a_i}$, or else $y \not\in \seq{x}_i$ and $a_i' = a_i^{y \to z}$. In the former case $\freevar{a_i'} \setminus \seq{x}_i \subseteq (\freevar{a_i'} \setminus \seq{x}_i) \setminus \{y\}$. In the latter case we have $\freevar{a_i'} \subseteq (\freevar{a_i} \setminus \{y\}) \union \{z\}$, so that $\freevar{a_i'} \setminus \seq{x}_i \subseteq ((\freevar{a_i} \setminus \seq{x}_i) \setminus \{y\}) \union \{z\}$. Hence
    %
    \begin{align*}
        \bigunion_{i=1}^n (\freevar{a_i'} \setminus \seq{x}_i)
            &\subseteq \Bigl(\bigunion_{i=1}^n (\freevar{a_i} \setminus \seq{x}_i) \setminus \{y\} \Bigr) \union \{z\} \\
            &= \Bigl( \freevar[\big]{\phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)} \setminus \{y\} \Bigr) \union \{z\},
    \end{align*}
    %
    as claimed.
\end{proof}


\begin{lemma}
    \label{lem:substitution-not-free}
    If $x \not\in \freevar{a}$, then the substitution $a[b/x]$ is defined and $a[b/x] = a$.
\end{lemma}

\begin{proof}
    The proof is by rule induction on $a$. If $a$ is a variable $z$, then we must have $z \neq x$, and so $z[b/x] = z$.

    Assume instead that $a = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)$ and that the claim holds for $a_1, \ldots, a_n$. For $i \in \{1, \ldots, n\}$, if $x \in \seq{x}_i$ then we do not substitute $b$ into $a_i$, so assume that $x \not\in \seq{x}_i$. By the definition of free variables, $x$ cannot be free in $a_i$ since it is not free in $a$, so by induction $a_i[b/x] = a_i$. The claim follows.
\end{proof}


\begin{lemma}[The substitution lemma]
    \label{lem:substitution-lemma}
    TODO
\end{lemma}


\begin{lemma}
    If $\hassort{a}{s}$, $\hassort{b}{t}$ and $x \in \setVar_t$ such that the substitution $a[b/x]$ is defined, then $\hassort{a[b/x]}{s}$.
\end{lemma}

\begin{proof}
    Rule induction on the definition of ASTs. If $a$ is a variable $z$, then either $z = x$ in which case $a[b/x] = b$ indeed has the same sort as $a$, or else $z \neq x$ in which case $a[b/x] = a$.

    Assume that $a = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)$ and that the claim holds for $a_1, \ldots, a_n$. In this case
    %
    \begin{equation*}
        a[b/x]
            = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)[b/x]
            = \phi(\seq{x}_1.a_1' ; \ldots ; \seq{x}_n.a_n'),
    \end{equation*}
    %
    where each $a_i'$ is either $a_i$ or $a_i[b/x]$. By induction these have the same sort as $a_i$, so the claim follows by an application of [TODO rule].
\end{proof}


\newpar

As promised we now show that substitution respects $\alpha$-equivalence. This is a fairly technical result, and it is not easy to find detailed proofs of this fact in the literature\footnote{\Textcite{harper-pl} simply leaves it as an exercise, cf. Exercise~1.3.}, even for the simpler case of the untyped $\lambda$-calculus.

The main reference on the $\lambda$-calculus, \textcite{barendregt-lambda}, uses the so-called \keyword{variable convention} (cf. his §2.1), in which all bound variables are chosen to be different from the free variables occurring in a given context. Furthermore, $\alpha$-equivalent terms are identified. He afterwards goes on to define substitution, but for substitution to be well-defined in this context it must respect $\alpha$-equivalence. Barendregt relegates this to his Appendix~C: Here he defines substitution anew with using the variable convention and describes how substitution respects $\alpha$-equivalence. However, he simply refers to \textcite[§3.E]{curry-combinatory} for a proof of this result, but the proof that the authors give is rather anachronistic, and as is the nature of proofs of these technical results, it depends highly on the precise way in which the relevant concepts are defined. Hence we give a detailed proof of the claim below for UASTs.

\begin{proposition}
    \label{prop:substitution-alpha-respects}
    If $a =_\alpha a'$ and $b =_\alpha b'$, then $a[b/x] =_\alpha a'[b'/x]$ if the substitutions $a[b/x]$ and $a'[b'/x]$, as well as either $a'[b/x]$ or $a[b'/x]$, are all defined.
\end{proposition}

\begin{proof}
    By \cref{lem:substitution-alpha-1} and \cref{lem:substitution-alpha-2} below, we get either
    %
    \begin{equation*}
        a[b/x]
            =_\alpha a'[b/x]
            =_\alpha a'[b'/x]
    \end{equation*}
    %
    or
    %
    \begin{equation*}
        a[b/x]
            =_\alpha a[b'/x]
            =_\alpha a'[b'/x],
    \end{equation*}
    %
    depending on which substitutions are defined.
\end{proof}


\begin{lemma}
    \label{lem:substitution-alpha-1}
    If $a =_\alpha a'$, then $a[b/x] =_\alpha a'[b/x]$ if both substitutions are defined.
\end{lemma}

\begin{proof}
    We prove this claim by rule induction on the definition of $=_\alpha$.
    %
    \begin{proofsec}
        \item[Reflexivity]
        TODO

        \item[Transitivity]
        TODO

        \item[Symmetry]
        TODO

        \item[First rule [TODO ref]]
        Assume that $a_i[b/x] =_\alpha b_i[b/x]$ for some $i$. Then [TODO rule] implies that
        %
        \begin{align*}
            \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)[b/x]
                &= \phi(\seq{x}_1.a_1[b/x] ; \ldots ; \seq{x}_n.a_n[b/x]) \\
                &=_\alpha \phi(\seq{x}_1.a_1[b/x] ; \ldots ; \seq{x}_i.b_i[b/x] ; \ldots ; \seq{x}_n.a_n[b/x]) \\
                &= \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_i.b_i ; \ldots ; \seq{x}_n.a_n)[b/x].
        \end{align*}

        \item[Second rule [TODO ref]]
        Fix $i \in \{1, \ldots, n\}$. We may assume that $y \in \seq{x}_i$, $z \not\in \seq{x}_i$ and $z \not\in \allvar{a_i}$ [TODO from the rule], so that $y \neq z$ in particular. From the definition of substitution we have four cases:
        %
        \begin{enumerate}
            \item $x \in \seq{x}_i$ and $x \in \seq{x}_i^{y \to z}$: In this case no substitution happens, so this is obvious.

            \item $x \in \seq{x}_i$ and $x \not\in \seq{x}_i^{y \to z}$: By definition of renaming of sequences of variables, this implies that $x = y$. But then $x$ is not free in $a_i^{y \to z}$ by \cref{lem:renaming-freevars}, so \cref{lem:substitution-not-free} implies that $a_i^{y \to z}[b/x] = a_i^{y \to z}$. Since $x \in \seq{x}_i$ we do not substitute $b$ into $a_i$, so the claim follows from [TODO second rule].

            \item $x \not\in \seq{x}_i$ and $x \in \seq{x}_i^{y \to z}$: In this case we must have $x = z$, so $x \not\in \allvar{a_i}$, and in particular $x$ is not free in $a_i$. Hence $a_i[b/x] = a_i$ as before, and since $x \in \seq{x}_i^{y \to z}$ we do not substitute $b$ into $a_i^{y \to z}$. The claim then follows as in the previous case.

            \item $x \not\in \seq{x}_i$ and $x \not\in \seq{x}_i^{y \to z}$: In this case $x$ is neither $y$ nor $z$, so $x \not\in \freevar{a_i}$ if and only if $x \not\in \freevar{a_i^{y \to z}}$ by \cref{lem:renaming-freevars}. The claim thus follows from \cref{lem:substitution-not-free} as above.

            Hence we may assume that both $\seq{x}_i \disj \freevar{b}$ and $\seq{x}_i^{y \to z} \disj \freevar{b}$, and we must show that
            %
            \begin{equation*}
                \phi(\ldots ; \seq{x}_i.a_i[b/x] ; \ldots)
                    =_\alpha \phi( \ldots ; \seq{x}_i^{y \to z}.a_i^{y \to z}[b/x] ; \ldots ).
            \end{equation*}
            %
            An application of [TODO rule] implies that it suffices to show that $a_i^{y \to z}[b/x] =_\alpha a_i[b/x]^{y \to z}$. Notice that $y \in \seq{x}_i$ so $z \in \seq{x}_i^{y \to z}$, and so neither $y$ nor $z$ is free in $b$. By \cref{lem:substitution-lemma} we thus have $a_i^{y \to z}[b/x] = a_i[b/x]^{y \to z}$ as desired.
        \end{enumerate}
    \end{proofsec}
\end{proof}


\begin{lemma}
    \label{lem:substitution-alpha-2}
    If $b =_\alpha b'$, then $a[b/x] =_\alpha a[b'/x]$ if both substitutions are defined.
\end{lemma}

\begin{proof}
    TODO
\end{proof}


\newpar

It turns out to be useful to substitute an AST of one sort for a variable of another sort. This is permitted since we have defined substitution for general UASTs, but we need this operation to respect sorts in certain circumstances. To describe the next result we first define renaming of sorts in arities: If $t,u$ are sorts, then we define [TODO specify symbols]
%
\begin{align*}
    s^{t \to u}
        &\defeq \begin{cases}
            u, & s = t, \\
            s, & s \neq t,
        \end{cases} \\
    (s_1, \ldots, s_k)^{t \to u}
        &\defeq (s_1^{t \to u}, \ldots, s_k^{t \to u}), \\
    (\seq{s}.s)^{t \to u}
        &\defeq \seq{s}^{t \to u}.s^{t \to u}, \\
    \bigl( (v_1; \ldots; v_n)s \bigr)^{t \to u}
        &\defeq (v_1^{t \to u}; \ldots, v_n^{t \to u})s^{t \to u}.
\end{align*}
%
Put simply, substituting one sort $t$ with another $u$ in an arity $\alpha$ just means going through the arity and replacing every occurrence of $t$ with $u$.

Consider now an AST $a$ of sort $t$ with a single free variable $y$ of sort $t$, and assume that all operators in $a$ have two arities of a particular type: Each operator can be thought of as taking arguments of sort $t$ and returning sort $t$, \emph{as well as} taking arguments of sort $u$ and returning sort $u$. In this case we can substitute $y$ for another variable $z$ of sort $u$, and the result $a^{y \to z}$ will then have sort $u$ instead. [TODO refer forward to application]

\newcommand{\allops}[1]{\Phi(#1)}

\begin{lemma}
    Assume that
    %
    \begin{enumlem}
        \item $\hassort{a}{t}$,
        
        \item $a$ has a single free variable $y$ of sort $t$,

        \item $z$ is a variable of sort $u$, and

        \item every operator in $\allops{a}$ [TODO define this] has an arity $\alpha$ which takes precisely one argument of sort $t$, it also has arity $\alpha^{t \to u}$, and it does not bind any variables of sort $t$ [TODO make more precise].
        
        % has both arity $\alpha$ and $\alpha^{t \to u}$, where the arity $\alpha$ binds no variables of sort $t$ [TODO make more precise].
    \end{enumlem}
    %
    Then $\hassort{a^{y \to z}}{u}$.
\end{lemma}

\begin{proof}
    Rule induction on the definition of ASTs. If $a$ is a variable $z$, then the only free variable in $a$ is $z$, and so $z = y$. But then $a^{y \to z} = z$, and this indeed has sort $u$.

    Assume instead that $a = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)$ and that the claim holds for $a_1, \ldots, a_n$. Furthermore, write
    %
    \begin{equation*}
        a^{y \to z}
            = \phi(\seq{x}_1.a_1 ; \ldots ; \seq{x}_n.a_n)^{y \to z}
            = \phi(\seq{x}_1.a_1' ; \ldots ; \seq{x}_n.a_n'),
    \end{equation*}
    %
    where the $a_i'$ are as in the definition of renaming.
    
    
    
    
    By definition of free variables we must have [TODO define notation for sorts]
    %
    \begin{equation*}
        \bigunion_{i=1}^n (\freevarsort{a_i}{t} \setminus \seq{x}_i)
            = \freevarsort{a}{t}
            = \{y\},
    \end{equation*}
    %
    so that $\freevarsort{a_i}{t} \setminus \seq{x}_i \subseteq \{y\}$ for each $i$. If $\freevarsort{a_i}{t} \setminus \seq{x}_i = \emptyset$, then either $y$ is not free in $a_i$, in which case $a_i' = a_i^{y \to z} = a_i$ by \cref{lem:substitution-not-free} [TODO formulate for renaming, not just for substitution], or else $y \in \seq{x}_i$, in which case also $a_i' = a_i$.

    Otherwise we have $\freevarsort{a_i}{t} \setminus \seq{x}_i = \{y\}$. Since $\phi$ binds no variables of sort $t$, $\seq{x}_i$ contains no variables of sort $t$, and so $\freevarsort{a_i}{t} = \{y\}$. Hence we may apply the induction hypothesis to $a_i$, implying that $\hassort{a_i^{y \to z}}{u}$. [TODO TODO TODO]
\end{proof}


\begin{convention}
    Abstract syntax trees that are $\alpha$-equivalent are identified.
\end{convention}


\section{Abstract reduction systems}

\newcommand{\reduce}{\rightarrow}
\newcommand{\reduceR}{\overset{=}{\rightarrow}}
\newcommand{\reduceT}{\overset{+}{\rightarrow}}
\newcommand{\reduceRT}{\overset{*}{\rightarrow}}
\newcommand{\reduceI}{\leftarrow}
\newcommand{\reduceIR}{\overset{=}{\leftarrow}}
\newcommand{\reduceIT}{\overset{+}{\leftarrow}}
\newcommand{\reduceIRT}{\overset{*}{\leftarrow}}
\newcommand{\reduceS}{\leftrightarrow}
\newcommand{\reduceTS}{\overset{+}{\leftrightarrow}}
\newcommand{\reduceRTS}{\overset{*}{\leftrightarrow}}
\newcommand{\joinable}{\downarrow}

Given two relations $R \subseteq X \prod Y$ and $S \subseteq Y \prod Z$, we define their composition
%
\begin{equation*}
    R \circ S
        \defeq \set{(x,z) \in X \prod Z}{\exists y \in Y \colon (x,y) \in R \land (y,z) \in S},
\end{equation*}
%
(note the ordering), and we also define the inverse
%
\begin{equation*}
    R\inv
        \defeq \set{(y,x) \in Y \prod X}{(x,y) \in R}.
\end{equation*}
%
If $X = Y$, then we let $R^0 \defeq \set{(x,x)}{x \in X}$ be the identity relation on $X$ (i.e., equality), and for $n \in \naturals$ we recursively define $R^{n+1} \defeq R^n \circ R$. We further define the following:
%
\begin{alignat*}{2}
    R^= &\defeq R \union R^0 && \quad \text{reflexive closure} \\
    R^+ &\defeq \bigunion_{n\in\naturals^+} R^n && \quad \text{transitive closure} \\
    R^* &\defeq R^+ \union R^0 && \quad \text{reflexive transitive closure}
\end{alignat*}
%
If we denote $R$ by an arrow, e.g., $\reduce$, then we also write ${\reduceR} \defeq (\reduce)^=$, ${\reduceT} \defeq (\reduce)^+$ and ${\reduceRT} \defeq (\reduce)^*$, and we further define
%
\begin{alignat*}{2}
    {\reduceI} &\defeq (\reduce)\inv && \quad \text{inverse} \\
    {\reduceS} &\defeq {\reduce} \union {\reduceI} && \quad \text{symmetric  closure} \\
    {\reduceTS} &\defeq (\reduceS)^+ && \quad \text{transitive symmetric closure} \\
    {\reduceRTS} &\defeq (\reduceS)^* && \quad \text{reflexive transitive symmetric closure}
\end{alignat*}
%
Clearly $\reduceRT$ is a preorder on $X$, while $\reduceRTS$ is an equivalence relation.

An \keyword{abstract reduction system} is simply a pair $(X,\reduce)$, where $X$ is a set and $\reduce$ is a binary relation on $X$. If $X$ is a set of states of a computer program, then we think of $\reduce$ as formalising computation, in the sense that if $e \reduce e'$, then $e$ reduces to $e'$ by performing some sort of computation.

\begin{remarkbreak}
    \begin{enumrem}
        \item We say that $x \in X$ is \keyword{reducible} if there is a $y \in X$ such that $x \reduce y$, and otherwise $x$ is \keyword{irreducible} or \keyword{in normal form}. An element $y \in X$ is \keyword{a normal form} of $x$ if $x \reduceRT y$ and $y$ is in normal form. In the context of programming languages, normal forms are supposed to model the end result of a successful computation.
        
        \item If $x,y \in X$, then $x$ and $y$ are \keyword{joinable} if there is a $z \in X$ with $x \reduceRT z \reduceIRT y$. In this case we write $x \joinable y$. Note that this does not mean that $z$ is the join of $x$ and $y$ with respect to the preorder $\reduceRT$, but simply that $z$ is an upper bound.

        We say that $\reduce$ is \keyword{Church--Rosser} if $x \reduceRTS y$ implies $x \joinable y$ for all $x,y \in X$. Seemingly a weaker property, $\reduce$ is \keyword{confluent} if instead $x \reduceIRT z \reduceRT y$ implies $x \joinable y$, but one can show that these properties are equivalent, cf. \textcite[Theorem~2.1.5]{baader-nipkow-term-rewriting}.

        In a particular state of a computer program we might allow the computation to proceed to multiple different ways. For instance, we might allow the arguments to functions to be evaluated in arbitary order, and if $\reduce$ is confluent then no matter which computations are performed when, it is always possible to continue the computation in such a way that both branches end up the same place. Of course, confluence does not ensure that this happens, it only ensures that it is possible.

        \item We call $\reduce$ \keyword{terminating} if there is no infinite chain $x_1 \reduce x_2 \reduce \cdots$. If every such chain terminates, then $x_1$ must have a normal form. If every $x \in X$ has a normal form, then $\reduce$ is called \keyword{normalising}, and this is clearly a strictly weaker property than being terminating.\footnote{Termination and normalisation are also called \keyword{strong} and \keyword{weak normalisation} respectively.}

        Termination is of course a desirable property, but it is not exhibited by most programming languages. Neither is normalisation. Note the difference between these two properties: Termination says that any computation always terminates, normalisation says that no matter the state of the program, there is a computation which terminates the program, but the program is not required to perform this computation.
    \end{enumrem}
\end{remarkbreak}

\begin{examplebreak}[$\lambda$-calculus]
    In the $\lambda$-calculus there are various ways of defining a notion of reduction (for instance corresponding to call-by-value or call-by-name, cf. \cite[§5.1]{pierce-types}), but the usual is \keyword{full $\beta$-reduction}, which is nondeterministic and allows any subexpression which is not a normal form [TODO redexes?? Bader/Nipkow ch 4 + nederpelt/geuvers] to be reduced. Equipped with this notion of reduction, the untyped $\lambda$-calculus and both the first- and second-order simply typed $\lambda$-calculi are confluent, and so is the calculus of constructions.
    
    However, while the typed $\lambda$-calculi are terminating, the untyped $\lambda$-calculus is not. For instance, when the term $\Omega = (\lambda x.xx)(\lambda x.xx)$ is reduced we simply obtain $\Omega$ itself again. On the other hand, the termination of e.g. simply typed $\lambda$-calculus implies that it is impossible to implement recursive functions using only $\lambda$-abstraction and application (since recursive functions may not terminate), and recursive functions must instead be added to the theory \enquote{by hand}. [TODO refer to later, System F]
    
    We refer to \textcite{nederpelt-geuvers-types} for further discussion and proofs or references to proofs of the above claims.
\end{examplebreak}



\newcommand{\hastype}[5]{%
    \ifstrempty{#1}%
        {%
            \ifstrempty{#2}%
            {%
                #3 \vdash #4 : #5%
            }{%
                #2 \mid #3 \vdash #4 : #5%
            }%
        }{%
            \ifstrempty{#2}%
            {%
                #1 \mid \emptyset \mid #3 \vdash #4 : #5%
            }{%
                #1 \mid #2 \mid #3 \vdash #4 : #5%
            }%
        }%
}

\newcommand{\stotype}[4]{%
    \ifstrempty{#1}%
        {%
            \ifstrempty{#2}%
            {%
                #3 \vdash #4%
            }{%
                #2 \mid #3 \vdash #4%
            }%
        }{%
            \ifstrempty{#2}%
            {%
                #1 \mid \emptyset \mid #3 \vdash #4%
            }{%
                #1 \mid #2 \mid #3 \vdash #4%
            }%
        }%
}



\chapter{General stuff about languages}

We describe the general framework in which we may describe various programming languages, introducing the concepts that will later be defined precisely in the concrete setting of System~F.

\section{Syntax}

We first fix countable sets $\setVar$ of variables and $\setLoc$ of locations. The \keyword{expressions} of the language will be a set $\setExp$ containing both $\setVar$ and $\setLoc$, and we designate some of these expressions to be \keyword{values}, collected in a set $\setVal$. We think of an expression as specifying the state of a program, and values are states in which the computation of the program has finished. All locations will also be values, and these are supposed to model memory addresses. The memory state of the program (i.e. the part of the memory that the program has access to) is modelled by a \keyword{store}\footnote{Sometimes called a \keyword{heap}, but this has nothing to do with the heap \emph{data structure}.}, which is an element of $\pmaps[\omega]{\setLoc}{\setExp}$. We simply write $\setSto$ for this set of partial maps.

For $e \in \setExp$ we define the set $\freevar{e} \subseteq \setVar$ of \keyword{free variables}. There are various ways of binding variables in expressions, and the notion of free variables is supposed to capture the idea that variables can be bound, e.g. by lambda abstraction, and hence also \emph{not} bound. If $\freevar{e} = \emptyset$, then we say that $e$ is \keyword{closed}. Complete programs do not have free variables, or if they do we specify their values before running the program, so we may assume that all programs are closed expressions. We will see the importance of this assumption when we prove a progress theorem for System F in [TODO ref].

When defining the set of expressions of a language, we are strictly speaking defining a concrete syntax for the language. However, while we write expressions as a linear sequence of characters, they are thought of as describing an abstract syntax. But since writing abstract syntax trees quickly becomes impractical, we instead express them using a more convenient linear shorthand. If $e_1$, $e_2$ and $e_3$ are expressions, an expression in the concrete syntax of the language could be $e_1 \oplus e_2 \otimes e_3$, but this might have two different derivations from the grammar and hence correspond to two different abstract syntax trees represented by $(e_1 \oplus e_2) \otimes e_3$ and $e_1 \oplus (e_2 \otimes e_3)$. To disambiguate the expression $e_1 \oplus e_2 \otimes e_3$ we must either introduce precedence and associativity rules (external to the grammar itself), or else rewrite the grammar to be unambiguous. Luckily we do not need to deal with these issues since we will not have to parse programs written in our version of System F. If the need arises, we simply use parentheses to make the structure of the abstract syntax tree clear. [TODO Mogensen?]


\section{Type system}

In order to reason statically about the correctness and safety of a program, we introduce \keyword{types}. Each sufficiently \enquote{well-formed} expression will be assigned a type, and if it is possible to assign a type to an expression, then we say that the expression is \keyword{well-typed}. We specify rules which determine which expressions can be typed based on the types of their subexpressions.

We thus define a set $\setType$ of types. This includes a countable set $\setTVar$ of type variables, any base types (e.g. unit, integer, and boolean types), as well as more complex types that can be constructed recursively from the base types such as function, product, or sum types. It also includes reference types, which are the types of locations. For $\tau \in \setType$ we define the set of \keyword{free type variables} $\freeTvar{\tau} \subseteq \setTVar$ in $\tau$. If $\freeTvar{\tau} = \emptyset$, then we also say that $\tau$ is \keyword{closed}. A pair $(e,\tau)$ of an expression and a type is usually written $e : \tau$. An expression may also have types as subexpressions, for instance if a lambda abstraction has a type annotation on its parameter, though this will not be the case for our version of System F.

If an expression $e$ has free variables, then to assign a type to $e$ it is (usually) necessary to first assign types to the free variables in $e$. This is done using a \keyword{type context}, which is a partial function $\pmaps[\omega]{\setVar}{\setType}$. If $e$ is to be well-typed in a type context $\Gamma$, then we (again usually) require that $\freevar{e} \subseteq \dom \Gamma$. That is, $\Gamma$ must in fact specify the types of the variables that occur free in $e$. Notice that it is possible for an expression to be well-typed in one context but not another. If for instance $e$ is the expression $x + 1$, then the typing rules will probably require the free variable $x$ to have some sort of numeric type in the given type context for $e$ to be well-typed.

Furthermore, if $e$ has a location as a subexpression, then we need to be able to look up the type of the expression stored at this location in order to specify the type of $e$. Hence the type of $e$ can also depend on the store in question. However, it not in general possible to deduce the type of $e$ just by knowing the contents of the store: If $\sigma$ is a store with $l_1,l_2 \in \dom \sigma$, and if $\sigma(l_1)$ references $l_2$ and $\sigma(l_2)$ references $l_1$, then it is impossible to deduce the type of $\sigma(l_1)$. Hence we introduce a \keyword{store typing}, which is an element of $\pmaps[\omega]{\setLoc}{\setType}$ that assigns a type to each location. We of course require that the store typing in question actually contains in its domain all locations referenced in $e$, and we furthermore require that all free variables of $e$ lie in the domain of the current type context.

Since a store $\sigma$ specifies the expressions at each location, and a store typing $\Sigma$ specifies the types of those locations, we of course require $\sigma(l)$ to be of type $\Sigma(l)$. In particular, for $\sigma$ to be well-typed we must have $\dom \sigma \subseteq \dom \Sigma$. [TODO but why equal? Refer to later proofs where we use this]

It may be that the type of $e$ or of the types of the variables in the type context $\Gamma$ or of the expressions in the store typing $\Sigma$ contain type variables. In order to keep track of these we collect these in a (finite) set $\Xi$ and require that the free type variables in $\Gamma$ and $\Sigma$, defined by
%
\begin{equation*}
    \freeTvar{\Gamma}
        = \bigunion_{\tau \in \ran \Gamma} \freeTvar{\tau}
    \quad \text{and} \quad
    \freeTvar{\Sigma}
        = \bigunion_{\tau \in \ran \Sigma} \freeTvar{\tau},
\end{equation*}
%
are contained in $\Xi$. A variable $x$ is called \keyword{fresh} for $\Gamma$ if $x \not\in \dom \Gamma$. The finitude of $\dom \Gamma$ ensures that there always exist fresh variables (recall that there are countably infinitely many variables). If $\Delta$ is another type context such that $\dom \Gamma \intersect \dom \Delta = \emptyset$, then instead of $\Gamma \union \Delta$ we simply write $\Gamma,\Delta$. Furthermore, if $\Delta = \{x_1 : \tau_1, \ldots, x_n : \tau_n\}$ for distinct $x_i$, then we omit the braces and write $\Gamma, x_1 : \tau_1, \ldots, x_n : \tau_n$. If $\Xi$ and $\Phi$ are finite disjoint subsets of $\setTVar$, then we similarly write $\Xi,\Phi$ for $\Xi \union \Phi$, and if $\Phi = \{X_1, \ldots, X_n\}$ for distinct $X_i$, then we also write $\Xi, X_1, \ldots, X_n$.

We are now ready to describe the semantics of the type system precisely. The semantics is captured by a subset of the set
%
\begin{equation*}
    \powersetfin{\setTVar}
        \prod (\pmaps[\omega]{\setVar}{\setType})
        \prod (\pmaps[\omega]{\setLoc}{\setType})
        \prod \setExp
        \prod \setType,
\end{equation*}
%
where an element $(\Xi,\Gamma,\Sigma,e,\tau)$ of this set is written $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and is called a \keyword{type derivation}. If $\Xi = \emptyset$, then we simply denote the above element by $\hastype{}{\Gamma}{\Sigma}{e}{\tau}$ [TODO do we need this?], and we furthermore write $\hastype{}{}{\Sigma}{e}{\tau}$ if also $\Gamma = \emptyset$.

Say that we have somehow settled on a semantics for the type system, i.e. a subset of the above product. We do not often refer to this set explicitly, but let us temporarily denote it $\calT$. Usually $\calT$ is defined recursively, by specifying a series of inference rules. Indeed, these inference rules represent a generating function, and $\calT$ will be the least fixed-point of this function.

If a type derivation $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ lies in $\calT$, then we say that $e$ is \keyword{well-typed} in $\Xi,\Gamma,\Sigma$ with type $\tau$.


\section{Dynamics}

The operational semantics of the language is specified in a small-step style. We describe this semantics in stages, beginning with the \keyword{pure head reductions}. These are evaluations that can be performed (1) on expressions that have no subexpressions that can be evaluated, (2) without reading or modifying the store. More precisely, we specify a binary relation $\purestep$ on $\setExp$, such that $e \purestep e'$ is supposed to mean that $e$ evaluates to or reduces to $e'$.

Going one level up we define the relation $\headstep$ on $\setSto \prod \setExp$ of (not necessarily pure) \keyword{head reductions}. These are reductions that may affect and be affected by the contents of the store. Of course, if $\sigma$ is a store and $e \purestep e'$, then we have $(\sigma,e) \headstep (\sigma,e')$, but we augment the pure head reductions with reductions that e.g. read from or write to the store.

Finally we need a way to evaluate complex expressions. One way of doing this is to specify the evaluation rules for all expressions immediately instead of going through head reductions (this is the approach taken by Pierce TODO). Another is to introduce \keyword{evaluation contexts}, which are (essentially) maps $\setExp \to \setExp$. If $K$ is an evaluation context and $e$ is an expression, then we write $K[e]$ for the value of $K$ at $e$. We then define the final reduction relation $\step$ on $\setSto \prod \setExp$ by letting $(\sigma, K[e]) \step (\sigma', K[e'])$ if $(\sigma,e) \headstep (\sigma',e')$.

One role of evaluation contexts is to specify the evaluation order of complex expressions, e.g. if the evaluation of function applications is call-by-value or call-by-name, or if we evaluate the arguments to functions left-to-right or right-to-left. The possibilities thus depend on the available evaluation contexts.

TODO multiple threads


\chapter{System F}

\newcommand{\derives}{\Rightarrow}
\newcommand{\derivesmany}{\Rightarrow^*}

The following grammar defines the syntax, the sets of values and types, and the evaluation contexts of System F:
%
\begin{alignat*}{2}
    && x &\in \setVar \\
    && l &\in \setLoc \\
    && X &\in \setTVar \\
    & \setExp \quad & e &\Coloneqq \objUnit \mid x \mid l \mid \objPair{e}{e} \mid \cdots \\
    & \setVal \quad & v &\Coloneqq \objUnit \mid l \mid \objPair{v}{v} \mid \objInl{v} \mid \cdots \\
    & \setType \quad & \tau &\Coloneqq \typeUnit \mid X \mid \typeRef{\tau} \mid \tau \prod \tau \mid \cdots \\
    & \setECtx \quad & K &\Coloneqq \hole \mid \objPair{K}{e} \mid \objPair{v}{K} \mid \cdots
\end{alignat*}
%
We first note some difficulties with this and similar definitions. Recall that a \keyword{grammar} is a tuple $G = (V,\Sigma,P,S)$, where $V$ is a finite set of variables or non-terminal symbols, $\Sigma$ is a finite set of terminal symbols that is disjoint from $V$, $P$ is a finite rewriting system\footnote{A \keyword{rewriting system} $P$ on a set $A$ is a binary relation on $A^*$, i.e. a subset of $A^* \prod A^*$. An element of $P$ is called a \keyword{production}. If $(\alpha,\beta) \in P$ and $\gamma,\delta \in A^*$, then we write $\gamma \alpha \delta \derives \gamma \beta \delta$. This defines another binary relation $\derives$ on $A^*$, the reflexive and transitive closure of which is denoted $\derivesmany$.} on $V \union \Sigma$, and $S \in V$ is the start symbol. The language generated by $G$ is the language $L(G) = \set{\alpha \in \Sigma^*}{S \derivesmany \alpha}$. We say that $G$ is \keyword{context-free} if every production in $P$ is on the form $A \derives \alpha$, where $A \in V$.

Notice especially that a grammar must only contain \emph{finitely} many productions, but also that the above \enquote{grammar} defining System F\footnote{We put the word \enquote{grammar} in scare quotes since it is not actually a grammar.} as written has infinitely many productions: For instance, there is a production $(e,x)$ for each of the infinitely many variables $x \in \setVar$. While this poses no problems for the abstract study of System F that we are undertaking, we note that it is (or at least in practice it will be) possible to rewrite the \enquote{grammar} so that it has only finitely many productions. We may for instance define the countably many variables recursively by the grammar with productions
%
\begin{equation*}
    x \Coloneqq \objlang{x} \mid x\objlang{'},
\end{equation*}
%
yielding the countably many variables $\objlang{x}$, $\objlang{x'}$, $\objlang{x''}$, and so on. Including these productions instead of the postulate \enquote{$x \in \setVar$} would solve at least this problem. We can do similarly for locations and type variables.

Next we notice that the \enquote{grammar} has no obvious start symbol. Indeed, we may take any of $e$, $v$, $\tau$ or $K$ to be the start symbol, depending on the type of string we wish to construct.

With these issues dealt with, we now relate grammars and the languages they generate to the results from TODO ref, and in particular to inference rules. In fact, since the \enquote{grammar} defining System F is not a proper grammar, we consider more broadly \keyword{infinite grammars}. An infinite grammar is just a grammar $G = (V,\Sigma,P,S)$, except that we allow $V$, $\Sigma$ and $P$ to be infinite. Given $G$ we construct a collection of inference rules as follows: First add the axiom
%
\begin{equation*}
    \inferrule*{
    }{
        S
    }
\end{equation*}
%
saying that we can always derive the start symbol $S$. Next, for every production $(\alpha,\beta) \in P$ and strings $\gamma,\delta \in (V \union \Sigma)^*$ we add the rule
%
\begin{equation*}
    \inferrule*{
        \gamma\alpha\delta
    }{
        \gamma\beta\delta
    }
\end{equation*}
%
Denote the resulting collection of inference rules $\calR_G$.

\begin{proposition}
    $\mu \calR_G = \set{\alpha \in (V \union \Sigma)^*}{S \derivesmany \alpha}$. In particular, TODO no variables % TODO: Define \mu \calR
\end{proposition}

\begin{proof}
    Denote the set $\set{\alpha \in (V \union \Sigma)^*}{S \derivesmany \alpha}$ by $\tilde{L}(G)$. We prove the inclusion \enquote{$\subseteq$} by rule induction, cf. \cref{thm:rule-induction}.
\end{proof}

TODO if all rules have one antecedent, maybe let that define a relation (axioms don't give an element of the relation) + show about transitive closure?



\begin{lemma}
    $\setVal \subseteq \setExp$.
\end{lemma}

\begin{proof}
    
\end{proof}

Notice that $\setLoc \subseteq \setVal \subseteq \setExp$ as we required in TODO ref. If $K$ is an evaluation context and $e$ is an expression, then we define $K[e]$ recursively by
%
\begin{align*}
    \hole[e] &= e \\
    \objPair{K}{e'}[e] &= \objPair{K[e]}{e'} \\
    \objPair{v}{K}[e] &= \objPair{v}{K[e]} \\
    & etc.
\end{align*}
%
It is easy to prove (by induction in $K$) that $K[e]$ is an expression, so every evaluation context $K$ can indeed be thought of as a map $\setExp \to \setExp$ given by $e \mapsto K[e]$.

If $e$ is an expression, then the set $\freevar{e}$ of free variables in $e$ is defined recursively as follows:
%
\begin{align*}
    \freevar{\objUnit} &= \emptyset \\
    \freevar{x} &= \{x\} \\
    \freevar{\objPair{e_1}{e_2}} &= \freevar{e_1} \union \freevar{e_2} \\
    & etc.
\end{align*}
%
Similarly, if $\tau$ is a type, then we define the set $\freeTvar{\tau}$ of free type variables in $\tau$ as follows:
%
\begin{align*}
    \freeTvar{\typeUnit} &= \emptyset \\
    \freeTvar{X} &= \{X\} \\
    \freeTvar{\tau_1 \prod \tau_2} &= \freeTvar{\tau_1} \union \freeTvar{\tau_2} \\
    & etc.
\end{align*}

We are now in a position to define the typing relation. This is the smallest relation on the set
%
\begin{equation*}
    \powersetfin{\setTVar}
        \prod (\pmaps[\omega]{\setVar}{\setType})
        \prod (\pmaps[\omega]{\setLoc}{\setType})
        \prod \setExp
        \prod \setType,
\end{equation*}
%
satisfying the following inference rules:

\begin{equation*}
    \inferrule*[right=T-var]{
        \freeTvar{\Gamma} \subseteq \Xi
        \and
        \freeTvar{\Sigma} \subseteq \Xi
        \and
        (x : \tau) \in \Gamma
    }{
        \hastype{\Xi}{\Gamma}{\Sigma}{x}{\tau}
    }
\end{equation*}


\begin{equation*}
    \inferrule*[right=T-loc]{
        \freeTvar{\Gamma} \subseteq \Xi
        \and
        \freeTvar{\Sigma} \subseteq \Xi
        \and
        l \in \dom \Sigma
    }{
        \hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\Sigma(l)}}
    }
\end{equation*}


Lemma: If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, then $\freeTvar{\Gamma} \subseteq \Xi$ and $\freevar{e} \subseteq \dom \Gamma$. In particular, if $\Xi = \emptyset$ then $\tau$ is closed, and if $\Gamma = \emptyset$ then $e$ is closed. TODO

TODO all type variables in tau also in Xi? All locations in e also in Sigma?

TODO lemma weakening?


\section{Lemmas}

\begin{lemma}[Inversion]
    \label{lem:inversion}
    Assume that $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$.
    %
    \begin{enumlem}
        \item\label{enum:inversion-variable} If $e = x$ is a variable, then $(x : \tau) \in \Gamma$.
        
        \item\label{enum:inversion-unit} If $e = \objUnit$, then $\tau = \typeUnit$.

        \item\label{enum:inversion-pair} If $e = \objPair{e_1}{e_2}$, then $\tau = \tau_1 \prod \tau_1$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau_2}$.

        \item\label{enum:inversion-fst} If $e = \objFst{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau \prod \tau_2}$.
        
        \item\label{enum:inversion-snd} If $e = \objSnd{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_1 \prod \tau}$.
        
        \item\label{enum:inversion-inl} If $e = \objInl{e'}$, then $\tau = \tau_1 + \tau_2$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_1}$.
        
        \item\label{enum:inversion-inr} If $e = \objInr{e'}$, then $\tau = \tau_1 + \tau_2$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_2}$.

        \item\label{enum:inversion-match} If $e = \objMatch{e_1}{x}{e_2}{e_3}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1 + \tau_2}$ and $\hastype{\Xi}{\Gamma, x : \tau_1}{\Sigma}{e_2}{\tau}$ and $\hastype{\Xi}{\Gamma, x : \tau_2}{\Sigma}{e_3}{\tau}$.

        \item\label{enum:inversion-rec} If $e = \objRec{f}{x}{e'}$, then $\tau = \tau_1 \to \tau_2$ and $\hastype{\Xi}{\Gamma, f : \tau_1 \to \tau_2, x : \tau_1}{\Sigma}{e'}{\tau_2}$.

        \item\label{enum:inversion-app} If $e = \objApp{e_1}{e_2}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1 \to \tau}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau}$.

        \item\label{enum:inversion-forall} If $e = \objForall{X}{e'}$, then $\tau = \typeForall{X}{\tau'}$ and $\hastype{\Xi,X}{\Gamma}{\Sigma}{e'}{\tau'}$.
        
        \item\label{enum:inversion-tapp} If $e = \objTapp{e'}{X}$, then $\tau = \tau'[\tau''/X]$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\typeForall{X}{\tau'}}$.

        \item\label{enum:inversion-pack} If $e = \objPack{e'}$, then $\tau = \typeExists{X}{\tau'}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau'[\tau''/X]}$.

        \item\label{enum:inversion-unpack} If $e = \objUnpack{e_1}{x}{e_2}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\typeExists{X}{\tau'}}$ and $\hastype{\Xi,X}{\Gamma, x \colon \tau'}{\Sigma}{e_2}{\tau}$.

        \item\label{enum:inversion-fold} fold TODO

        \item\label{enum:inversion-unfold} unfold TODO

        \item\label{enum:inversion-location} If $e = l$ is a location, then $l \in \dom \Sigma$ and $\tau = \typeRef{\Sigma(l)}$.

        \item\label{enum:inversion-ref} If $e = \objRef{e'}$, then $\tau = \typeRef{\tau'}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau'}$.

        \item\label{enum:inversion-ass} If $e = \objAss{e_1}{e_2}$, then $\tau = \typeUnit$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\typeRef{\tau'}}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau'}$.

        \item\label{enum:inversion-load} If $e = \objLoad{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\typeRef{\tau}}$.
    \end{enumlem}
\end{lemma}

\begin{proof}
    Notice that since the conclusions of different inference rules are distinct, there is a unique rule that was applied last in the derivation of $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ [TODO ref what that means]. This means that if $e$ has any of the above forms, then it must have the type as assigned by the relevant inference rule, and the assumptions of that rule must also hold.

    For instance, if there exist expressions $e_1$ and $e_2$ such that $e = \objPair{e_1}{e_2}$, then the last rule applied must be \infrule{T-pair}. But then $\tau$ must be on the form $\tau_1 \prod \tau_2$ for types $\tau_1$ and $\tau_2$. And furthermore, the assumptions must also hold, implying that $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau_2}$. The other cases are proved in the same way.
\end{proof}

Notice the significance of the inversion lemma: While an expression can generally have many different types (if nothing else then due to substitution into type variables), it seems natural to believe that e.g. pairs cannot be of function type. The inversion lemma says precisely this, that if a pair has any type, then that type must be a product type. Note that the lemma does \emph{not} just say that a well-typed pair is of product type, it rather says that a well-typed pair is \emph{only} of product type.


\begin{lemma}[Canonical forms]
    \label{lem:canonical}
    Assume that $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau}$ where $v$ is a value.
    %
    \begin{enumlem}
        \item\label{enum:canonical-unit} If $\tau = \typeUnit$, then $v = \objUnit$.

        \item\label{enum:canonical-product} If $\tau = \tau_1 \prod \tau_2$, then $v = (v_1,v_2)$.
        
        \item\label{enum:canonical-sum} If $\tau = \tau_1 + \tau_2$, then either $v = \objInl{v'}$ or $v = \objInr{v'}$.

        \item\label{enum:canonical-function} If $\tau = \tau_1 \to \tau_2$, then $v = \objRec{f}{x}{e}$.

        \item\label{enum:canonical-forall} If $\tau = \typeForall{X}{\tau'}$, then $v = \objForall{X}{e}$.

        \item\label{enum:canonical-exists} If $\tau = \typeExists{X}{\tau'}$, then $v = \objPack{e}$.

        \item\label{enum:canonical-recursive} TODO fold

        \item\label{enum:canonical-ref} If $\tau = \typeRef{\tau'}$, then $v$ is a location.
    \end{enumlem}
\end{lemma}

\begin{proof}
    We assume that $\tau = \tau_1 \prod \tau_2$ for concreteness; the other cases are identical. In this case we simply check for each production of the grammar with non-terminal $v$ whether the relevant value can have type $\tau_1 \prod \tau_2$. For instance, \cref{enum:inversion-inl} implies that a value $\objInl{v'}$ can only be of sum type, and hence $v$ cannot be of this form. The only possibility is that $v$ is in fact a pair.
\end{proof}


--- TODO substitution lemma


\begin{lemma}
    If $\hastype{\Xi,X}{\Gamma}{\Sigma}{e}{\tau}$, then $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{e}{\tau[\tau'/X]}$.
\end{lemma}

\begin{proof}
\begin{proofsec*}
    \item[\infrule{T-var}]
    Assume that $\hastype{\Xi,X}{\Gamma}{\Sigma}{x}{\tau}$. By \cref{enum:inversion-variable} we thus have $(x : \tau) \in \Gamma$, so $(x : \tau[\tau'/X]) \in \Gamma[\tau'/X]$ by definition of substitution. But then \infrule{T-var} implies that $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{x}{\tau[\tau'/X]}$ (where we use that $X$ does not occur in the context or type, so it doesn't need to appear in $\Xi$).

    \item[\infrule{T-rec}]
    Assume that $\hastype{\Xi,X}{\Gamma}{\Sigma}{\objRec{f}{x}{e}}{\tau_1 \to \tau_2}$. By the inversion lemma [TODO ref -- also can't we just do induction??] we have $\hastype{\Xi,X}{\Gamma, f : \tau_1 \to \tau_2, x : \tau_1}{\Sigma}{e}{\tau_2}$, so by induction it follows that $\hastype{\Xi}{\Gamma[\tau'/X], f : \tau_1[\tau'/X] \to \tau_2[\tau'/X], x : \tau_1[\tau'/X]}{\Sigma}{e}{\tau_2[\tau'/X]}$ [TODO by substitution on envs, function types etc.]. Applying \infrule{T-rec} we obtain the desired claim.
\end{proofsec*}

TODO rest

% Antag Ξ, X | Γ ⊢ rec f(x) := e : τ₁ → τ₂ og vis Ξ | Γ[τ'/X] ⊢ rec f(x) := e : (τ₁ → τ₂)[τ'\X]
% Per inversion i antagelsen ved vi, at Ξ, X | Γ, f : τ₁ → τ₂, x : τ₁ ⊢ e : τ₂. Fra induktionshypotesen konkludrer vi så Ξ | Γ[τ'\X], f : τ₁[τ'\X] → τ₂[τ'\X], x : τ₁[τ'\X] ⊢ e : τ₂[τ'\X] idet (τ₁ → τ₂)[τ'\X] = τ₁[τ'\X] → τ₂[τ'\X] og da definitionen af substitution på miljøjer giver os, at (Γ, x : τ)[τ'\X] = Γ[τ'\X], x : τ[τ'\X]. Dernæst følger udsagnet, som vi vil vise direkte af T-rec.

% De interessante tilfælde er (surprise, surprise) derimod for T-Tlam og T-Ttapp.

% T-Tlam 
% Antag Ξ, X | Γ ⊢ Λ e : ∀ Y . τ og vis at Ξ | Γ[τ'\X] ⊢ Λ e : (∀ Y . τ)[τ'/X]
% Vær spids på, at jeg her udnytter, at vi ræsonnerer modulo "alpha-ækvivalens", dvs. at vi kan omnavngive (type)variable frit. Det gør jeg ved at gøre brug en frisk variabel Y og ræsonere med en implicit antagelse om at X ≠ Y.
% Per inversion i antagelsen får vi, at Ξ, X, Y | Γ ⊢ e : τ. Husk, at typemiljøer er mængder, dvs Ξ, X, Y = Ξ, Y, X, og dermed gælder (Ξ, Y), X | Γ ⊢ e : τ også. Brug nu induktionshypotesen med (Ξ, Y) som miljø, og vi får Ξ, Y | Γ[τ'\X] ⊢ e : τ[τ'\X]. Vi kan nu konkludere vha. T-Tlam og da (∀ Y . τ)[τ'/X] = ∀ Y . τ[τ'\X].

% T-Ttapp
% Antag Ξ, X | Γ ⊢ e _ : τ[τ''\Y] og vis at Ξ | Γ[τ'\X] ⊢ e _ : τ[τ''\Y][τ'\X]. Igen, vær opmærksom på, at jeg er meget påpasselig med variabelnavnene!
% Per inversion i antagelsen får vi, at Ξ, X | Γ ⊢ e : ∀ Y . τ. Fra induktionhypotesen følger at Ξ | Γ[τ'\X] ⊢ e : (∀ Y . τ)[τ'\X]. Vi bruger nu igen, at (∀ Y . τ)[τ'/X] = ∀ Y . τ[τ'\X] og konkludrerer, at Ξ | Γ[τ'\X] ⊢ e : ∀ Y . τ[τ'\X]. Fra T-Tapp følger nu, at Ξ | Γ[τ'\X] ⊢ e _ : τ[τ'\X][τ''\Y]. Vi er færdige antaget τ[τ'\X][τ''\Y] = τ[τ'\Y][τ''\X], hvilket er tilfældet da X ≠ Y.

% Som I kan se er det altså forholdvist ligetil at vise, givet et par egenskaber omkring substitution, som afhænger lidt af, hvordan vi helt præcist formaliserer denne (som diskuteret i TAPL). Men det er helt fint at ræsonnere, som jeg gør ovenfor.

% For at opresumere er planen for næste gang dermed:
% - Færdiggør preservation for System F (vha typesubstitutionslemmaet ovenfor)
% - Vis progress + preservation for System F + rekursive typer (dette burde være rimeligt ligetil vha. ovenstående)
% - Kig på referencer og kom så langt I kan med progress + preservation for denne udvidelse.
\end{proof}


\section{Progress}

\begin{theorem}[Progress]
    If $\hastype{}{}{\Sigma}{e}{\tau}$, then either $e$ is a value or else, for any store $\sigma$ with $\stotype{}{}{\Sigma}{\sigma}$, there exists an expression $e'$ and a store $\sigma'$ such that $(\sigma,e) \step (\sigma',e')$.
\end{theorem}

\begin{proof}
The proof is by induction on the typing relation $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, but the claim to be proved is augmented by \textquote{or either $\Xi$ or $\Gamma$ is non-empty}.\footnote{This ensures that we can perform the induction on the entire $4$-ary relation, which is important since this relation is the one that is defined by the inference rules, \emph{not} the corresponding binary relation obtained by restricting the ternary relation to the subset where $\Xi$ and $\Gamma$ are empty.} Notice that the induction step for each inference rule is trivial if $\Xi$ or $\Gamma$ is non-empty, so we need only prove each case when $\Xi$ and $\Gamma$ are empty. Furthermore, since the store is relevant for only a few reductions, we suppress it from the notation in most of the cases below, simply taking about expressions reducing to other expressions.
%
\begin{proofsec}
    \item[\infrule{T-unit}]
    Since $\objUnit$ is a value, this follows.

    \item[\infrule{T-var}]
    Since we may assume that $\Gamma$ is empty, this implication is vacuously true.\footnote{In the formalism of TODO ref \infrule{T-var} would not be an inference rule, it would instead be an axiom requiring the relation to include all quadruples $(\Xi, \Gamma, e, \tau)$ such that $(e : \tau) \in \Gamma$ and all type variables in $\tau$ occur in $\Xi$. This is of course also vacuously true when $\Gamma$ is empty.}

    \item[\infrule{T-pair}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1}$ and $\hastype{}{}{\Sigma}{e_2}{\tau_2}$. If both $e_1$ and $e_2$ are values, then $\objPair{e_1}{e_2}$ is also a value, so assume that only $e_1 = v_1$ is a value and that $e_2 \step e_2'$. Since the only rule that generates instances of the one-step relation $\to$ is \infrule{head-step-step}, it follows that $e_2$ is on the form $K[d_2]$ and $e_2'$ is on the form $K[d_2']$, where $K$ is an evaluation context and $d_2$ and $d_2'$ are subexpressions of $e_2$ and $e_2'$ respectively such that $d_2 \headstep d_2'$. Letting $K' = \objPair{v_1}{K}$ it follows that $\objPair{v_1}{e_2} = K'[d_2]$ and $\objPair{v_1}{e_2'} = K'[d_2']$, and so
    %
    \begin{equation*}
        \objPair{v_1}{e_2}
            = K'[d_2]
            \step K'[d_2']
            = \objPair{v_1}{e_2'}
    \end{equation*}
    %
    by \infrule{head-step-step}. If instead $e_1$ is not a value, then the same argument (using the evaluation context $\objPair{K}{e_2}$) yields the same result.

    \item[\infrule{T-fst}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau_1 \prod \tau_2}$. If $e$ is a value, then it is on the form $\objPair{v_1}{v_2}$ by \cref{enum:canonical-product}, where $v_1$ and $v_2$ are values. Hence $\objFst{e} = \objFst{\objPair{v_1}{v_2}}$, and this reduces via a head-step to $v_1$. Choosing the evaluation context $K = \hole$, \infrule{head-step-step} implies that $\objFst{\objPair{v_1}{v_2}} \step v_1$. If instead $e$ is not a value, then by induction there is some $e'$ such that $e \to e'$. Hence there are subexpressions $d$ and $d'$ and an evaluation context $K$ such that $e = K[d]$, $e' = K[d']$ and $d \headstep d'$. Letting $K' = \objFst{K}$ we have
    %
    \begin{equation*}
        \objFst{e}
            = K'[d]
            \step K'[d']
            = \objFst{e'},
    \end{equation*}
    %
    as desired.

    \item[\infrule{T-snd}]
    Similar to \infrule{T-fst}.

    \item[\infrule{T-inj1}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau_1}$. If $e$ is a value $v$, then so is $\objInl{v}$. If instead $e \step e'$, then as before $e = K[d]$, $e' = K[d']$ and $d \headstep d'$. Letting $K' = \objInl{K}$ we get $\objInl{e} = K'[d]$ and $\objInl{e'} = K'[d']$, so $\objInl{e} \step \objInl{e'}$.

    \item[\infrule{T-inj2}]
    Similar to \infrule{T-inj1}.

    \item[\infrule{T-match}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1 + \tau_2}$. If $e_1$ is a value, then by \cref{enum:canonical-sum} it must be on the form $\objInl{v}$ or $\objInr{v}$ for a value $v$. Hence the expression $\objMatch{e_1}{x}{e_2}{e_3}$ can reduce via a head step by either \infrule{E-match-inj1} or \infrule{E-match-inj2}, so it reduces by \infrule{head-step-step} (using the evaluation context $K = \hole$). If instead $e_1 \step e_1'$, then by the same argument as in previous cases with $K' = \objMatch{K}{x}{e_2}{e_3}$, it follows that $\objMatch{e_1}{x}{e_2}{e_3}$ reduces.

    \item[\infrule{T-rec}]
    This is obvious since $\objRec{f}{x}{e}$ is a value.

    \item[\infrule{T-app}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1 \to \tau_2}$ and $\hastype{}{}{\Sigma}{e_2}{\tau_1}$. If $e_1$ is a value, then by \cref{enum:canonical-function} it must be on the form $\objRec{f}{x}{e}$. If also $e_2$ is a value, then the claim follows by \infrule{E-rec-app}. If $e_1 = v$ is a value but $e_2$ is not, then $e_2 \to e_2'$. The same argument as in previous cases with $K' = \objApp{v}{K}$ shows that $\objApp{v}{e_2}$ reduces. Finally, if $e_1$ is not a value, then $e_1 \to e_1'$, and choosing $K' = \objApp{K}{e_2}$ proves the claim.

    \item[\infrule{T-Tlam}]
    This is obvious since $\objForall{X}{e}$ is a value.

    \item[\infrule{T-Tapp}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\typeForall{X}{\tau}}$. If $e$ is a value, then by \cref{enum:canonical-forall} it must be on the form $\objForall{X}{e'}$, so the claim follows from \infrule{E-tapp-tlam} (via \infrule{head-step-step} using the evaluation context $K = \hole$). If $e$ is not a value, then $e \to e'$ for some expression $e'$ by induction. These expressions then have subexpressions $d$ and $d'$ respectively such that $d \headstep d'$, and such that $e = K[d]$ and $e' = K[d']$ for some evaluation context $K$. Letting $K' = \objTapp{K}{\tau'}$ we thus have $\objTapp{e}{\tau'} = K'[d]$ and $\objTapp{e'}{\tau'} = K'[d']$, proving the claim.

    \item[\infrule{T-pack}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau[\tau'/X]}$. If $e$ is a value, then so is $\objPack{e}$. Otherwise $e \step e'$ for some expression $e'$. The same argument as before using the evaluation context $\objPack{K}$ for an appropriate $K$ yields the claim.

    \item[\infrule{T-unpack}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\typeExists{X}{\tau}}$. If $e_1$ is a value, then it must be on the form $\objPack{v}$ by \cref{enum:canonical-exists}, so an application of \infrule{E-unpack-pack} yields the claim. Otherwise $e \step e'$ for some $e'$, and we use the evaluation context $\objUnpack{K}{x}{e_2}$ for an appropriate $K$.

    \item[\infrule{T-fold}]
    TODO

    \item[\infrule{T-unfold}]
    TODO

    \item[\infrule{T-loc}]
    Locations are values, so this is obvious.

    \item[\infrule{T-alloc}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau}$. If $e$ is a value, then the claim follows by applying \infrule{E-alloc}, noting that there always exists a location $l \not\in \dom \sigma$. Otherwise there is an expression $e'$ and a store $\sigma'$ such that $(\sigma,e) \step (\sigma',e')$. But then there is some evaluation context $K$ and subexpressions $d$ and $d'$ of $e$ and $e'$ respectively, such that $e = K[d]$, $e' = K[d']$ and $(\sigma,d) \headstep (\sigma',d')$. Letting $K' = \objRef{K}$ we have $\objRef{e} = K'[d]$ and $\objRef{e'} = K'[d']$, so \infrule{head-step-step} implies that $(\sigma,\objRef{e}) \step (\sigma',\objRef{e'})$.

    \item[\infrule{T-store}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\typeRef{\tau}}$ and $\hastype{}{}{\Sigma}{e_2}{\tau}$, and let $\sigma$ be a store with $\stotype{}{}{\Sigma}{\sigma}$. If $e_1$ is a value, then by \cref{enum:canonical-ref} it is a location $l$, and by \cref{enum:inversion-location} we have $l \in \dom \Sigma = \dom \sigma$. If $e_2$ is also a value, then the claim follows from \infrule{E-store}. If $e_1 = l$ is a value but $e_2$ is not, then there is some $e_2'$ and $\sigma'$ such that $(\sigma,e_2) \step (\sigma',e_2')$. Again writing $e_2 = K[d_2]$ and $e_2' = K[d_2']$ with $(\sigma,d) \headstep (\sigma',d')$, we use the evaluation context $\objAss{l}{K}$. Finally, if $e_1$ is not a value, then the same argument using instead $\objAss{K}{e_2}$ yields the claim.

    \item[\infrule{T-load}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\typeRef{\tau}}$, and let $\sigma$ be a store with $\stotype{}{}{\Sigma}{\sigma}$. If $e$ is a value, then as before it is a location $l$, and $l \in \dom \sigma$. It then follows from \infrule{E-load} that $(\sigma, \objLoad{l}) \headstep (\sigma,v)$, where $v = \sigma(l)$. If instead $(\sigma,e) \step (\sigma',e')$ for an expression $e'$ and a store $\sigma'$, then we simply use the evaluation context $\objLoad{K}$ for an appropriate $K$.
\end{proofsec}
\end{proof}


\section{Preservation}

If $\Xi \subseteq_\omega \setTVar$, $\Gamma$ is a type context and $\Sigma$ is a store typing, then we say that a store $\sigma$ is \keyword{well-typed} with respect to $\Xi$, $\Gamma$ and $\Sigma$ if $\dom \sigma = \dom \Sigma$ and $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$ for all $l \in \dom \sigma$. In this case we write $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$, and if $\Xi$ and $\Gamma$ are both empty we simply write $\stotype{}{}{\Sigma}{\sigma}$.

\begin{theorem}[Preservation]
    If
    %
    \begin{equation*}
        \hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau},
        \quad
        \stotype{\Xi}{\Gamma}{\Sigma}{\sigma}
        \quad \text{and} \quad
        (\sigma,e) \step (\sigma',e'),
    \end{equation*}
    %
    then there exists some store typing $\Sigma'$ with $\Sigma \subseteq \Sigma'$ such that
    %
    \begin{equation*}
        \hastype{\Xi}{\Gamma}{\Sigma'}{e'}{\tau}
        \quad \text{and} \quad
        \stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}.
    \end{equation*}
\end{theorem}

\begin{proof}
    By definition of the one-step relation, there exist an evaluation context $K$ and subexpressions $d$ of $e$ and $d'$ of $e'$ such that $e = K[d]$, $e' = K[d']$, and $(\sigma,d) \headstep (\sigma',d')$. By \cref{lem:subexp-well-typed} there is some type $\rho$ such that $\hastype{\Xi}{\Gamma}{\Sigma}{d}{\rho}$. Next it follows from \cref{lem:preservation-head-steps} that $\hastype{\Xi}{\Gamma}{\Sigma'}{d'}{\rho}$ for some store typing $\Sigma'$ with $\Sigma \subseteq \Sigma'$ and $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$. By \cref{lem:store-typing-weakening} we also have $\hastype{\Xi}{\Gamma}{\Sigma'}{d}{\rho}$, so it follows from \cref{lem:evaluation-contexts-respect-types} that $\hastype{\Xi}{\Gamma}{\Sigma'}{K[d']}{\tau}$ as desired.
\end{proof}


\begin{lemma}
    \label{lem:subexp-well-typed}
    If $K$ is an evaluation context, $e$ is an expression and $\hastype{\Xi}{\Gamma}{\Sigma}{K[e]}{\tau}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.
\end{lemma}

\begin{proof}
Every evaluation context is obtained from the hole \enquote{$\hole$} by finitely many applications of the productions in the grammar [TODO prove this?]. We prove the claim by induction on the length of such a sequence of productions. If $K = \hole$, then the claim is obvious, since then $K[e] = e$. Hence we assume that $K$ is obtained from some evaluation context $K'$ by some application of a production, so that the induction hypothesis holds for $K'$.
%
\begin{proofsec}
    \item[$K = \objPair{K'}{e'}$]
    Then $K[e] = \objPair{K'[e]}{e'}$, and since this is well-typed with type $\tau$, \cref{enum:inversion-pair} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1}$ for some type $\tau_1$. By induction applied to $K'$ we have $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.

    \item[$K = \objPair{v}{K'}$]
    Similar to the above.

    \item[$K = \objFst{K'}$]
    Then $K[e] = \objFst{K'[e]$}, so \cref{enum:inversion-fst} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau \prod \tau_2}$ for some $\tau_2$. By induction we have $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.

    \item[$K \in \{\objSnd{K'}, \objInl{K'}, \objInr{K'}\}$]
    Similar to the above.

    \item[$K = \objMatch{K'}{x}{e_1}{e_2}$]
    Here \cref{enum:inversion-match} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1 + \tau_2}$, so the claim follows by induction.

    \item[$K = \objApp{K'}{e'}$]
    Then $K[e] = \objApp{K'[e]}{e'}$, so \cref{enum:inversion-app} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1 \to \tau}$ for some type $\tau_1$. The claim follows by induction as before.

    \item[$K = \objApp{v}{K'}$]
    Similar to the above.

    \item[$K = \objTapp{K'}{X}$]
    \Cref{enum:inversion-tapp} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1}$ for some type $\tau_1$, so the claim follows by induction.

    \item[$K = $]
    TODO

    \item[$K = \objRef{K'}$]
    Then $K[e] = \objRef{K'[e]}$, so the inversion lemma implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau'}$. The claim follows by induction.

    \item[$K = \objAss{K'}{e'}$]
    Then $K[e] = \objAss{K'[e]}{e'}$, so the inversion lemma implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau'}$ ... TODO
\end{proofsec}

TODO rest -- but they are all the same, so maybe just do one?
\end{proof}


\begin{lemma}[Weakening]
    \label{lem:store-typing-weakening}
    If $\Sigma$ and $\Sigma'$ are store typings with $\Sigma \subseteq \Sigma'$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, then $\hastype{\Xi}{\Gamma}{\Sigma'}{e}{\tau}$.
\end{lemma}

\begin{proof}
    This is a straightforward induction on type derivations, in that we notice that in all inference rules, the store typing is the same in the conclusion as it is in the hypotheses. Furthermore, if the axiom \infrule{T-loc} holds for $\Sigma$, then it clearly holds for $\Sigma'$.
\end{proof}


\begin{lemma}
    \label{lem:evaluation-contexts-respect-types}
    If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau}$ for the same type $\tau$, then $\hastype{\Xi}{\Gamma}{\Sigma}{K[e]}{\rho}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{K[e']}{\rho}$ for the same type $\rho$.
\end{lemma}

\begin{proof}
The proof is by induction on $K$. If $K = \hole$, then this is obvious.
%
\begin{proofsec}
    \item[$K = \objPair{K'}{e''}$]
    Then $K'[e]$ and $K'[e']$ have the same type, so by \infrule{T-pair}, so do $K[e]$ and $K[e']$.

    \item[$K = \objTapp{K'}{\tau'}$]
    Then $K'[e]$ and $K'[e']$ have the same type by induction, and so do $K[e] = \objTapp{K'[e]}{\tau'}$ and $K[e'] = \objTapp{K'[e']}{\tau'}$ by \infrule{T-Tapp}.\footnote{TODO since we just apply it to underscore, it actually has a lot of different types. But we can find one type that works for both.} [TODO need lemma saying that $\tau[\tau'/X]$ is a type!]
\end{proofsec}

TODO rest
\end{proof}


\begin{lemma}[Preservation for head-steps]
    \label{lem:preservation-head-steps}
    If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$ and $(\sigma,e) \headstep (\sigma',e')$, then there exists a store typing $\Sigma'$ such that $\Sigma \subseteq \Sigma'$, $\hastype{\Xi}{\Gamma}{\Sigma'}{e'}{\tau}$, and $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$.
\end{lemma}

\begin{proof}
We simply check all cases. [TODO mention pure cases]
%
\begin{proofsec}
    \item[\infrule{E-fst}]
    In this case $e = \objFst{\objPair{v_1}{v_2}}$ and $e' = v_1$ for values $v_1,v_2$. Then $e$ is a value, so [TODO canonical forms] implies first that $\hastype{\Xi}{\Gamma}{\Sigma}{\objPair{v_1}{v_2}}{\tau \prod \tau'}$ for some type $\tau'$, and then that $\hastype{\Xi}{\Gamma}{\Sigma}{v_1}{\tau}$.

    \item[\infrule{E-tapp-tlam}]
    Write the type of $\objTapp{\objForall{X}{e}}{\tau'}$ as $\tau[\tau'/X]$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{\objForall{X}{e}}{\typeForall{X}{\tau}}$, and again by inversion this implies that $\hastype{\Xi,X}{\Gamma}{\Sigma}{e}{\tau}$. But then it follows from [TODO lemma 0] that $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{e}{\tau[\tau'/X]}$, and since $\Gamma$ does not contain $X$ (since $\Xi$ does not) we have $\Gamma[\tau'/X] = \Gamma$, so the claim follows.

    \item[\infrule{E-alloc}]
    In this case $e = \objRef{v}$, $e' = l$, $\sigma' = \sigma[l \mapsto v]$, and $l \not\in \dom \sigma$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{\objRef{v}}{\typeRef{\tau'}}$ for some $\tau'$, and we further have $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau'}$. Now letting $\Sigma' = \Sigma[l \mapsto \tau']$, it follows from \infrule{T-loc} that $\hastype{\Xi}{\Gamma}{\Sigma'}{l}{\typeRef{\Sigma'(l)}}$, so we have both $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$ and $\hastype{\Xi}{\Gamma}{\Sigma'}{l}{\typeRef{\tau'}}$.

    \item[\infrule{E-store}]
    Here $e = \objAss{l}{v}$, $e' = \objUnit$ and $\sigma' = \sigma[l \mapsto v]$ with $l \in \dom \sigma$. Notice first that $\objAss{l}{v}$ and $\objUnit$ both have type $\typeUnit$ by inversion, so that $\hastype{\Xi}{\Gamma}{\Sigma}{\objUnit}{\typeUnit}$ as required.

    By inversion we also have $\hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\tau'}}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau'}$ for some type $\tau'$, and another application of inversion (TODO via \infrule{T-loc}, or canonical forms?) implies that $\tau' = \Sigma(l)$. Furthermore, since $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$, we have $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$. Since $v = \sigma'(l)$ it thus follows that $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma'(l)}{\Sigma(l)}$, so $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma'}$.

    \item[\infrule{E-load}]
    Here $e = \objLoad{l}$, $e' = v$ and $\sigma = \sigma'$ with $\sigma(l) = v$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\tau}}$, so $\tau = \Sigma(l)$ [TODO again]. But since $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$ we have $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$, or in other words, $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau}$.
\end{proofsec}

TODO rest
\end{proof}


\chapter{Misc}

\section{Lambda calculus}

The syntax of the untyped lambda calculus consists only of variables, abstractions and applications:
%
%
\begin{alignat*}{2}
    && x &\in \setVar \\
    & \setExp \quad & e &\Coloneqq x \mid \lambda x.e \mid e\,e \\
    & \setVal \quad & v &\Coloneqq \lambda x.e
\end{alignat*}
%
This means that there in particular are expressions on the form $e_1\,e_2\,e_3$, and we use the convention that application is left-associative, i.e. that the above expression is to be read $(e_1\,e_2)\,e_3$. [TODO build into the grammar, Mogensen]

The small-step reduction relation $\step$ on expressions formalises how to reduce expressions. For instance, the rule
%
\begin{equation*}
    \inferrule*[right=E-app-abs]{ }{
        (\lambda x . e_1) \, e_2 \step e_1[x \mapsto e_2]
    }
\end{equation*}
%
says that we can apply abstractions to other expressions. Such a rule is called a \keyword{computation rule}, and an expression $(\lambda x . e_1) \, e_2$ is called a \keyword{redex}. Rewriting a redex according to the above rule is called \keyword{$\beta$-reduction}.

A more complex expression might not itself be a redex but instead have a redex as a subexpression. In this case we need other rules, so-called \keyword{congruence rules}, which tell us how to reduce complex expressions by reducing subexpressions. For instance, in the expression $e_1\,e_2$, do we evaluate $e_1$ before $e_2$ or vice versa? That is, does evaluation happen left-to-right or right-to-left, or do we allow this to be determined arbitrarily? This is of course especially important in languages with side-effects. Formally we impose an evaluation order by having either (or both) of the congruence rules
%
\begin{equation*}
    \inferrule*[right=E-app1 \quad {\normalfont and} \quad]{
        e_1 \step e_1'
    }{
        e_1 \, e_2 \step e_1' \, e_2
    }
    % \quad \text{and} \quad
    \inferrule*[right=E-app2.]{
        e_2 \step e_2'
    }{
        e_1 \, e_2 \step e_1 \, e_2'
    }
\end{equation*}
%
If we desire right-to-left evaluation order, then we choose \infrule{E-app2}, but we also need a restricted form of \infrule{E-app1}, namely
%
\begin{equation*}
    \inferrule*[right=E-app1'.]{
        e \step e'
    }{
        e \, v \step e' \, v
    }
\end{equation*}
%
That is, only when the right expression has been reduced to a value $v$ can we evaluate the left expression.

We may also formalise the evaluation order by defining the reduction relation in terms of head reductions $\headstep$, and using evaluation contexts to impose an evaluation order. For instance,
%
\begin{equation*}
    K \Coloneqq \hole \mid K \, e \mid v \, K
    \quad \text{and} \quad
    K \Coloneqq \hole \mid e \, K \mid K \, v
\end{equation*}
%
define evaluation contexts for left-to-right and right-to-left evaluation, respectively. The symbol \enquote{$\hole$} is called the \keyword{hole}, and we think of the hole as the place into which we substitute the expression $e$ when writing $K[e]$.

Computation and congruence rules together might also allow for different evaluation strategies, for instance:
%
\begin{enumerate}
    \item Full $\beta$-reduction: We may reduce \emph{any} redex contained in an expression.

    \item Normal order reduction: We must reduce the leftmost, outermost redex first.

    \item Call-by-name: The subexpression $e_2$ of a redex $(\lambda x . e_1) \, e_2$ cannot be reduced. Instead, we must perform $\beta$-reduction without reducing $e_2$.

    \item Call-by-value: Instead, $e_2$ \emph{must} be reduced to a value before $\beta$-reduction can take place.
\end{enumerate}
%
For instance, in call-by-value we might have a restricted form of the rule \infrule{E-app-abs}, namely
%
\begin{equation*}
    \inferrule*[right=E-app-abs'.]{ }{
        (\lambda x . e) \, v \step e[x \mapsto v]
    }
\end{equation*}
%
That is, the argument must be a value $v$ for the reduction to take place. If we use evaluation contexts, this computation rule would be a rule concerning head reductions $\headstep$.


Since the untyped lambda calculus does not allow abstractions to be named, it is not obvious how to define recursive functions. TODO


\section{Recursion}

Call by name: $Y = \lambda f . (\lambda x . f (xx)) (\lambda x . f (xx))$

\begin{align*}
    Y g
        &= \lambda f . (\lambda x . f (xx)) (\lambda x . f (xx)) g \\
        &\step (\lambda x . g (xx)) (\lambda x . g (xx)) \\
        &\step g ((\lambda x . g (xx))(\lambda x . g (xx)))
\end{align*}
%
On the other hand we also have
%
\begin{align*}
    g (Y g)
        &= g (\lambda f . (\lambda x . f (xx)) (\lambda x . f (xx)) g) \\
        &\step g ((\lambda x . g (xx))(\lambda x . g (xx))).
\end{align*}
%
That is, $Y g$ and $g (Y g)$ reduce to the same expression.

Call by value: $Z = \lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y))$

\begin{align*}
    Z g
        &= \lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y)) g \\
        &\step (\lambda x . g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)) \\
        &\step g (\lambda y. (\lambda x. g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)) y) \\
        &\step g (  )
\end{align*}

\begin{align*}
    g (Z g)
        &= g (\lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y)) g) \\
        &\step g ((\lambda x . g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)))
\end{align*}


\appendix

\chapter{Topology stuff}

\section{Objects}

If $P$ is a poset and $x \in P$, then we write
%
\begin{equation*}
    x^\downarrow
        \defeq \set{y \in P}{y \leq x}
    \quad \text{and} \quad
    x^\uparrow
        \defeq \set{y \in P}{x \leq y}.
\end{equation*}


\begin{definition}
    A poset $P$ is called
    %
    \begin{enumdef}
        \item \keyword{$\omega$-complete}, for short an \keyword{$\omega$-cpo}, if every nonempty $\omega$-chain in $P$ has a join,
        
        \item \keyword{chain-complete}, for short a \keyword{ccpo}, if every nonempty chain in $P$ has a join, and
        
        \item \keyword{directedly-complete}, for short a \keyword{dcpo}, if every $\omega$-chain in $P$ has a join.
    \end{enumdef}
\end{definition}
%
A chain-complete poset is also said to be \keyword{inductive} or \keyword{inductively ordered}. If a poset with one of the above properties also has a least element, then we say that they are \keyword{pointed} and add an extra \enquote{p} to their abbreviations, so that we get $\omega$-cppo, ccppo and dcppo.


\section{Arrows}

\begin{proposition}
    Let $P$ and $Q$ be posets and let $f \colon P \to Q$ be a function. Then the following are equivalent:
    %
    \begin{enumlem}
        \item $f$ is monotone.
        \item If $B \subseteq Q$ is upward closed, then $f\preim{B}$ is also upward closed.
        \item If $B \subseteq Q$ is downward closed, then $f\preim{B}$ is also downward closed.
    \end{enumlem}
\end{proposition}

\begin{proof}
    First assume that $f$ is monotone, let $B \subseteq Q$ be upward closed, and let $y \in f\preim{B}$. If $x \leq y$, then $f(x) \leq f(y)$, and so $f(x) \in B$. But then $x \in f\preim{B}$ as required.

    Conversely assume that the preimage under $f$ of an upward closed set is upward closed, and let $x,y \in P$ with $x \leq y$. Then $f(y)^\downarrow$ is downward closed, and hence so is $f\preim{f(y)^\downarrow}$. But then $x \in f\preim{f(y)^\downarrow}$, so $f(x) \in f(y)^\downarrow$, implying that $f(x) \leq f(y)$.

    Since the complement of an upward closed set is downward closed, and vice-versa, the last two properties are clearly equivalent.
\end{proof}


\begin{definition}
    Let $P$ be a poset. A subset $U \subseteq P$ is said to be
    %
    \begin{enumdef}
        \item \keyword{inaccessible by joins of $\omega$-chains} if $\bigjoin C \in U$ implies $C \intersect U \neq \emptyset$ for all $\omega$-chains $C \subseteq P$,

        \item \keyword{inaccessible by joins of chains} if $\bigjoin C \in U$ implies $C \intersect U \neq \emptyset$ for all chains $C \subseteq P$, and

        \item \keyword{inaccessible by directed joins} if $\bigdjoin D \in U$ implies $D \intersect U \neq \emptyset$ for all directed sets $D \subseteq P$.
    \end{enumdef}
    %
    [TODO nonempty chains??]
\end{definition}
%
When we write e.g. $\bigjoin C$ we implicitly assume that the join in fact exists.

Let $\calT$ be the collection of subsets of $P$ that are upward closed and inaccessible by directed joins. Then $\calT$ is closed under arbitrary unions, since if $D \subseteq P$ is directed and $\bigdjoin D \in \bigunion_{i \in I} U_i$ with $U_i \in \calT$, then $\bigdjoin D \in U_i$ for some $i$, and hence $\emptyset \neq \bigdjoin D \intersect U_i \subseteq \bigdjoin D \intersect \bigunion_{i \in I} U_i$. Similarly, if $U,V \in \calT$ and $\bigdjoin D \in U \intersect V$, then there are elements $x \in D \intersect U$ and $y \in D \intersect V$. And since $D$ is directed $x$ and $y$ have an upper bound $z$ in $D$, and since $U$ and $V$ are upward closed we have $z \in U \intersect V$. That is, $\calT$ is a topology on $P$.

We similarly find that the collections of subsets of $P$ that are upwards closed and inaccessible by joins of ($\omega$-)chains also constitute topologies on $P$. At least the latter two topologies are known as the \keyword{Scott topology}, but we use this name for any of the three topologies. To disambiguate, we also call them the \keyword{$\omega$-}, \keyword{chain}, and \keyword{directed Scott topologies} respectively. If $f \colon P \to Q$ is a map between posets, then we say that $f$ is \keyword{$\omega$-}, \keyword{chain}, and \keyword{directedly Scott continuous} if $f$ is continuous when $P$ and $Q$ are equipped with the relevant Scott topology. If $f$ is continuous in any of these ways, then we simply call $f$ \keyword{Scott continuous}.

It is also useful to note that a subset $F \subset P$ is closed in e.g. the directed Scott topology if and only if is downward closed and $\bigdjoin D \in F$ for all directed subsets $D \subseteq F$. Note that if $x \in P$, then $x^\downarrow$ is closed: It is clearly downward closed, and $x$ is an upper bound of every subset $A \subseteq x^\downarrow$, so the join of $A$ (if it exists) also lies in $x^\downarrow$. This in particular leads to the following:

\begin{lemma}
    If $f \colon P \to Q$ is Scott continuous, then $f$ is monotone.
\end{lemma}

\begin{proof}
    Let $x,y \in P$ with $x \leq y$. The set $f(y)^\downarrow$ is closed in $Q$, so its preimage $f\preim{f(y)^\downarrow}$ is closed in $P$ and is in particular downward closed. Hence it contains $x$, and so $f(x) \in f(y)^\downarrow$, which means that $f(x) \leq f(y)$.
\end{proof}
%
In particular, if $f$ is continuous and $B \subseteq Q$ is upward/downward closed, then $f\preim{B}$ is also upward/downward closed by [TODO ref].

There is a different characterisation of continuous maps between posets. If $f \colon P \to Q$ is a map between posets, then we say that $f$ \keyword{preserves existing joins} if whenever $\bigjoin A$ exists in $P$ for a subset $A \subseteq P$, then $\bigjoin f\image{A}$ exists in $Q$ and $f(\bigjoin A) = \bigjoin f\image{A}$. We often consider slightly different properties where we require $f$ to preserve certain properties of subsets as well as existing joins of subsets with these properties. For instance, we say that $f$ \keyword{preserves existing directed joins} if, whenever $D \subseteq P$ is directed and $\bigdjoin D$ exists in $P$, then $f\image{D}$ is also directed, $\bigdjoin f\image{D}$ exists in $Q$ and $f(\bigdjoin D) = \bigdjoin f\image{D}$. This leads to the following definition:

\begin{definition}
    For posets $P$ and $Q$, a function $f \colon P \to Q$ is called
    %
    \begin{enumdef}
        \item \keyword{$\omega$-continuous} if $f$ preserves existing joins of $\omega$-chains.
        
        \item \keyword{chain-continuous} if $f$ preserves existing joins of chains.
        
        \item \keyword{directedly-continuous} if $f$ preserves existing directed joins.
    \end{enumdef}
\end{definition}
%
If $f$ has any of the above properties, then we simply call $f$ \keyword{continuous}. Again continuous maps are monotone:

\begin{lemma}
    If $f \colon P \to Q$ is continuous, then $f$ is monotone.
\end{lemma}

\begin{proof}
    If $x,y \in P$ with $x \leq y$, then $\{x,y\}$ is a directed set with $\bigdjoin \{x,y\} = y$, and so $f(y) = \bigdjoin \{f(x),f(y)\}$. In particular, $f(x) \leq f(y)$.
\end{proof}
%
However, while continuous functions are automatically Scott continuous, the converse does not generally hold. We need to make a further assumption on the codomain of the function in question, namely that is has one of the following properties:

[TODO moved]

\begin{proposition}
    Let $P$ and $Q$ be posets. If $f \colon P \to Q$ is $\omega$-, chain- or directedly-continuous, then $f$ is $\omega$-, chain or directedly Scott continuous, respectively.

    If $Q$ is $\omega-$, chain- or directedly-complete, respectively, then the converse also holds.
\end{proposition}

\begin{proof}
    We prove the claim in the case where $f$ is directedly-continuous or directedly Scott continuous. The other cases are similar.
    
    First assume that $f$ is directedly-continuous, and let $F \subseteq Q$ be closed in the directed Scott topology. Assume that $D \subseteq f\preim{F}$ is directed and that $\bigdjoin D$ exists in $P$. Then $f\image{D} \subseteq F$, so
    %
    \begin{equation*}
        f \bigl( \bigdjoin D \bigr)
            = \bigdjoin f\image{D}
            \subseteq F,
    \end{equation*}
    %
    which implies that $\bigdjoin D \in f\preim{F}$. By [TODO ref], $f\preim{F}$ is also closed.

    Conversely assume that $Q$ is directedly-complete and that $f$ is directedly Scott continuous. Let $D \subseteq P$ be directed such that $\bigdjoin D$ exists in $P$. By [TODO ref] $f$ is monotone so $f\image{D}$ also directed, and hence the join $z \defeq \bigdjoin f\image{D}$ exists in $Q$. Notice that $D \subseteq f\preim{z^\downarrow}$, implying that $\bigdjoin D \in f\preim{z^\downarrow}$ since $f\preim{z^\downarrow}$ is closed. But then $f(\bigdjoin D) \leq z = \bigdjoin f\image{D}$, so $f$ is directedly-continuous.
\end{proof}


\section{Fixed-point theorems}

\begin{theorem}[Knaster--Tarski's fixed-point theorem]
    If $L$ is a complete lattice and $F \colon L \to L$ is monotone, then $F$ has a least and a greatest fixed-point, and these are given by
    %
    \begin{equation*}
        \mu F
            = \bigmeet \set{x \in L}{F(x) \leq x}
        \quad \text{and} \quad
        \nu F
            = \bigjoin \set{x \in L}{x \leq F(x)}.
    \end{equation*}
    %
    In particular, $\mu F$ is the smallest $F$-closed element and $\nu F$ is the greatest $F$-consistent element in $L$.
\end{theorem}

\begin{prooffootnote}{This proof is based on \textcite[Theorem~2.35]{davey-priestley-order}.}
    Denote the meet above by $\alpha$. If $x$ is $F$-closed, then $\alpha \leq x$, so $F(\alpha) \leq F(x) \leq x$. Taking the meet of $x$ we get $F(\alpha) \leq \alpha$, so $\alpha$ is closed. It follows that $F(F(\alpha)) \leq F(\alpha)$, so $F(\alpha)$ is also closed, and so $\alpha \leq F(\alpha)$. Hence $\alpha$ is a fixed-point. Since every other fixed-point is in particular closed, $\alpha$ is the least fixed-point.
\end{prooffootnote}


\begin{theorem}[Kleene's fixed-point theorem I]
    If $P$ is an $\omega$-cppo and $F \colon P \to P$ is $\omega$-continuous, then $F$ has a least fixed-point given by\footnote{This theorem is an immediate generalisation of \textcite[Theorem~8.15]{davey-priestley-order} from dcppo's to $\omega$-cppo's.}
    %
    \begin{equation*}
        \mu F
            = \bigjoin_{n \in \naturals} F^n(\bot).
    \end{equation*}
\end{theorem}

\begin{proof}
    First notice that, since $F$ is continuous,
    %
    \begin{equation*}
        F \Bigl( \bigjoin_{n \in \naturals} F^n(\bot) \Bigr)
            = \bigjoin_{n \in \naturals} F^{n+1}(\bot)
            = \bigjoin_{n \in \naturals^+} F^n(\bot)
            = \bigjoin_{n \in \naturals} F^n(\bot),
    \end{equation*}
    %
    where we use that $F^0(\bot) = \bot$. Hence $\bigjoin_{n \in \naturals} F^n(\bot)$ is indeed a fixed-point of $F$. If $\beta$ is any fixed-point of $F$, then $\bot \leq \beta$, and hence $F^n(\bot) \leq F^n(\beta) = \beta$ since $F$ is monotone. Taking the join on the left-hand side yields $\bigjoin_{n \in \naturals} F^n(\bot) \leq \beta$ as desired.
\end{proof}
%
In the case where $F$ is $\omega$-continuous, it is thus easy to show that $F$ has a fixed-point and even give a fairly explicit formula for it. If $F$ is not continuous, we are not so lucky. However, since $\omega = (\naturals,\leq)$ is an ordinal we may be inspired to extend the recursive definition of $F^n(\bot)$ to ordinal powers $F^\alpha(\bot)$ and attempt to find a fixed-point among these elements. This is indeed possible, but since we cannot be sure that $\alpha$ is countable, we must assume that $P$ is chain-complete. The proof is also significantly more involved, requiring transfinite recursion and induction.

We present a proof based on \textcite[Exercise~8.19]{davey-priestley-order}. For a proof of the existence of a least fixed-points using Zermelo [TODO], see \textcite[Theorem~7.36]{moschovakis-set-theory}

\begin{theorem}[Kleene's fixed-point theorem II]
    Let $P$ be a ccppo and let $F \colon P \to P$ be monotone. Let
    %
    \begin{alignat*}{2}
        F^0(\bot)
            &= \bot, \\
        F^{\alpha}(\bot)
            &= F(F^{\alpha-1}(\bot))
            \quad &&\text{if $\alpha$ is a successor}, \\
        F^\alpha(\bot)
            &= \bigjoin_{\beta < \alpha} F^\beta(\bot)
            \quad &&\text{if $\alpha$ is a limit},
    \end{alignat*}
    %
    for all ordinals $\alpha$. Then $\mu F = F^\alpha(\bot)$ for some ordinal $\alpha$.
\end{theorem}
%
Notice that if $F$ is continuous, then [TODO Kleene I] says that $\mu F = F^\omega(\bot)$. Indeed, examining the proof below we see that the definition of $F^\alpha(\bot)$ for \emph{countable} $\alpha$ only requires $P$ to be an $\omega$-cppo.

\begin{prooffootnote}{This proof is }
    We first show that the above definition of $F^\alpha(\bot)$ makes sense. More generally, let $x \in P$. We define a binary (definite [TODO define]) operation $H$ taking as arguments an ordinal $\alpha$ and a function $g \colon \alpha \to P$: We first let $H(g,0) = x$. If $\alpha$ has an immediate predecessor $\beta$, then we let $H(g,\alpha) = F(g(\beta))$. If $\alpha$ is a limit ordinal and $g$ is monotone, then we let $H(g,\alpha) = \bigjoin_{\beta < \alpha} g(\beta)$. (Note that we indeed take the join of a chain since $\alpha$ is a chain and $g$ is assumed monotone.) Finally, if $\alpha$ is a limit ordinal but $g$ is not monotone, then we let $H(g,\alpha)$ be some arbitrary element of $P$ (we will not need to consider such $g$). By transfinite recursion [TODO ref] there thus exists a unary (definite) operation $G$ such that
    %
    \begin{equation*}
        G(\alpha)
            = H(G|_\alpha, \alpha)
    \end{equation*}
    %
    for all $\alpha$.
    
    We prove by transfinite induction that $G|_\alpha$ is monotone for all $\alpha$. For $\alpha = 0$ this is obvious, so assume that $G|_\xi$ is monotone for all $\xi < \alpha$ and let $\beta < \gamma < \alpha$. We consider three cases:
    %
    \begin{proofsec}
        \item[$\beta$ and $\gamma$ are successors]
        Then $G|_\gamma$ is monotone, and since $F$ is also monotone we have
        %
        \begin{equation*}
            G(\beta)
                = F(G(\beta-1))
                \leq F(G(\gamma-1))
                = G(\gamma).
        \end{equation*}

        \item[$\beta$ is a limit, $\gamma$ is a successor]
        Since $\beta$ is the union of all \emph{successor} ordinals smaller than it\footnote{This follows since if $\xi < \beta$, then also $\xi + 1 < \beta$.}, it suffices to show that $G(\xi) \leq G(\gamma)$ if $\xi < \beta$ is a successor ordinal. But since $G|_\gamma$ is monotone, we similarly to above find that
        %
        \begin{equation*}
            G(\xi)
                = F(G(\xi-1))
                \leq F(G(\gamma-1))
                = G(\gamma).
        \end{equation*}

        \item[$\beta$ is a successor, $\gamma$ is a limit]
        Let $\xi$ be a successor ordinal with $\beta \leq \xi < \gamma$. Since $G|_\gamma$ is monotone, we again have
        %
        \begin{equation*}
            G(\beta)
                = F(G(\beta-1))
                \leq F(G(\xi-1))
                = G(\xi),
        \end{equation*}
        %
        which implies that
        %
        \begin{equation*}
            G(\beta)
                \leq \bigjoin_{\xi < \gamma} G(\xi)
                = G(\gamma).
        \end{equation*}
    \end{proofsec}
    %
    Since $G|_\alpha$ is always monotone, we always have $G(\alpha) = \bigjoin_{\beta < \alpha} G(\beta)$ when $\alpha$ is a limit ordinal. Writing $F^\alpha(x) \defeq G(\alpha)$ we thus obtain a map $F^\alpha \colon P \to P$ given by
    %
    \begin{alignat*}{2}
        F^0(x)
            &= \bot, \\
        F^{\alpha}(x)
            &= F(F^{\alpha-1}(x))
            \quad &&\text{if $\alpha$ is a successor}, \\
        F^\alpha(\bot)
            &= \bigjoin_{\beta < \alpha} F^\beta(x)
            \quad &&\text{if $\alpha$ is a limit},
    \end{alignat*}
    %
    for all $x \in P$ and ordinals $\alpha$.

    We next show that $F^\alpha(\bot)$ is a fixed-point of $F$ for some $\alpha$. By Hartogs' theorem\footnote{TODO} there is some ordinal $\alpha$ such that there is no injection $\alpha \injto P$. On the other hand, $\beta \mapsto F^\beta(\bot)$ is a function $\alpha \to P$, so this cannot be injective. Hence there are distinct ordinals $\beta,\gamma < \alpha$ with $F^\beta(\bot) = F^\gamma(\bot)$. Because $\alpha$ is totally ordered we may assume without loss of generality that $\beta < \gamma$. Since the map $\beta \mapsto F^\beta(\bot)$ is monotone and $\beta + 1 \leq \gamma$, this implies that
    %
    \begin{equation*}
        F^\beta(\bot)
            \leq F^{\beta+1}(\bot)
            \leq F^\gamma(\bot),
    \end{equation*}
    %
    and hence
    %
    \begin{equation*}
        F(F^\beta(\bot))
            = F^{\beta+1}(\bot)
            = F^\beta(\bot).
    \end{equation*}
    %
    Thus $F^\beta(\bot)$ is a fixed-point of $F$.

    Finally we show that $F$ has a \emph{least} fixed-point. First we show that the map $F^\alpha$ is monotone for all ordinals $\alpha$, i.e. that if $x,y \in P$ with $x \leq y$, then $F^\alpha(x) \leq F^\alpha(y)$. If $\alpha = 0$, then this is obvious, so assume that it hold for all ordinals $\beta < \alpha$. If $\alpha$ is a successor, then
    %
    \begin{equation*}
        F^\alpha(x)
            = F(F^{\alpha-1}(x))
            \leq F(F^{\alpha-1}(y))
            = F^\alpha(y).
    \end{equation*}
    %
    If instead $\alpha$ is a limit, then
    %
    \begin{equation*}
        F^\beta(x)
            \leq F^\beta(y)
            \leq \bigjoin_{\beta < \alpha} F^\beta(y)
            = F^\alpha(y),
    \end{equation*}
    %
    and taking the join on the left-hand side we again get $F^\alpha(x) \leq F^\alpha(y)$. Now if $x$ is any fixed-point of $F$, then it is clear that $F^\alpha(x) = x$ for any $\alpha$. Hence if $F^\alpha(\bot)$ is a fixed-point, then $\bot \leq x$, and so
    %
    \begin{equation}
        F^\alpha(\bot)
            \leq F^\alpha(x)
            = x,
    \end{equation}
    %
    so $F^\alpha(\bot)$ is indeed the least fixed-point of $F$.
\end{prooffootnote}

A natural question might be whether we can use this extension of Kleene's fixed-point theorem to obtain inversion-type results for all monotone maps, not just those that are continuous. We might, for instance, allow inference rules to have infinitely many antecedents. If $X$ is the ground set, $S \subseteq X$ and $y \in X$, then we write $\myinfrule{R}{S}{y}$ to denote that the inference rule $R$ has as antecedents the elements in $S$, and $y$ as consequent. Given a collection $\calR$ of such inference rules, we may again define a function $F \colon \powerset{X} \to \powerset{X}$ by letting $F(A)$ consist of those $y \in X$ such that there is a rule $\myinfrule{}{S}{y}$ in $\calR$ with $S \subseteq A$. Clearly $F$ is then monotone, but it is not clear that it is continuous (and indeed it often is not).

We quickly see that there is no general inversion lemma for $F$: In the continuous case, all elements of $\omega$ are successor ordinals, so if $y \in F^n(\bot)$ for some $n \in \omega$, then we know that $y \in F(F^{n-1}(\bot))$, and hence there is a rule $\myinfrule{}{\seq{x}}{y}$ with $\seq{x} \subseteq F^{n-1}(\bot)$. And for successor ordinals in the continuous case we may draw a similar conclusion. But if $\alpha$ is a limit ordinal and $y \in F^\alpha(\bot)$, then $y \in \bigunion_{\beta < \alpha} F^\beta(\bot)$, which only tells us that $y \in F^\beta(\bot)$ for some $\beta < \alpha$. And while we may assume that $\beta$ is a successor ordinal, this may be the eventual successor of another limit ordinal, and this process may repeat itself infinitely many times. This implies that there is no way to derive $y$ with only \emph{finitely many} applications of the inference rules.


A map $f \colon P \to P$ on a poset $P$ is called \keyword{expansive} if $x \leq f(x)$ for all $x \in P$.\footnote{Some authors also use the adjectives \keyword{extensive}, \keyword{inflationary} or even \keyword{increasing}.}

\begin{theorem}[Zermelo's fixed-point theorem]
    If $P$ is a chain-complete poset and $F \colon P \to P$ is expansive, then $F$ has a fixed-point.\footnote{In \textcite{davey-priestley-order} this is known as \enquote{CPO Fixpoint Theorem III}, and they add the hypothesis that $P$ be a dcppo, but also claim that then $F$ has a \emph{minimal} fixed-point. But this is false: Consider for instance the poset $P = (\omega^\partial)_\bot$, i.e., the set $\naturals \union \{\bot\}$ equipped with the ordering $\preceq$ given by
    %
    \begin{equation*}
        \bot \prec \cdots \prec n \prec n-1 \prec \cdots \prec 1 \prec 0.
    \end{equation*}
    %
    This is clearly a dcppo since every nonempty subset even has a maximum. Define $F \colon P \to P$ by letting $F(\bot) = 0$ and $F(n) = n$ for $n \in \naturals$. Then $F$ is obviously expansive, but every natural number, of which there is no minimal with respect to $\preceq$, is a fixed-point. In the context of the proof below, \citeauthor{davey-priestley-order} claim that the smallest $F$-invariant sub-dcppo of $P$, here $\{\bot, 0\}$, has a greatest element, here $0$, and that this is a minimal fixed-point of $F$. But this is false, since e.g. $1$ is also a fixed-point. \par Our proof of Zermelo's theorem is, however, based on \textcite[Exercise~8.20]{davey-priestley-order}. Note that e.g. \textcite[Theorem~7.35]{moschovakis-set-theory} proves our version of the theorem, but using the theory of well-orderings. \par The above counterexample is taken from \textcite{hansen-zermelo-stackexchange}.}
\end{theorem}

\begin{proof}
    Consider instead the lift $P_\bot$, which is chain-complete and also pointed. Extend $F$ ot $P_\bot$ by letting $F(\bot)$ be some element of $P$ (that is, $F(\bot) \neq \bot$). Let $P_0$ be the smallest $F$-invariant subset of $P_\bot$ that is also a ccppo.
    
    We will call an element $x \in P_0$ a \keyword{roof} if $y < x$ implies $F(y) \leq x$ for all $y \in P_0$. For a roof $x$ we consider the set
    %
    \begin{equation*}
        Z_x
            = \set{y \in P_0}{y \leq x \text{ or } F(x) \leq y}.
    \end{equation*}
    %
    We claim that $Z_x$ is an $F$-invariant ccppo, so let $y \in Z_x$. If $y \leq x$, then either $y = x$ in which case $F(x) \leq F(y)$, or else $y < x$ so that $F(y) \leq x$ since $x$ is a roof. If instead $F(x) \leq y$, then since $F$ is expansive we have $F(x) \leq F(y)$. Hence $F(y) \in Z_x$, so $Z_x$ is $F$-invariant. Next let $C \subseteq Z_x$ be a chain. If $y \leq x$ for all $y \in x$, then we also have $\bigjoin C \subseteq x$. If instead there is some $y \in C$ with $F(x) \leq y$, then we clearly have $F(x) \leq \bigjoin C$. Thus $Z_x$ is a ccppo, so by minimality of $P_0$ we have $P_0 = Z_x$.

    Next we claim that every element in $P_0$ is a roof. Consider the set
    %
    \begin{equation*}
        Z
            = \set{x \in P_0}{\text{$x$ is a roof}}.
    \end{equation*}
    %
    We show that $Z$ is an $F$-invariant ccppo. If $x$ is a roof and $y \in P_0$, then $y \in Z_x$ and so either $y \leq x$ or $F(x) \leq y$. If $y < F(x)$ then we must have $y \leq x \leq F(x)$, so that $F(x)$ is also a roof, and so $Z$ is $F$-invariant. If $C \subseteq Z$ is a chain and $y < \bigjoin C$, then there is some $x \in C$ with $y < x \leq \bigjoin C$, so $Z$ is also chain-complete. Again by minimality we have $P_0 = Z$.

    Now notice that $P_0$ is a chain: If $x,y \in P_0$, then $x$ is a roof and so $y \in Z_x$, which implies that either $y \leq x$ or $x \leq F(x) \leq y$. Since $P_\bot$ is chain-complete, $P_0$ has a greatest element $\top$, which is then also a roof. Since $F$ is expansive and $P_0$ is $F$-invariant we have $\top \leq F(\top) \leq \top$, so $\top$ is a fixed-point of $F$.

    Finally notice that since $F(\bot) \neq \bot$, $\bot$ is not a fixed-point of $F$, and so $F$ has a fixed-point lying in $P$, proving the original claim.
\end{proof}


% \begin{proof}
%     Consider instead the lift $P_\bot$, which is chain-complete and also pointed. Extend $F$ ot $P_\bot$ by letting $F(\bot)$ be some element of $P$ (that is, $F(\bot) \neq \bot$). Let $P_0$ be the smallest $F$-invariant subset of $P_\bot$ that is also a ccppo.
    
%     We will call an element $x \in P_0$ a \keyword{roof} if $y < x$ implies $F(y) \leq x$ for all $y \in P_0$. For a roof $x$ we consider the set
%     %
%     \begin{equation*}
%         Z_x
%             = \set{y \in P_0}{y \leq x \text{ or } F(x) \leq y}.
%     \end{equation*}
%     %
%     We claim that $Z_x$ is an $F$-invariant ccppo, so let $y \in Z_x$. If $y \leq x$, then either $y = x$ in which case $F(x) \leq F(y)$, or else $y < x$ so that $F(y) \leq x$. If instead $F(x) \leq y$, then since $F$ is expansive we have $F(x) \leq F(y)$. Hence $F(y) \in Z_x$, so $Z_x$ is $F$-invariant. Next let $C \subseteq Z_x$ be a chain. If $y \leq x$ for all $y \in x$, then we also have $\bigjoin C \subseteq x$. If instead there is some $y \in C$ with $F(x) \leq y$, then we clearly have $F(x) \leq \bigjoin C$. Thus $Z_x$ is a ccppo, so by minimality of $P_0$ we have $P_0 \subseteq Z_x$.

%     We next claim that every element in $P_0$ is a roof. Consider the set
%     %
%     \begin{equation*}
%         Z
%             = \set{x \in P_0}{\text{$x$ is a roof}}.
%     \end{equation*}
%     %
%     We show that $Z$ is an $F$-invariant ccppo. If $x$ is a roof and $y \in P_0$, then either $y \leq x$ or $F(x) \leq y$. If $y < F(x)$ then we must have $y \leq x \leq F(x)$, so that $F(x)$ is also a roof.
% \end{proof}



\nocite{*}
\chapter*{\bibname}
\markboth{\bibname}{\bibname}
\addcontentsline{toc}{chapter}{\bibname}
\printbibliography[heading=none]

\end{document}