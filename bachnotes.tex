\newcommand{\doctitle}{Bachelor Project in Computer Science}
\newcommand{\docauthor}{Danny Nygård Hansen}

\documentclass[a4paper, 11pt, article, danish, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}
\usepackage[autostyle]{csquotes}

\usepackage[final]{microtype}
\frenchspacing
\raggedbottom

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[largesmallcaps]{kpfonts}
\linespread{1.06}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
\usepackage{inconsolata}

\usepackage{hyperref}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor={\docauthor},
    hidelinks,
}

\usepackage{theorems-changedot}
\usepackage{theorems-references}

\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}
\setlist{
	listparindent=\parindent,
	parsep=0pt,
}
\usepackage{array}

\title{\doctitle}
\author{\docauthor}

\newcommand{\overbar}[3]{\mkern #1mu\overline{\mkern-#1mu#3\mkern-#2mu}\mkern #2mu}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\extreals}{\overbar{1.5}{1.5}{\reals}}
\newcommand{\complex}{\mathbb{C}}


\usepackage{pgffor}

\newcommand{\rvar}[1]{\mathsf{#1}}

\foreach \x in {A,...,Z}{%
    \expandafter\xdef\csname cal\x\endcsname{\noexpand\mathcal{\x}}
    \expandafter\xdef\csname frak\x\endcsname{\noexpand\mathfrak{\x}}
    \expandafter\xdef\csname rand\x\endcsname{\noexpand\rvar{\x}}
}


\usepackage{etoolbox}
\newcommand{\blank}{\mathrel{\;\cdot\;}}
\newcommand{\blankifempty}[1]{\ifstrempty{#1}{\blank}{#1}}
\DeclarePairedDelimiter{\auxdelimlvert}{\lvert}{\rvert}
\DeclarePairedDelimiter{\auxdelimlVert}{\lVert}{\rVert}
\DeclarePairedDelimiterX{\auxdelimanglescomma}[2]{\langle}{\rangle}{#1,#2}
\newcommand{\abs}[1]{\auxdelimlvert{\blankifempty{#1}}}
\newcommand{\norm}[1]{\auxdelimlVert{\blankifempty{#1}}}
\newcommand{\inner}[2]{\auxdelimanglescomma{\blankifempty{#1}}{\blankifempty{#2}}}


\DeclarePairedDelimiter{\auxdelimparen}{(}{)}
\DeclarePairedDelimiterX{\auxdelimparencomma}[2]{(}{)}{#1,#2}
\DeclarePairedDelimiter{\auxdelimbracket}{[}{]}
\DeclarePairedDelimiterX{\auxdelimbracketcomma}[2]{[}{]}{#1,#2}
\newcommand{\powerset}[2][]{\calP\auxdelimparen[#1]{#2}}
\newcommand{\powersetcard}[3][]{\calP_{#2}\auxdelimparen[#1]{#3}}
\newcommand{\powersetfin}[2][]{\powersetcard[#1]{\omega}{#2}}
\newcommand{\borel}[2][]{\calB\auxdelimparen[#1]{#2}}
\newcommand{\meas}[2][]{\calM\auxdelimparen[#1]{#2}}
\newcommand{\measC}[2][]{\calM_\complex\auxdelimparen[#1]{#2}}
\newcommand{\measpos}[2][]{\meas[#1]{#2}^+}
\newcommand{\measbound}[2][]{\calM_b\auxdelimparen[#1]{#2}}
\newcommand{\measboundpos}[2][]{\measbound[#1]{#2}^+}


\newcommand{\extmeas}[2][]{\overbar{4.5}{0.5}{\calM}\auxdelimparen[#1]{#2}}
\newcommand{\extmeaspos}[2][]{\extmeas[#1]{#2}^+}
\newcommand{\simplemeas}[2][]{\calS\!\calM\auxdelimparen[#1]{#2}}
\newcommand{\simplemeaspos}[2][]{\simplemeas[#1]{#2}^+}
\newcommand{\sigmaalg}[2][]{\sigma\auxdelimparen[#1]{#2}}
\newcommand{\deltasys}[2][]{\delta\auxdelimparen[#1]{#2}}

\newcommand{\expval}[2][]{\mathbb{E}\auxdelimbracket[#1]{#2}}
\newcommand{\var}[2][]{\operatorname{Var}\auxdelimbracket[#1]{#2}}
\newcommand{\cov}[3][]{\operatorname{Cov}\auxdelimbracketcomma[#1]{#2}{#3}}


\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\id}{id}
\newcommand{\indicator}[1]{\mathbf{1}_{#1}}

% Lattice operations
\newcommand{\meet}{\land}
\newcommand{\join}{\lor}

\DeclareMathOperator*{\smallbigvee}{\textstyle\bigvee}
\DeclareMathOperator*{\bigjoin}{\mathchoice
    {\smallbigvee}%
    {\bigvee}%
    {\bigvee}%
    {\bigvee}%
}
\DeclareMathOperator*{\smallbigsqcup}{\textstyle\bigsqcup}
\DeclareMathOperator*{\bigdjoin}{\mathchoice
    {\smallbigsqcup}%
    {\bigsqcup}%
    {\bigsqcup}%
    {\bigsqcup}%
}
\DeclareMathOperator*{\smallbigwedge}{\textstyle\bigwedge}
\DeclareMathOperator*{\bigmeet}{\mathchoice
    {\smallbigwedge}%
    {\bigwedge}%
    {\bigwedge}%
    {\bigwedge}%
}



\newcommand*\union\cup
\newcommand*\intersect\cap

\DeclareMathOperator*{\smallbigcup}{\textstyle\bigcup}
\DeclareMathOperator*{\bigunion}{\mathchoice
    {\smallbigcup}%
    {\bigcup}%
    {\bigcup}%
    {\bigcup}%
}
\DeclareMathOperator*{\smallbigcap}{\textstyle\bigcap}
\DeclareMathOperator*{\bigintersect}{\mathchoice
    {\smallbigcap}%
    {\bigcap}%
    {\bigcap}%
    {\bigcap}%
}


\DeclarePairedDelimiterX{\set}[2]{\lbrace}{\rbrace}{#1\;\delimsize\vert\;#2}

\newcommand{\defeq}{\coloneqq}
\renewcommand{\phi}{\varphi}
\newcommand{\iu}{\mathrm{i}\mkern1mu}
\DeclareMathOperator{\e}{\mathrm{e}}

\newcommand{\ball}[3][]{%
    \ifstrempty{#1}%
        {%
            b\auxdelimparencomma{#2}{#3}%
        }{%
            b_{#1}\auxdelimparencomma{#2}{#3}%
        }%
}

\newcommand{\converges}[1]{\xrightarrow[#1]{}}
\DeclareMathOperator{\supp}{supp}
\let\oldvec\vec
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\Tr}[1][]{%
    \ifstrempty{#1}%
        {%
            \operatorname{Tr}%
        }{%
            \operatorname{Tr}_{#1}%
        }%
}


\usepackage{listofitems}
\setsepchar{,}

\makeatletter
\newcommand{\mat@dims}[1]{%
    \readlist*\@dims{#1}%
    \ifnum \@dimslen=1
        \def\@dimsout{\@dims[1]}%
    \else
        \def\@dimsout{\@dims[1], \@dims[2]}%
    \fi
    \@dimsout
}


\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\matGL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\trans}{^{\top}}
\newcommand{\mat}[2]{M_{\mat@dims{#1}}(#2)}

\makeatother

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\clSpan}{\overbar{0.5}{1.5}{span}}

\newcommand\inv{^{\langle-1\rangle}}
\newcommand{\preim}[2][]{^{-1}\auxdelimparen[#1]{#2}}
\newcommand{\image}[2][]{\auxdelimbracket[#1]{#2}}

\newcommand{\dsupp}[2][]{\mathrm{Sp}_d\auxdelimparen[#1]{#2}}

\usepackage{xcolor}
\usepackage{biolinum}
\usepackage{mathpartir}



% Chapter style section-like
\makeatletter
\makechapterstyle{articlestyle}{%
  \chapterstyle{default}
  \setlength{\beforechapskip}{3.5ex \@plus 1ex \@minus .2ex}
  \renewcommand*{\chapterheadstart}{\vspace{\beforechapskip}}
  \setlength{\afterchapskip}{2.3ex \@plus .2ex}
  \renewcommand{\printchaptername}{}
  \renewcommand{\chapternamenum}{}
  \renewcommand{\chaptitlefont}{\normalfont\scshape\Large\bfseries\color{white!20!black}}
  \renewcommand{\chapnumfont}{\chaptitlefont}
  \renewcommand{\printchapternum}{\chapnumfont \thechapter~~\ensuremath{\diamond}~~}%\quad}
  \renewcommand{\afterchapternum}{}}
\makeatother

\chapterstyle{articlestyle}


%   \setsecheadstyle{\normalfont\Large\bfseries\color{white!20!black}}
\setsecheadstyle{\normalfont\large\bfseries}
\setsubsecheadstyle{\normalfont\large\itshape}
%   \setsecnumformat{\csname the#1\endcsname~~\ensuremath{\diamond}~~}

\setsecnumformat{\csname gablin#1\endcsname}
% \newcommand{\gablinsection}{{\thesection~~\ensuremath{\diamond}~~}}
\newcommand{\gablinsection}{{\thesection~~{\normalsize\textbullet}~~}}
\newcommand{\gablinsubsection}{{\thesubsection.~~}}
\newcommand{\gablinparagraph}{{\normalfont\theparagraph}}

% Paragrahs
\maxsecnumdepth{paragraph}
\renewcommand{\theparagraph}{(\alph{paragraph})}
\crefname{paragraph}{paragraph}{paragraph}
\crefformat{paragraph}{#2§#1#3}
\crefformat{chapter}{#2§#1#3}
\crefformat{section}{#2§#1#3}
\makeatletter
\renewcommand{\p@paragraph}{\thesection} % https://tex.stackexchange.com/questions/531572/full-references-to-subordinated-sections-with-cref
\makeatother
\newcommand{\newpar}{\paragraph{}}
\setbeforeparaskip{.5\baselineskip}


\begin{document}

\maketitle

% \newcommand{\hastype}[4]{#1 \mid #2 \vdash #3 : #4}
\newcommand{\hastype}[5]{%
    \ifstrempty{#1}%
        {%
            \ifstrempty{#2}%
            {%
                #3 \vdash #4 : #5%
            }{%
                #2 \mid #3 \vdash #4 : #5%
            }%
        }{%
            \ifstrempty{#2}%
            {%
                #1 \mid \emptyset \mid #3 \vdash #4 : #5%
            }{%
                #1 \mid #2 \mid #3 \vdash #4 : #5%
            }%
        }%
}

\newcommand{\stotype}[4]{%
    \ifstrempty{#1}%
        {%
            \ifstrempty{#2}%
            {%
                #3 \vdash #4%
            }{%
                #2 \mid #3 \vdash #4%
            }%
        }{%
            \ifstrempty{#2}%
            {%
                #1 \mid \emptyset \mid #3 \vdash #4%
            }{%
                #1 \mid #2 \mid #3 \vdash #4%
            }%
        }%
}


\newcommand{\step}{\to}
\newcommand{\headstep}{\to_h}
\newcommand{\purestep}{\to_p}
\newcommand{\infrule}[1]{{\normalfont\textsc{#1}}}
\newcommand{\hole}{-}
\newcommand{\unitval}{()}
\newcommand{\STLC}{\lambda_{\rightarrow}}

% \newcommand{\keyword}[1]{\textbf{\textit{#1}}}
\newcommand{\pto}{\rightharpoonup}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\ran}{\operatorname{ran}}

\newcommand{\textlang}[1]{\textsf{#1}}
\newcommand{\langkw}[1]{\textlang{\color{objlangcolor} #1}}
\newcommand{\Fst}{\operatorname{\langkw{fst}}}
\newcommand{\Snd}{\operatorname{\langkw{snd}}}
\renewcommand{\prod}{\times}

\newcommand{\objlang}[1]{{\normalfont\textsf{\textcolor{objlangcolor}{#1}}}}
\definecolor{objlangcolor}{HTML}{A91616}

\newcommand{\objOp}[1]{\operatorname{\objlang{#1}}}
\newcommand{\objDelim}[1]{\objlang{(}#1\objlang{)}}

\newcommand{\objFst}[1]{\objOp{fst}#1}
\newcommand{\objSnd}[1]{\objOp{snd}#1}
\newcommand{\objInl}[1]{\objOp{inj}_{\objlang{1}}#1}
\newcommand{\objInr}[1]{\objOp{inj}_{\objlang{2}}#1}

\newcommand{\objPair}[2]{\objDelim{#1\mathpunct{\objlang{,}}#2}}
\newcommand{\objUnit}{\objlang{()}}
\newcommand{\objRec}[3]{\objOp{rec}#1\objDelim{#2} \mathrel{\textcolor{objlangcolor}{\ensuremath{\coloneqq}}} #3}
\newcommand{\objApp}[2]{#1\,#2}
\newcommand{\objAss}[2]{#1 \mathrel{\textcolor{objlangcolor}{\ensuremath{\coloneqq}}} #2}

\newcommand{\objMatch}[4]{\objlang{match} \;#1\, \objlang{with}\: \objInl{#2} \mathbin{\textcolor{objlangcolor}{\Rightarrow}} #3 \mathbin{\textcolor{objlangcolor}{\mid}} \objInr{#2} \mathbin{\textcolor{objlangcolor}{\Rightarrow}} #4 \,\objlang{end}} % TODO xcolor now has \mathcolor? Have to update TeXlive?

\newcommand{\objForall}[2]{\objApp{\textcolor{objlangcolor}{\Lambda}}{#2}}

\newcommand{\setVar}{\mathit{Var}}
\newcommand{\setTVar}{\mathit{TVar}}
\newcommand{\setLoc}{\mathit{Loc}}
\newcommand{\setSto}{\mathit{Sto}}
\newcommand{\setExp}{\mathit{Exp}}
\newcommand{\setVal}{\mathit{Val}}
\newcommand{\setType}{\mathit{Type}}
\newcommand{\setECtx}{\mathit{ECtx}}
\newcommand{\setTAnn}{\mathit{TAnn}}
\newcommand{\typeUnit}{{\normalfont\textsf{Unit}}}
\newcommand{\freevar}[1]{\mathit{FV}(#1)}
\newcommand{\freeTvar}[1]{\mathit{FTV}(#1)}
% \newcommand{\powersetcard}[2]{\calP_\omega(#1)}
\newcommand{\pmaps}[3][]{#2 \pto_{#1} #3}
\newcommand{\typeForall}[2]{\forall #1. #2}
\newcommand{\typeExists}[2]{\exists #1. #2}
\newcommand{\typeRef}[1]{{\normalfont\textsf{ref(}#1\textsf{)}}}
\newcommand{\objTapp}[2]{\objApp{#1}{\textcolor{objlangcolor}{\_}}}
\newcommand{\objPack}[1]{\objOp{pack}#1}
\newcommand{\objUnpack}[3]{\objlang{unpack}\,#1\,\objlang{as}\,#2\,\objlang{in}\,#3}
\newcommand{\objRef}[1]{\objOp{ref}#1}
\newcommand{\objLoad}[1]{\objOp{!}#1}


\chapter{Preliminaries}

If $X$ and $A$ are sets, then we write $A \subseteq_\kappa X$ if $A$ is a subset of $X$ with cardinality strictly less than $\kappa$. We denote the power set of $X$ by $\powerset{X}$, and we furthermore write $\powersetcard{\kappa}{X}$ for the collection of sets $A$ with $A \subseteq_\kappa X$. We do not distinguish between the ordinal $\omega$ and the cardinal $\aleph_0$, so we write $A \subseteq_\omega X$ if $A$ is a finite subset of $X$, and $\powersetfin{X}$ similarly denotes the set of finite subsets of $X$. If $Y$ is another set, then we denote the set of partial maps from $X$ to $Y$ by $\pmaps{X}{Y}$. For $f \in \pmaps{X}{Y}$ we let $\dom f$ and $\ran f$ denote the domain and range of $f$, respectively. We also write $\pmaps[\kappa]{X}{Y}$ for the subset of $\pmaps{X}{Y}$ of partial maps whose domain has cardinality strictly less than $\kappa$, and elements in $\pmaps[\omega]{X}{Y}$ will be called \keyword{finite partial maps}. For $x_0 \in X$ and $y_0 \in Y$ we denote by $f[x_0 \mapsto y_0]$ the partial map given by
%
\begin{equation*}
    f[x_0 \mapsto y_0](x) =
    \begin{cases}
        y_0, & x = x_0, \\
        f(x), & x \in \dom f \setminus \{x_0\}.
    \end{cases}
\end{equation*}
%
Notice in particular that $\dom f[x_0 \mapsto y_0] = \dom f \union \{x_0\}$, so that if $f \in \pmaps[\omega]{X}{Y}$, then also $f[x_0 \mapsto y_0] \in \pmaps[\omega]{X}{Y}$.

If $\Xi \subseteq_\omega \setTVar$, $\Gamma$ is a type context and $\Sigma$ is a store typing, then we say that a store $\sigma$ is \keyword{well-typed} with respect to $\Xi$, $\Gamma$ and $\Sigma$ if $\dom \sigma = \dom \Sigma$ and $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$ for all $l \in \dom \sigma$. In this case we write $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$, and if $\Xi$ and $\Gamma$ are both empty we simply write $\stotype{}{}{\Sigma}{\sigma}$.

% OLD
% Progress: Let $e$ be an expression and $\tau$ a type. If $\hastype{}{}[\Sigma}{e}{\tau}$, then either $e$ is a value or there exists an expression $e'$ such that $e \step e'$ and $\hastype{}{}{e'}{\tau}$. TODO the type of $e'$ is part of preservation, isn't it??

% The proof is by induction on the typing relation $\hastype{}{\Gamma}{e}{\tau}$, but the claim to be proved is augmented by \textquote{or $\Gamma$ is non-empty}.\footnote{This ensures that we can perform the induction on the entire ternary relation, which is important since this relation is the one that is defined by the inference rules, \emph{not} the corresponding binary relation obtained by restricting the ternary relation to the subset where $\Gamma = \emptyset$.} Notice that the induction step for each inference rule is trivial if $\Gamma$ is non-empty, so we need only prove the induction step when $\Gamma = \emptyset$.

% \infrule{T-unit} Since $\unitval$ is a value, this follows.

% \infrule{T-var} Since we may assume that $\Gamma$ is empty, this implication is vacuously true.

% \infrule{T-pair} Assume that $\hastype{}{}{e_1}{\tau_1}$ and $\hastype{}{}{e_2}{\tau_2}$. If both $e_1$ and $e_2$ are values, then $(e_1,e_2)$ is also a value, so assume that only $e_1 = v_1$ is a value and that $e_2 \step e_2'$ with $\hastype{}{}{e_2'}{\tau_2}$. Then by definition of the one-step relation, there must be an evaluation context $K$ and basic [TODO what to call them?] subexpressions $d_2$ and $d_2'$ of $e_2$ and $e_2'$ such that $e_2 = K[d_2]$, $e_2' = K[d_2']$ and $d_2 \headstep d_2'$. Letting $K' = (v_1,K)$ it follows that $(v_1,e_2) = K'[d_2]$ and $(v_1,e_2') = K'[d_2']$, and so
% %
% \begin{equation*}
%     (v_1,e_2)
%         = K'[d_2]
%         \step K'[d_2']
%         = (v_1,e_2').
% \end{equation*}
% %
% Finally, since $d_2$ and $d_2'$ have the same type [TODO], by [TODO ECtx typing lemma] $K'[d_2']$ is well-typed and has the same type as $K'[d_2]$.

% If instead $e_1$ is not a value, then the same argument (using the evaluation context $(K,e_2)$) yields the same result.


---------

Let $X$ be a set, and let $P$ be a predicate on $X$ (i.e., a unary relation on -- or simply a subset of -- $X$). For $x \in X$ we write $P(x)$ to mean $x \in P$. If $f \colon X^k \pto X$ is a partial function for some $k \in \naturals$, then we say that $P$ \keyword{satisfies} the inference rule $R_f$ if $P(x_1) \land \cdots \land P(x_k)$ implies $P(f(x_1,\ldots,x_k))$ for all $(x_1,\ldots,x_k) \in \dom f$. We write the inference rule on the form
%
\begin{equation*}
    \frac{P(x_1) \quad \cdots \quad P(x_k)}{P(f(x_1,\ldots,x_k))},
\end{equation*}
%
where it is implicit that $(x_1,\ldots,x_k)$ lies in $\dom f$.

As an example, in the simply typed lambda calculus $X$ is the set of triples $(\Gamma,e,\tau)$, where $\Gamma$ is a type context, $e$ an expression and $\tau$ a type. The function $f$ might be unary and have as domain the set of triples such that $\tau$ is a product type, say $\tau_1 \prod \tau_2$, and it might then map $(\Gamma,e,\tau_1 \prod \tau_2)$ to $(\Gamma, \Snd e, \tau_2)$. Of course, using the conventional notation we would write e.g. the statement $P(\Gamma,e,\tau_1 \prod \tau_2)$ as $\hastype{}{\Gamma}{\Sigma}{e}{\tau_1 \prod \tau_2}$, and $P(\Gamma, \Snd e, \tau_2)$ would be written $\hastype{}{\Gamma}{\Sigma}{\Snd e}{\tau_2}$, so the rule $R_f$ would then just be the rule $\infrule{T-snd}$.

We also wish to require a given predicate to satisfy a collection of axioms. As motivation, notice that the antecedent of the rule \infrule{T-var} is not on the same form as the consequent. That is, it is not an inductive rule, and it can thus be stated as an axiom. To wit, if $(e : \tau) \in \Gamma$ then $P(\Gamma,e,\tau)$. Another way to describe this is as the set $\set{(\Gamma,e,\tau)}{(e : \tau) \in \Gamma}$. Hence we may simply use subsets of the universal set $X$ as axioms. Of course, if $\calA$ is a collection of axioms, then $\bigunion \calA$ is also an axiom. Hence if given any number [TODO set many?] of axioms, we can always take their union and assume that there is only one axiom.

Given an axiom [TODO or set of axioms?] $A$ and a collection $\calR$ of inference rules, we first define a function $F \colon \powerset{X} \to \powerset{X}$ by
%
\begin{equation*}
    F(B)
        = \bigunion_{R \in \calR} f_R(B^{\rho(R)}),
\end{equation*}
%
where $f_R$ is some partial function inducing $R$, and $\rho(R)$ is the arity of $f_R$. For $A \subseteq X$ we further let $F_A(B) = F(B) \union A$ [TODO alternatively consider sublattice of sets $\supseteq A$]. It is clear that a set $P$ is a fixed-point of $F$ if and only if $P$ is closed under application of the inference rules in $\calR$. It is furthermore easy to show that $P$ is a fixed-point of $F_A$ if and only if $P$ is a fixed-point of $F$ which contains $A$ [TODO need that $B \subseteq F(B)$, maybe assume that $\calR$ contains the identity/trivial rule?]. Now notice that $\powerset{X}$ is a complete lattice (with respect to set inclusion), and that $F_A$ is order-preserving. The Knaster--Tarski fixed-point theorem [TODO ref] then implies that $F_A$ has a least fixed-point\footnote{It also has a greatest fixed-point, but this is trivially $\powerset{X}$ itself and is of no interest to us.}. We say that this fixed-point is the predicate \keyword{induced} by $\calR$ and $A$.

We would also like a way to generate the predicate $P$ induced by $\calR$ and $A$. Let $A_0 = A$, and for $n \in \naturals$ define
%
\begin{equation*}
    A_{n+1}
        = F_{A_n}(A_n)
        = A_n \union \bigunion_{R \in \calR} f_R(A_n^{\rho(R)}).
\end{equation*}
%
The sequence $(A_n)_{n \in \naturals}$ is thus increasing. Letting $P = \bigunion_{n \in \naturals} A_n$, we then have the following result:

\begin{proposition}
    $P$ is the smallest predicate on $X$ which is true on all elements in $A$ and which satisfies all inference rules in $\calR$.
\end{proposition}

\begin{proof}
    Clearly $P$ is true on $A$, so consider an inference rule $R \in \calR$ and let $(x_1,\ldots,x_k) \in \dom f_R$ be such that $P(x_1), \ldots, P(x_k)$ hold, i.e. such that $x_1, \ldots, x_k \in P$. We must then show that also $f_R(x_1, \ldots, x_k) \in P$. Then there must be indices $n_1, \ldots, n_k$ such that $x_i \in A_{n_i}$. Letting $n = \max\{n_1, \ldots, n_k\}$ we thus have\footnote{This is where the proof fails if the arity of $f_R$ is infinite. For instance, the proof does not go through for $\sigma$-algebras -- where $f_R$ might map a sequence $(B_n)_{n \in \naturals}$ of sets to the union $\bigunion_{n \in \naturals} B_n$ -- nor for topologies.} $x_1, \ldots, x_k \in A_n$. Hence $f_R(x_1, \ldots, x_k) \in f_R(A_n^k)$, so by definition of $A_{n+1}$ we have $f_R(x_1, \ldots, x_k) \in A_{n+1} \subseteq P$ as desired.

    Conversely we show that $P$ is the smallest such predicate. But this is clear since it must be contained in every predicate containing $A$ and satisfying the inference rules.
\end{proof}

\begin{corollarynoproof}
    For every $x \in P$ either $x \in A$ or there is a smallest $n \in \naturals$ such that $x \in f_R(A_n^{\rho(R)})$ for some $R \in \calR$.
\end{corollarynoproof}
%
Note that while $n$ is uniquely determined, $R$ may not be. We say that an inference rule $R$ satisfying the corollary is the rule \keyword{applied last} in the derivation of $x$. The intuition is that we construct a derivation tree for $x$, and $R$ is the rule at the root of the tree.

We are of course used to these kinds of results in computer science, and we rarely stop to spell out the details. Since $P$ is the smallest predicate on $X$ with the above properties, we obtain a notion of structural induction or rule induction, and this is often sufficient to prove properties of $P$. But sometimes it is at least much more convenient to appeal to the \textquote{last inference rule applied}, as we shall see.\footnote{As mentioned, the proof of TODO does not go through for $\sigma$-algebras. The best we can do is apparently the following: If $\calD = \calD_0$ is a collection of subsets of $X$, let $\sigma(\calD)$ denote the smallest $\sigma$-algebra on $X$ containing $\calD$. If $\alpha$ is a countable ordinal with an immediate predecessor $\beta$, then we define $\calD_\alpha$ by transfinite induction to be the set of countable unions of elements of $\calD_\beta$ and complements of such sets, and otherwise we let $\calD_\alpha = \bigunion_{\beta < \alpha} \calD_\beta$. Then one can show that $\sigma(\calD) = \bigunion_{\alpha \in \Omega} \calD_\alpha$, where $\Omega$ is the set of countable ordinals. Cf. Folland TODO. Note however that we by minimality of $\sigma(\calD)$ do get a notion of induction, just as for predicates defined by inference rules. \par For topologies we do have a simple characterisation of the topology $\calT$ generated by a collection of sets $\calS$. In this case $\calS$ is a subbasis for $\calT$, i.e., every set in $\calT$ is a (generally infinite) union of finite intersections of elements in $\calS$. But notice that infinitely many set operations (finite intersections or arbitrary unions) are still required to obtain a general open set, since we take a finite intersection infinitely many times. \par On the other hand, results such as TODO ref are of course standard fare in algebra, where we obtain a similar description of the subalgebra generated by a subset of some algebra. See e.g. Bergman TODO}

% \newcommand{\wand}{\mathrel{-\!\!*}}
% \newcommand{\wand}{\mathrel{-\mkern-9mu-\mkern-7mu*}}
% \newcommand{\revast}{\text{\reflectbox{$*$}}}
% \newcommand{\wand}{\mathrel{-\mkern-10mu-\mkern-7mu\revast}}

% \begin{equation*}
%     \inferrule*[right=$\wand$E]{
%         \inferrule*{}{
%             P \vdash Q \wand R
%         }
%         \and
%         \inferrule*[right=Asm]{ }{
%             Q \vdash Q
%         }
%     }{
%         P * Q \vdash R
%     }
% \end{equation*}

% \begin{equation*}
%     \inferrule*[right=$\land$I]{
%         \inferrule*[right=$*$-weak]{ }{
%             P * Q \vdash P
%         }
%         \and
%         \inferrule*[right=Trans]{
%             \inferrule*[right=$*$-comm]{ }{
%                 P * Q \vdash Q * P
%             }
%             \and
%             \inferrule*[right=$*$-weak]{ }{
%                 Q * P \vdash Q
%             }
%         }{
%             P * Q \vdash Q
%         }
%     }{
%         P * Q \vdash P \land Q
%     }
% \end{equation*}

% Preservation: If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $e \step e'$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau}$.


\chapter{General stuff about languages}

\section{Syntax}

We describe the general framework in which we may describe various programming languages, introducing the concepts that will later be defined precisely in the concrete setting of System~F.

We first fix countable sets $\setVar$ of variables and $\setLoc$ of locations. The \keyword{expressions} of the language will be a set $\setExp$ containing both $\setVar$ and $\setLoc$, and we designate some of these expressions to be \keyword{values}, collected in a set $\setVal$. We think of an expression as specifying (part of) the state of the program, and values are states in which the computation of the program has finished. All locations will also be values, and these are supposed to model memory addresses. The memory state of the program (i.e. the part of the memory that the program has access to) is modelled by a \keyword{store}\footnote{Sometimes called a \keyword{heap}, but this has nothing to do with the heap \emph{data structure}.}, which is an element of $\pmaps[\omega]{\setLoc}{\setExp}$. We simply write $\setSto$ for this set of maps.

For $e \in \setExp$ we define the set $\freevar{e} \subseteq \setVar$ of \keyword{free variables}. There are various ways of binding variables in expressions, and the notion of free variables is supposed to capture the idea that variables can be bound, e.g. by lambda abstraction, and hence also \emph{not} bound. If $\freevar{e} = \emptyset$, then we say that $e$ is \keyword{closed}.

When defining the set of expressions of a language, we are strictly speaking defining a concrete syntax for the language. However, while we write expressions as a linear sequence of characters, they are thought of as describing an abstract syntax. But since writing abstract syntax trees quickly becomes impractical, we instead express them using a more convenient linear shorthand. If $e_1$, $e_2$ and $e_3$ are expressions, an expression of the language might be $e_1\,e_2\,e_3$, but this might have two different derivations corresponding to $(e_1\,e_2)\,e_3$ and $e_1\,(e_2\,e_3)$, and to disambiguate the expression $e_1\,e_2\,e_3$ we must TODO [TODO Mogensen].


\section{Type system}

We next define a set $\setType$ of types. This includes a countable set $\setTVar$ of type variables, any base types (e.g. unit, integer, and boolean types) as well as more complex types that can be constructed recursively from the base types. It also includes reference types, which are the types of locations. For $\tau \in \setType$ we define the set of \keyword{free type variables} $\freeTvar{\tau} \subseteq \setTVar$. As with free variables, these can be bound in various ways. If $\freeTvar{\tau} = \emptyset$, then we also say that $\tau$ is \keyword{closed}. A pair $(e,\tau)$ of an expression and a type is usually written $e : \tau$. An expression may also have types as subexpressions, for instance if a lambda abstraction has a type annotation on its parameter.

If an expression $e$ has free variables, then to specify the type of $e$ it is necessary to first specify the types of the free variables in $e$. This is done using a \keyword{type context}, which is an element of $\pmaps[\omega]{\setVar}{\setType}$. If $e$ is supposed to be well-typed in a type context $\Gamma$, then we of course require that $\freevar{e} \subseteq \dom \Gamma$. That is, $\Gamma$ must in fact specify the types of the variables that occur free in $e$.

Furthermore, if $e$ has a location as a subexpression, then we need to be able to look up the type of the expression stored at this location in order to specify the type of $e$. Hence the type of $e$ will also depend on the store in question. However, it not in general possible to deduce the type of $e$ in this case: If $\sigma$ is a store, $l_1,l_2 \in \dom \sigma$, and if $\sigma(l_1)$ references $l_2$ and $\sigma(l_2)$ references $l_1$, then it is impossible to deduce the type of $\sigma(l_1)$. Hence the type of $e$ will instead depend on a given \keyword{store typing}, which is an element of $\pmaps[\omega]{\setLoc}{\setType}$. We of course require that the store typing in question actually contains in its domain all locations referenced in $e$, and we furthermore require that all free variables of $e$ lie in the domain of the current type context.

It may be that the type of $e$ or of the types of the variables in the type context $\Gamma$ or of the expressions in the store typing $\Sigma$ contain type variables. In order to keep track of these we collect these in a (finite) set $\Xi$ and require that the free type variables in $\Gamma$ and $\Sigma$, defined by
%
\begin{equation*}
    \freeTvar{\Gamma}
        = \bigunion_{\tau \in \ran \Gamma} \freeTvar{\tau}
    \quad \text{and} \quad
    \freeTvar{\Sigma}
        = \bigunion_{\tau \in \ran \Sigma} \freeTvar{\tau},
\end{equation*}
%
are contained in $\Xi$. A variable $x$ is called \keyword{fresh} for $\Gamma$ if $x \not\in \dom \Gamma$. The finitude of $\dom \Gamma$ ensures that there always exist fresh variables (recall that there are countably infinitely many variables). If $\Delta$ is another type context such that $\dom \Gamma \intersect \dom \Delta = \emptyset$, then instead of $\Gamma \union \Delta$ we simply write $\Gamma,\Delta$. Furthermore, if $\Delta = \{x_1 : \tau_1, \ldots, x_n : \tau_n\}$ for distinct $x_i$, then we omit the braces and write $\Gamma, x_1 : \tau_1, \ldots, x_n : \tau_n$. If $\Xi$ and $\Phi$ are finite disjoint subsets of $\setTVar$, then we similarly write $\Xi,\Phi$ for $\Xi \union \Phi$, and if $\Phi = \{X_1, \ldots, X_n\}$ for distinct $X_i$, then we also write $\Xi, X_1, \ldots, X_n$.

The semantics of the type system is captured by a five-place relation on the set
%
\begin{equation*}
    \powersetfin{\setTVar}
        \prod (\pmaps[\omega]{\setVar}{\setType})
        \prod (\pmaps[\omega]{\setLoc}{\setType})
        \prod \setExp
        \prod \setType,
\end{equation*}
%
where an element $(\Xi,\Gamma,\Sigma,e,\tau)$ of this relation is written $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and is called a \keyword{type derivation}. This relation is usually defined recursively, by specifying a series of axioms and inference rules\footnote{Or more usually axiom and rule \emph{schemas}. TODO Pierce}. If $\Xi = \emptyset$, then we write $\hastype{}{\Gamma}{\Sigma}{e}{\tau}$ [TODO do we need this?], and we furthermore write $\hastype{}{}{\Sigma}{e}{\tau}$ if also $\Gamma = \emptyset$.


\section{Dynamics}

The operational semantics of the language is specified in a small-step style. We describe this semantics in stages, beginning with the \keyword{pure head reductions}. These are evaluations that can be performed (1) on expressions that have no subexpressions that can be evaluated, (2) without reading or modifying the store. More precisely, we specify a binary relation $\purestep$ on $\setExp$, such that $e \purestep e'$ is supposed to mean that $e$ evaluates to or reduces to $e'$.

Going one level up we define the relation $\headstep$ on $\setSto \prod \setExp$ of (not necessarily pure) \keyword{head reductions}. These are reductions that may affect and be affected by the contents of the store. Of course, if $\sigma$ is a store and $e \purestep e'$, then we have $(\sigma,e) \headstep (\sigma,e')$, but we augment the pure head reductions with reductions that e.g. read from or write to the store.

Finally we need a way to evaluate complex expressions. One way of doing this is to specify the evaluation rules for all expressions immediately instead of going through head reductions (this is the approach taken by Pierce TODO). Another is to introduce \keyword{evaluation contexts}, which are (essentially) maps $\setExp \to \setExp$. If $K$ is an evaluation context and $e$ is an expression, then we write $K[e]$ for the value of $K$ at $e$. We then define the final reduction relation $\step$ on $\setSto \prod \setExp$ by letting $(\sigma, K[e]) \step (\sigma', K[e'])$ if $(\sigma,e) \headstep (\sigma',e')$.

One role of evaluation contexts is to specify the evaluation order of complex expressions, e.g. if the evaluation of function applications is call-by-value or call-by-name, or if we evaluate the arguments to functions left-to-right or right-to-left. The possibilities thus depend on the available evaluation contexts.

TODO multiple threads


\section{Lambda calculus}

The syntax of the untyped lambda calculus consists only of variables, abstractions and applications:
%
%
\begin{alignat*}{2}
    && x &\in \setVar \\
    & \setExp \quad & e &\Coloneqq x \mid \lambda x.e \mid e\,e \\
    & \setVal \quad & v &\Coloneqq \lambda x.e
\end{alignat*}
%
This means that there in particular are expressions on the form $e_1\,e_2\,e_3$, and we use the convention that application is left-associative, i.e. that the above expression is to be read $(e_1\,e_2)\,e_3$. [TODO build into the grammar, Mogensen]

The small-step reduction relation $\step$ on expressions formalises how to reduce expressions. For instance, the rule
%
\begin{equation*}
    \inferrule*[right=E-app-abs]{ }{
        (\lambda x . e_1) \, e_2 \step e_1[x \mapsto e_2]
    }
\end{equation*}
%
says that we can apply abstractions to other expressions. Such a rule is called a \keyword{computation rule}, and an expression $(\lambda x . e_1) \, e_2$ is called a \keyword{redex}. Rewriting a redex according to the above rule is called \keyword{$\beta$-reduction}.

A more complex expression might not itself be a redex but instead have a redex as a subexpression. In this case we need other rules, so-called \keyword{congruence rules}, which tell us how to reduce complex expressions by reducing subexpressions. For instance, in the expression $e_1\,e_2$, do we evaluate $e_1$ before $e_2$ or vice versa? That is, does evaluation happen left-to-right or right-to-left, or do we allow this to be determined arbitrarily? This is of course especially important in languages with side-effects. Formally we impose an evaluation order by having either (or both) of the congruence rules
%
\begin{equation*}
    \inferrule*[right=E-app1 \quad {\normalfont and} \quad]{
        e_1 \step e_1'
    }{
        e_1 \, e_2 \step e_1' \, e_2
    }
    % \quad \text{and} \quad
    \inferrule*[right=E-app2.]{
        e_2 \step e_2'
    }{
        e_1 \, e_2 \step e_1 \, e_2'
    }
\end{equation*}
%
If we desire right-to-left evaluation order, then we choose \infrule{E-app2}, but we also need a restricted form of \infrule{E-app1}, namely
%
\begin{equation*}
    \inferrule*[right=E-app1'.]{
        e \step e'
    }{
        e \, v \step e' \, v
    }
\end{equation*}
%
That is, only when the right expression has been reduced to a value $v$ can we evaluate the left expression.

We may also formalise the evaluation order by defining the reduction relation in terms of head reductions $\headstep$, and using evaluation contexts to impose an evaluation order. For instance,
%
\begin{equation*}
    K \Coloneqq \hole \mid K \, e \mid v \, K
    \quad \text{and} \quad
    K \Coloneqq \hole \mid e \, K \mid K \, v
\end{equation*}
%
define evaluation contexts for left-to-right and right-to-left evaluation, respectively. The symbol \enquote{$\hole$} is called the \keyword{hole}, and we think of the hole as the place into which we substitute the expression $e$ when writing $K[e]$.

Computation and congruence rules together might also allow for different evaluation strategies, for instance:
%
\begin{enumerate}
    \item Full $\beta$-reduction: We may reduce \emph{any} redex contained in an expression.

    \item Normal order reduction: We must reduce the leftmost, outermost redex first.

    \item Call-by-name: The subexpression $e_2$ of a redex $(\lambda x . e_1) \, e_2$ cannot be reduced. Instead, we must perform $\beta$-reduction without reducing $e_2$.

    \item Call-by-value: Instead, $e_2$ \emph{must} be reduced to a value before $\beta$-reduction can take place.
\end{enumerate}
%
For instance, in call-by-value we might have a restricted form of the rule \infrule{E-app-abs}, namely
%
\begin{equation*}
    \inferrule*[right=E-app-abs'.]{ }{
        (\lambda x . e) \, v \step e[x \mapsto v]
    }
\end{equation*}
%
That is, the argument must be a value $v$ for the reduction to take place. If we use evaluation contexts, this computation rule would be a rule concerning head reductions $\headstep$.


Since the untyped lambda calculus does not allow abstractions to be named, it is not obvious how to define recursive functions. TODO


\section{Recursion}

Call by name: $Y = \lambda f . (\lambda x . f (xx)) (\lambda x . f (xx))$

\begin{align*}
    Y g
        &= \lambda f . (\lambda x . f (xx)) (\lambda x . f (xx)) g \\
        &\step (\lambda x . g (xx)) (\lambda x . g (xx)) \\
        &\step g ((\lambda x . g (xx))(\lambda x . g (xx)))
\end{align*}
%
On the other hand we also have
%
\begin{align*}
    g (Y g)
        &= g (\lambda f . (\lambda x . f (xx)) (\lambda x . f (xx)) g) \\
        &\step g ((\lambda x . g (xx))(\lambda x . g (xx))).
\end{align*}
%
That is, $Y g$ and $g (Y g)$ reduce to the same expression.

Call by value: $Z = \lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y))$

\begin{align*}
    Z g
        &= \lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y)) g \\
        &\step (\lambda x . g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)) \\
        &\step g (\lambda y. (\lambda x. g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)) y) \\
        &\step g (  )
\end{align*}

\begin{align*}
    g (Z g)
        &= g (\lambda f . (\lambda x . f (\lambda y. x x y)) (\lambda x. f (\lambda y. x x y)) g) \\
        &\step g ((\lambda x . g (\lambda y. x x y)) (\lambda x. g (\lambda y. x x y)))
\end{align*}


\chapter{System F}

The following grammar defines the syntax, the sets of values and types, and the evaluation contexts of System F:
%
\begin{alignat*}{2}
    && x &\in \setVar \\
    && l &\in \setLoc \\
    && X &\in \setTVar \\
    & \setExp \quad & e &\Coloneqq \objUnit \mid x \mid l \mid \objPair{e}{e} \mid \cdots \\
    & \setVal \quad & v &\Coloneqq \objUnit \mid l \mid \objPair{v}{v} \mid \objInl{v} \mid \cdots \\
    & \setType \quad & \tau &\Coloneqq \typeUnit \mid X \mid \typeRef{\tau} \mid \tau \prod \tau \mid \cdots \\
    & \setECtx \quad & K &\Coloneqq \hole \mid \objPair{K}{e} \mid \objPair{v}{K} \mid \cdots
\end{alignat*}
%
Notice that $\setLoc \subseteq \setVal \subseteq \setExp$ as we required in TODO ref. If $K$ is an evaluation context and $e$ is an expression, then we define $K[e]$ recursively by
%
\begin{align*}
    \hole[e] &= e \\
    \objPair{K}{e'}[e] &= \objPair{K[e]}{e'} \\
    \objPair{v}{K}[e] &= \objPair{v}{K[e]} \\
    & etc.
\end{align*}
%
It is easy to prove (by induction in $K$) that $K[e]$ is an expression, so every evaluation context $K$ can indeed be thought of as a map $\setExp \to \setExp$ given by $e \mapsto K[e]$.

If $e$ is an expression, then the set $\freevar{e}$ of free variables in $e$ is defined recursively as follows:
%
\begin{align*}
    \freevar{\objUnit} &= \emptyset \\
    \freevar{x} &= \{x\} \\
    \freevar{\objPair{e_1}{e_2}} &= \freevar{e_1} \union \freevar{e_2} \\
    & etc.
\end{align*}
%
Similarly, if $\tau$ is a type, then we define the set $\freeTvar{\tau}$ of free type variables in $\tau$ as follows:
%
\begin{align*}
    \freeTvar{\typeUnit} &= \emptyset \\
    \freeTvar{X} &= \{X\} \\
    \freeTvar{\tau_1 \prod \tau_2} &= \freeTvar{\tau_1} \union \freeTvar{\tau_2} \\
    & etc.
\end{align*}

We are now in a position to define the typing relation. This is the smallest relation on the set
%
\begin{equation*}
    \powersetfin{\setTVar}
        \prod (\pmaps[\omega]{\setVar}{\setType})
        \prod (\pmaps[\omega]{\setLoc}{\setType})
        \prod \setExp
        \prod \setType,
\end{equation*}
%
satisfying the following inference rules:

\begin{equation*}
    \inferrule*[right=T-var]{
        \freeTvar{\Gamma} \subseteq \Xi
        \and
        \freeTvar{\Sigma} \subseteq \Xi
        \and
        (x : \tau) \in \Gamma
    }{
        \hastype{\Xi}{\Gamma}{\Sigma}{x}{\tau}
    }
\end{equation*}


\begin{equation*}
    \inferrule*[right=T-loc]{
        \freeTvar{\Gamma} \subseteq \Xi
        \and
        \freeTvar{\Sigma} \subseteq \Xi
        \and
        l \in \dom \Sigma
    }{
        \hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\Sigma(l)}}
    }
\end{equation*}


Lemma: If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, then $\freeTvar{\Gamma} \subseteq \Xi$ and $\freevar{e} \subseteq \dom \Gamma$. In particular, if $\Xi = \emptyset$ then $\tau$ is closed, and if $\Gamma = \emptyset$ then $e$ is closed. TODO

TODO all type variables in tau also in Xi? All locations in e also in Sigma?

TODO lemma weakening?


\section{Lemmas}

\begin{lemma}[Inversion]
    \label{lem:inversion}
    Assume that $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$.
    %
    \begin{enumlem}
        \item\label{enum:inversion-variable} If $e = x$ is a variable, then $(x : \tau) \in \Gamma$.
        
        \item\label{enum:inversion-unit} If $e = \objUnit$, then $\tau = \typeUnit$.

        \item\label{enum:inversion-pair} If $e = \objPair{e_1}{e_2}$, then $\tau = \tau_1 \prod \tau_1$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau_2}$.

        \item\label{enum:inversion-fst} If $e = \objFst{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau \prod \tau_2}$.
        
        \item\label{enum:inversion-snd} If $e = \objSnd{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_1 \prod \tau}$.
        
        \item\label{enum:inversion-inl} If $e = \objInl{e'}$, then $\tau = \tau_1 + \tau_2$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_1}$.
        
        \item\label{enum:inversion-inr} If $e = \objInr{e'}$, then $\tau = \tau_1 + \tau_2$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau_2}$.

        \item\label{enum:inversion-match} If $e = \objMatch{e_1}{x}{e_2}{e_3}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1 + \tau_2}$ and $\hastype{\Xi}{\Gamma, x : \tau_1}{\Sigma}{e_2}{\tau}$ and $\hastype{\Xi}{\Gamma, x : \tau_2}{\Sigma}{e_3}{\tau}$.

        \item\label{enum:inversion-rec} If $e = \objRec{f}{x}{e'}$, then $\tau = \tau_1 \to \tau_2$ and $\hastype{\Xi}{\Gamma, f : \tau_1 \to \tau_2, x : \tau_1}{\Sigma}{e'}{\tau_2}$.

        \item\label{enum:inversion-app} If $e = \objApp{e_1}{e_2}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1 \to \tau}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau}$.

        \item\label{enum:inversion-forall} If $e = \objForall{X}{e'}$, then $\tau = \typeForall{X}{\tau'}$ and $\hastype{\Xi,X}{\Gamma}{\Sigma}{e'}{\tau'}$.
        
        \item\label{enum:inversion-tapp} If $e = \objTapp{e'}{X}$, then $\tau = \tau'[\tau''/X]$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\typeForall{X}{\tau'}}$.

        \item\label{enum:inversion-pack} If $e = \objPack{e'}$, then $\tau = \typeExists{X}{\tau'}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau'[\tau''/X]}$.

        \item\label{enum:inversion-unpack} If $e = \objUnpack{e_1}{x}{e_2}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\typeExists{X}{\tau'}}$ and $\hastype{\Xi,X}{\Gamma, x \colon \tau'}{\Sigma}{e_2}{\tau}$.

        \item\label{enum:inversion-fold} fold TODO

        \item\label{enum:inversion-unfold} unfold TODO

        \item\label{enum:inversion-location} If $e = l$ is a location, then $l \in \dom \Sigma$ and $\tau = \typeRef{\Sigma(l)}$.

        \item\label{enum:inversion-ref} If $e = \objRef{e'}$, then $\tau = \typeRef{\tau'}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau'}$.

        \item\label{enum:inversion-ass} If $e = \objAss{e_1}{e_2}$, then $\tau = \typeUnit$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\typeRef{\tau'}}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau'}$.

        \item\label{enum:inversion-load} If $e = \objLoad{e'}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\typeRef{\tau}}$.
    \end{enumlem}
\end{lemma}

\begin{proof}
    Notice that since the conclusions of different inference rules are distinct, there is a unique rule that was applied last in the derivation of $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ [TODO ref what that means]. This means that if $e$ has any of the above forms, then it must have the type as assigned by the relevant inference rule, and the assumptions of that rule must also hold.

    For instance, if there exist expressions $e_1$ and $e_2$ such that $e = \objPair{e_1}{e_2}$, then the last rule applied must be \infrule{T-pair}. But then $\tau$ must be on the form $\tau_1 \prod \tau_2$ for types $\tau_1$ and $\tau_2$. And furthermore, the assumptions must also hold, implying that $\hastype{\Xi}{\Gamma}{\Sigma}{e_1}{\tau_1}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e_2}{\tau_2}$. The other cases are proved in the same way.
\end{proof}

Notice the significance of the inversion lemma: While an expression can generally have many different types (if nothing else then due to substitution into type variables), it seems natural to believe that e.g. pairs cannot be of function type. The inversion lemma says precisely this, that if a pair has any type, then that type must be a product type. Note that the lemma does \emph{not} just say that a well-typed pair is of product type, it rather says that a well-typed pair is \emph{only} of product type.


\begin{lemma}[Canonical forms]
    \label{lem:canonical}
    Assume that $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau}$ where $v$ is a value.
    %
    \begin{enumlem}
        \item\label{enum:canonical-unit} If $\tau = \typeUnit$, then $v = \objUnit$.

        \item\label{enum:canonical-product} If $\tau = \tau_1 \prod \tau_2$, then $v = (v_1,v_2)$.
        
        \item\label{enum:canonical-sum} If $\tau = \tau_1 + \tau_2$, then either $v = \objInl{v'}$ or $v = \objInr{v'}$.

        \item\label{enum:canonical-function} If $\tau = \tau_1 \to \tau_2$, then $v = \objRec{f}{x}{e}$.

        \item\label{enum:canonical-forall} If $\tau = \typeForall{X}{\tau'}$, then $v = \objForall{X}{e}$.

        \item\label{enum:canonical-exists} If $\tau = \typeExists{X}{\tau'}$, then $v = \objPack{e}$.

        \item\label{enum:canonical-recursive} TODO fold

        \item\label{enum:canonical-ref} If $\tau = \typeRef{\tau'}$, then $v$ is a location.
    \end{enumlem}
\end{lemma}

\begin{proof}
    We assume that $\tau = \tau_1 \prod \tau_2$ for concreteness; the other cases are identical. In this case we simply check for each production of the grammar with non-terminal $v$ whether the relevant value can have type $\tau_1 \prod \tau_2$. For instance, \cref{enum:inversion-inl} implies that a value $\objInl{v'}$ can only be of sum type, and hence $v$ cannot be of this form. The only possibility is that $v$ is in fact a pair.
\end{proof}


--- TODO substitution lemma


\begin{lemma}
    If $\hastype{\Xi,X}{\Gamma}{\Sigma}{e}{\tau}$, then $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{e}{\tau[\tau'/X]}$.
\end{lemma}

\begin{proof}
\begin{proofsec*}
    \item[\infrule{T-var}]
    Assume that $\hastype{\Xi,X}{\Gamma}{\Sigma}{x}{\tau}$. By \cref{enum:inversion-variable} we thus have $(x : \tau) \in \Gamma$, so $(x : \tau[\tau'/X]) \in \Gamma[\tau'/X]$ by definition of substitution. But then \infrule{T-var} implies that $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{x}{\tau[\tau'/X]}$ (where we use that $X$ does not occur in the context or type, so it doesn't need to appear in $\Xi$).

    \item[\infrule{T-rec}]
    Assume that $\hastype{\Xi,X}{\Gamma}{\Sigma}{\objRec{f}{x}{e}}{\tau_1 \to \tau_2}$. By the inversion lemma [TODO ref -- also can't we just do induction??] we have $\hastype{\Xi,X}{\Gamma, f : \tau_1 \to \tau_2, x : \tau_1}{\Sigma}{e}{\tau_2}$, so by induction it follows that $\hastype{\Xi}{\Gamma[\tau'/X], f : \tau_1[\tau'/X] \to \tau_2[\tau'/X], x : \tau_1[\tau'/X]}{\Sigma}{e}{\tau_2[\tau'/X]}$ [TODO by substitution on envs, function types etc.]. Applying \infrule{T-rec} we obtain the desired claim.
\end{proofsec*}

TODO rest

% Antag Ξ, X | Γ ⊢ rec f(x) := e : τ₁ → τ₂ og vis Ξ | Γ[τ'/X] ⊢ rec f(x) := e : (τ₁ → τ₂)[τ'\X]
% Per inversion i antagelsen ved vi, at Ξ, X | Γ, f : τ₁ → τ₂, x : τ₁ ⊢ e : τ₂. Fra induktionshypotesen konkludrer vi så Ξ | Γ[τ'\X], f : τ₁[τ'\X] → τ₂[τ'\X], x : τ₁[τ'\X] ⊢ e : τ₂[τ'\X] idet (τ₁ → τ₂)[τ'\X] = τ₁[τ'\X] → τ₂[τ'\X] og da definitionen af substitution på miljøjer giver os, at (Γ, x : τ)[τ'\X] = Γ[τ'\X], x : τ[τ'\X]. Dernæst følger udsagnet, som vi vil vise direkte af T-rec.

% De interessante tilfælde er (surprise, surprise) derimod for T-Tlam og T-Ttapp.

% T-Tlam 
% Antag Ξ, X | Γ ⊢ Λ e : ∀ Y . τ og vis at Ξ | Γ[τ'\X] ⊢ Λ e : (∀ Y . τ)[τ'/X]
% Vær spids på, at jeg her udnytter, at vi ræsonnerer modulo "alpha-ækvivalens", dvs. at vi kan omnavngive (type)variable frit. Det gør jeg ved at gøre brug en frisk variabel Y og ræsonere med en implicit antagelse om at X ≠ Y.
% Per inversion i antagelsen får vi, at Ξ, X, Y | Γ ⊢ e : τ. Husk, at typemiljøer er mængder, dvs Ξ, X, Y = Ξ, Y, X, og dermed gælder (Ξ, Y), X | Γ ⊢ e : τ også. Brug nu induktionshypotesen med (Ξ, Y) som miljø, og vi får Ξ, Y | Γ[τ'\X] ⊢ e : τ[τ'\X]. Vi kan nu konkludere vha. T-Tlam og da (∀ Y . τ)[τ'/X] = ∀ Y . τ[τ'\X].

% T-Ttapp
% Antag Ξ, X | Γ ⊢ e _ : τ[τ''\Y] og vis at Ξ | Γ[τ'\X] ⊢ e _ : τ[τ''\Y][τ'\X]. Igen, vær opmærksom på, at jeg er meget påpasselig med variabelnavnene!
% Per inversion i antagelsen får vi, at Ξ, X | Γ ⊢ e : ∀ Y . τ. Fra induktionhypotesen følger at Ξ | Γ[τ'\X] ⊢ e : (∀ Y . τ)[τ'\X]. Vi bruger nu igen, at (∀ Y . τ)[τ'/X] = ∀ Y . τ[τ'\X] og konkludrerer, at Ξ | Γ[τ'\X] ⊢ e : ∀ Y . τ[τ'\X]. Fra T-Tapp følger nu, at Ξ | Γ[τ'\X] ⊢ e _ : τ[τ'\X][τ''\Y]. Vi er færdige antaget τ[τ'\X][τ''\Y] = τ[τ'\Y][τ''\X], hvilket er tilfældet da X ≠ Y.

% Som I kan se er det altså forholdvist ligetil at vise, givet et par egenskaber omkring substitution, som afhænger lidt af, hvordan vi helt præcist formaliserer denne (som diskuteret i TAPL). Men det er helt fint at ræsonnere, som jeg gør ovenfor.

% For at opresumere er planen for næste gang dermed:
% - Færdiggør preservation for System F (vha typesubstitutionslemmaet ovenfor)
% - Vis progress + preservation for System F + rekursive typer (dette burde være rimeligt ligetil vha. ovenstående)
% - Kig på referencer og kom så langt I kan med progress + preservation for denne udvidelse.
\end{proof}


\section{Progress}

\begin{theorem}[Progress]
    If $\hastype{}{}{\Sigma}{e}{\tau}$, then either $e$ is a value or else, for any store $\sigma$ with $\stotype{}{}{\Sigma}{\sigma}$, there exists an expression $e'$ and a store $\sigma'$ such that $(\sigma,e) \step (\sigma',e')$.
\end{theorem}

\begin{proof}
The proof is by induction on the typing relation $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, but the claim to be proved is augmented by \textquote{or either $\Xi$ or $\Gamma$ is non-empty}.\footnote{This ensures that we can perform the induction on the entire $4$-ary relation, which is important since this relation is the one that is defined by the inference rules, \emph{not} the corresponding binary relation obtained by restricting the ternary relation to the subset where $\Xi$ and $\Gamma$ are empty.} Notice that the induction step for each inference rule is trivial if $\Xi$ or $\Gamma$ is non-empty, so we need only prove each case when $\Xi$ and $\Gamma$ are empty. Furthermore, since the store is relevant for only a few reductions, we suppress it from the notation in most of the cases below, simply taking about expressions reducing to other expressions.
%
\begin{proofsec}
    \item[\infrule{T-unit}]
    Since $\objUnit$ is a value, this follows.

    \item[\infrule{T-var}]
    Since we may assume that $\Gamma$ is empty, this implication is vacuously true.\footnote{In the formalism of TODO ref \infrule{T-var} would not be an inference rule, it would instead be an axiom requiring the relation to include all quadruples $(\Xi, \Gamma, e, \tau)$ such that $(e : \tau) \in \Gamma$ and all type variables in $\tau$ occur in $\Xi$. This is of course also vacuously true when $\Gamma$ is empty.}

    \item[\infrule{T-pair}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1}$ and $\hastype{}{}{\Sigma}{e_2}{\tau_2}$. If both $e_1$ and $e_2$ are values, then $\objPair{e_1}{e_2}$ is also a value, so assume that only $e_1 = v_1$ is a value and that $e_2 \step e_2'$. Since the only rule that generates instances of the one-step relation $\to$ is \infrule{head-step-step}, it follows that $e_2$ is on the form $K[d_2]$ and $e_2'$ is on the form $K[d_2']$, where $K$ is an evaluation context and $d_2$ and $d_2'$ are subexpressions of $e_2$ and $e_2'$ respectively such that $d_2 \headstep d_2'$. Letting $K' = \objPair{v_1}{K}$ it follows that $\objPair{v_1}{e_2} = K'[d_2]$ and $\objPair{v_1}{e_2'} = K'[d_2']$, and so
    %
    \begin{equation*}
        \objPair{v_1}{e_2}
            = K'[d_2]
            \step K'[d_2']
            = \objPair{v_1}{e_2'}
    \end{equation*}
    %
    by \infrule{head-step-step}. If instead $e_1$ is not a value, then the same argument (using the evaluation context $\objPair{K}{e_2}$) yields the same result.

    \item[\infrule{T-fst}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau_1 \prod \tau_2}$. If $e$ is a value, then it is on the form $\objPair{v_1}{v_2}$ by \cref{enum:canonical-product}, where $v_1$ and $v_2$ are values. Hence $\objFst{e} = \objFst{\objPair{v_1}{v_2}}$, and this reduces via a head-step to $v_1$. Choosing the evaluation context $K = \hole$, \infrule{head-step-step} implies that $\objFst{\objPair{v_1}{v_2}} \step v_1$. If instead $e$ is not a value, then by induction there is some $e'$ such that $e \to e'$. Hence there are subexpressions $d$ and $d'$ and an evaluation context $K$ such that $e = K[d]$, $e' = K[d']$ and $d \headstep d'$. Letting $K' = \objFst{K}$ we have
    %
    \begin{equation*}
        \objFst{e}
            = K'[d]
            \step K'[d']
            = \objFst{e'},
    \end{equation*}
    %
    as desired.

    \item[\infrule{T-snd}]
    Similar to \infrule{T-fst}.

    \item[\infrule{T-inj1}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau_1}$. If $e$ is a value $v$, then so is $\objInl{v}$. If instead $e \step e'$, then as before $e = K[d]$, $e' = K[d']$ and $d \headstep d'$. Letting $K' = \objInl{K}$ we get $\objInl{e} = K'[d]$ and $\objInl{e'} = K'[d']$, so $\objInl{e} \step \objInl{e'}$.

    \item[\infrule{T-inj2}]
    Similar to \infrule{T-inj1}.

    \item[\infrule{T-match}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1 + \tau_2}$. If $e_1$ is a value, then by \cref{enum:canonical-sum} it must be on the form $\objInl{v}$ or $\objInr{v}$ for a value $v$. Hence the expression $\objMatch{e_1}{x}{e_2}{e_3}$ can reduce via a head step by either \infrule{E-match-inj1} or \infrule{E-match-inj2}, so it reduces by \infrule{head-step-step} (using the evaluation context $K = \hole$). If instead $e_1 \step e_1'$, then by the same argument as in previous cases with $K' = \objMatch{K}{x}{e_2}{e_3}$, it follows that $\objMatch{e_1}{x}{e_2}{e_3}$ reduces.

    \item[\infrule{T-rec}]
    This is obvious since $\objRec{f}{x}{e}$ is a value.

    \item[\infrule{T-app}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\tau_1 \to \tau_2}$ and $\hastype{}{}{\Sigma}{e_2}{\tau_1}$. If $e_1$ is a value, then by \cref{enum:canonical-function} it must be on the form $\objRec{f}{x}{e}$. If also $e_2$ is a value, then the claim follows by \infrule{E-rec-app}. If $e_1 = v$ is a value but $e_2$ is not, then $e_2 \to e_2'$. The same argument as in previous cases with $K' = \objApp{v}{K}$ shows that $\objApp{v}{e_2}$ reduces. Finally, if $e_1$ is not a value, then $e_1 \to e_1'$, and choosing $K' = \objApp{K}{e_2}$ proves the claim.

    \item[\infrule{T-Tlam}]
    This is obvious since $\objForall{X}{e}$ is a value.

    \item[\infrule{T-Tapp}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\typeForall{X}{\tau}}$. If $e$ is a value, then by \cref{enum:canonical-forall} it must be on the form $\objForall{X}{e'}$, so the claim follows from \infrule{E-tapp-tlam} (via \infrule{head-step-step} using the evaluation context $K = \hole$). If $e$ is not a value, then $e \to e'$ for some expression $e'$ by induction. These expressions then have subexpressions $d$ and $d'$ respectively such that $d \headstep d'$, and such that $e = K[d]$ and $e' = K[d']$ for some evaluation context $K$. Letting $K' = \objTapp{K}{\tau'}$ we thus have $\objTapp{e}{\tau'} = K'[d]$ and $\objTapp{e'}{\tau'} = K'[d']$, proving the claim.

    \item[\infrule{T-pack}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau[\tau'/X]}$. If $e$ is a value, then so is $\objPack{e}$. Otherwise $e \step e'$ for some expression $e'$. The same argument as before using the evaluation context $\objPack{K}$ for an appropriate $K$ yields the claim.

    \item[\infrule{T-unpack}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\typeExists{X}{\tau}}$. If $e_1$ is a value, then it must be on the form $\objPack{v}$ by \cref{enum:canonical-exists}, so an application of \infrule{E-unpack-pack} yields the claim. Otherwise $e \step e'$ for some $e'$, and we use the evaluation context $\objUnpack{K}{x}{e_2}$ for an appropriate $K$.

    \item[\infrule{T-fold}]
    TODO

    \item[\infrule{T-unfold}]
    TODO

    \item[\infrule{T-loc}]
    Locations are values, so this is obvious.

    \item[\infrule{T-alloc}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\tau}$. If $e$ is a value, then the claim follows by applying \infrule{E-alloc}, noting that there always exists a location $l \not\in \dom \sigma$. Otherwise there is an expression $e'$ and a store $\sigma'$ such that $(\sigma,e) \step (\sigma',e')$. But then there is some evaluation context $K$ and subexpressions $d$ and $d'$ of $e$ and $e'$ respectively, such that $e = K[d]$, $e' = K[d']$ and $(\sigma,d) \headstep (\sigma',d')$. Letting $K' = \objRef{K}$ we have $\objRef{e} = K'[d]$ and $\objRef{e'} = K'[d']$, so \infrule{head-step-step} implies that $(\sigma,\objRef{e}) \step (\sigma',\objRef{e'})$.

    \item[\infrule{T-store}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e_1}{\typeRef{\tau}}$ and $\hastype{}{}{\Sigma}{e_2}{\tau}$, and let $\sigma$ be a store with $\stotype{}{}{\Sigma}{\sigma}$. If $e_1$ is a value, then by \cref{enum:canonical-ref} it is a location $l$, and by \cref{enum:inversion-location} we have $l \in \dom \Sigma = \dom \sigma$. If $e_2$ is also a value, then the claim follows from \infrule{E-store}. If $e_1 = l$ is a value but $e_2$ is not, then there is some $e_2'$ and $\sigma'$ such that $(\sigma,e_2) \step (\sigma',e_2')$. Again writing $e_2 = K[d_2]$ and $e_2' = K[d_2']$ with $(\sigma,d) \headstep (\sigma',d')$, we use the evaluation context $\objAss{l}{K}$. Finally, if $e_1$ is not a value, then the same argument using instead $\objAss{K}{e_2}$ yields the claim.

    \item[\infrule{T-load}]
    Assume that the claim holds for $\hastype{}{}{\Sigma}{e}{\typeRef{\tau}}$, and let $\sigma$ be a store with $\stotype{}{}{\Sigma}{\sigma}$. If $e$ is a value, then as before it is a location $l$, and $l \in \dom \sigma$. It then follows from \infrule{E-load} that $(\sigma, \objLoad{l}) \headstep (\sigma,v)$, where $v = \sigma(l)$. If instead $(\sigma,e) \step (\sigma',e')$ for an expression $e'$ and a store $\sigma'$, then we simply use the evaluation context $\objLoad{K}$ for an appropriate $K$.
\end{proofsec}
\end{proof}


\section{Preservation}

\begin{theorem}[Preservation]
    If
    %
    \begin{equation*}
        \hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau},
        \quad
        \stotype{\Xi}{\Gamma}{\Sigma}{\sigma}
        \quad \text{and} \quad
        (\sigma,e) \step (\sigma',e'),
    \end{equation*}
    %
    then there exists some store typing $\Sigma'$ with $\Sigma \subseteq \Sigma'$ such that
    %
    \begin{equation*}
        \hastype{\Xi}{\Gamma}{\Sigma'}{e'}{\tau}
        \quad \text{and} \quad
        \stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}.
    \end{equation*}
\end{theorem}

\begin{proof}
    By definition of the one-step relation, there exist an evaluation context $K$ and subexpressions $d$ of $e$ and $d'$ of $e'$ such that $e = K[d]$, $e' = K[d']$, and $(\sigma,d) \headstep (\sigma',d')$. By \cref{lem:subexp-well-typed} there is some type $\rho$ such that $\hastype{\Xi}{\Gamma}{\Sigma}{d}{\rho}$. Next it follows from \cref{lem:preservation-head-steps} that $\hastype{\Xi}{\Gamma}{\Sigma'}{d'}{\rho}$ for some store typing $\Sigma'$ with $\Sigma \subseteq \Sigma'$ and $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$. By \cref{lem:store-typing-weakening} we also have $\hastype{\Xi}{\Gamma}{\Sigma'}{d}{\rho}$, so it follows from \cref{lem:evaluation-contexts-respect-types} that $\hastype{\Xi}{\Gamma}{\Sigma'}{K[d']}{\tau}$ as desired.
\end{proof}


\begin{lemma}
    \label{lem:subexp-well-typed}
    If $K$ is an evaluation context, $e$ is an expression and $\hastype{\Xi}{\Gamma}{\Sigma}{K[e]}{\tau}$, then $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.
\end{lemma}

\begin{proof}
Every evaluation context is obtained from the hole \enquote{$\hole$} by finitely many applications of the productions in the grammar [TODO prove this?]. We prove the claim by induction on the length of such a sequence of productions. If $K = \hole$, then the claim is obvious, since then $K[e] = e$. Hence we assume that $K$ is obtained from some evaluation context $K'$ by some application of a production, so that the induction hypothesis holds for $K'$.
%
\begin{proofsec}
    \item[$K = \objPair{K'}{e'}$]
    Then $K[e] = \objPair{K'[e]}{e'}$, and since this is well-typed with type $\tau$, \cref{enum:inversion-pair} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1}$ for some type $\tau_1$. By induction applied to $K'$ we have $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.

    \item[$K = \objPair{v}{K'}$]
    Similar to the above.

    \item[$K = \objFst{K'}$]
    Then $K[e] = \objFst{K'[e]$}, so \cref{enum:inversion-fst} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau \prod \tau_2}$ for some $\tau_2$. By induction we have $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\rho}$ for some type $\rho$.

    \item[$K \in \{\objSnd{K'}, \objInl{K'}, \objInr{K'}\}$]
    Similar to the above.

    \item[$K = \objMatch{K'}{x}{e_1}{e_2}$]
    Here \cref{enum:inversion-match} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1 + \tau_2}$, so the claim follows by induction.

    \item[$K = \objApp{K'}{e'}$]
    Then $K[e] = \objApp{K'[e]}{e'}$, so \cref{enum:inversion-app} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1 \to \tau}$ for some type $\tau_1$. The claim follows by induction as before.

    \item[$K = \objApp{v}{K'}$]
    Similar to the above.

    \item[$K = \objTapp{K'}{X}$]
    \Cref{enum:inversion-tapp} implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau_1}$ for some type $\tau_1$, so the claim follows by induction.

    \item[$K = $]
    TODO

    \item[$K = \objRef{K'}$]
    Then $K[e] = \objRef{K'[e]}$, so the inversion lemma implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau'}$. The claim follows by induction.

    \item[$K = \objAss{K'}{e'}$]
    Then $K[e] = \objAss{K'[e]}{e'}$, so the inversion lemma implies that $\hastype{\Xi}{\Gamma}{\Sigma}{K'[e]}{\tau'}$ ... TODO
\end{proofsec}

TODO rest -- but they are all the same, so maybe just do one?
\end{proof}


\begin{lemma}[Weakening]
    \label{lem:store-typing-weakening}
    If $\Sigma$ and $\Sigma'$ are store typings with $\Sigma \subseteq \Sigma'$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$, then $\hastype{\Xi}{\Gamma}{\Sigma'}{e}{\tau}$.
\end{lemma}

\begin{proof}
    This is a straightforward induction on type derivations, in that we notice that in all inference rules, the store typing is the same in the conclusion as it is in the hypotheses. Furthermore, if the axiom \infrule{T-loc} holds for $\Sigma$, then it clearly holds for $\Sigma'$.
\end{proof}


\begin{lemma}
    \label{lem:evaluation-contexts-respect-types}
    If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{e'}{\tau}$ for the same type $\tau$, then $\hastype{\Xi}{\Gamma}{\Sigma}{K[e]}{\rho}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{K[e']}{\rho}$ for the same type $\rho$.
\end{lemma}

\begin{proof}
The proof is by induction on $K$. If $K = \hole$, then this is obvious.
%
\begin{proofsec}
    \item[$K = \objPair{K'}{e''}$]
    Then $K'[e]$ and $K'[e']$ have the same type, so by \infrule{T-pair}, so do $K[e]$ and $K[e']$.

    \item[$K = \objTapp{K'}{\tau'}$]
    Then $K'[e]$ and $K'[e']$ have the same type by induction, and so do $K[e] = \objTapp{K'[e]}{\tau'}$ and $K[e'] = \objTapp{K'[e']}{\tau'}$ by \infrule{T-Tapp}.\footnote{TODO since we just apply it to underscore, it actually has a lot of different types. But we can find one type that works for both.} [TODO need lemma saying that $\tau[\tau'/X]$ is a type!]
\end{proofsec}

TODO rest
\end{proof}


\begin{lemma}[Preservation for head-steps]
    \label{lem:preservation-head-steps}
    If $\hastype{\Xi}{\Gamma}{\Sigma}{e}{\tau}$ and $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$ and $(\sigma,e) \headstep (\sigma',e')$, then there exists a store typing $\Sigma'$ such that $\Sigma \subseteq \Sigma'$, $\hastype{\Xi}{\Gamma}{\Sigma'}{e'}{\tau}$, and $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$.
\end{lemma}

\begin{proof}
We simply check all cases. [TODO mention pure cases]
%
\begin{proofsec}
    \item[\infrule{E-fst}]
    In this case $e = \objFst{\objPair{v_1}{v_2}}$ and $e' = v_1$ for values $v_1,v_2$. Then $e$ is a value, so [TODO canonical forms] implies first that $\hastype{\Xi}{\Gamma}{\Sigma}{\objPair{v_1}{v_2}}{\tau \prod \tau'}$ for some type $\tau'$, and then that $\hastype{\Xi}{\Gamma}{\Sigma}{v_1}{\tau}$.

    \item[\infrule{E-tapp-tlam}]
    Write the type of $\objTapp{\objForall{X}{e}}{\tau'}$ as $\tau[\tau'/X]$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{\objForall{X}{e}}{\typeForall{X}{\tau}}$, and again by inversion this implies that $\hastype{\Xi,X}{\Gamma}{\Sigma}{e}{\tau}$. But then it follows from [TODO lemma 0] that $\hastype{\Xi}{\Gamma[\tau'/X]}{\Sigma}{e}{\tau[\tau'/X]}$, and since $\Gamma$ does not contain $X$ (since $\Xi$ does not) we have $\Gamma[\tau'/X] = \Gamma$, so the claim follows.

    \item[\infrule{E-alloc}]
    In this case $e = \objRef{v}$, $e' = l$, $\sigma' = \sigma[l \mapsto v]$, and $l \not\in \dom \sigma$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{\objRef{v}}{\typeRef{\tau'}}$ for some $\tau'$, and we further have $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau'}$. Now letting $\Sigma' = \Sigma[l \mapsto \tau']$, it follows from \infrule{T-loc} that $\hastype{\Xi}{\Gamma}{\Sigma'}{l}{\typeRef{\Sigma'(l)}}$, so we have both $\stotype{\Xi}{\Gamma}{\Sigma'}{\sigma'}$ and $\hastype{\Xi}{\Gamma}{\Sigma'}{l}{\typeRef{\tau'}}$.

    \item[\infrule{E-store}]
    Here $e = \objAss{l}{v}$, $e' = \objUnit$ and $\sigma' = \sigma[l \mapsto v]$ with $l \in \dom \sigma$. Notice first that $\objAss{l}{v}$ and $\objUnit$ both have type $\typeUnit$ by inversion, so that $\hastype{\Xi}{\Gamma}{\Sigma}{\objUnit}{\typeUnit}$ as required.

    By inversion we also have $\hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\tau'}}$ and $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau'}$ for some type $\tau'$, and another application of inversion (TODO via \infrule{T-loc}, or canonical forms?) implies that $\tau' = \Sigma(l)$. Furthermore, since $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$, we have $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$. Since $v = \sigma'(l)$ it thus follows that $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma'(l)}{\Sigma(l)}$, so $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma'}$.

    \item[\infrule{E-load}]
    Here $e = \objLoad{l}$, $e' = v$ and $\sigma = \sigma'$ with $\sigma(l) = v$. By inversion we have $\hastype{\Xi}{\Gamma}{\Sigma}{l}{\typeRef{\tau}}$, so $\tau = \Sigma(l)$ [TODO again]. But since $\stotype{\Xi}{\Gamma}{\Sigma}{\sigma}$ we have $\hastype{\Xi}{\Gamma}{\Sigma}{\sigma(l)}{\Sigma(l)}$, or in other words, $\hastype{\Xi}{\Gamma}{\Sigma}{v}{\tau}$.
\end{proofsec}

TODO rest
\end{proof}


\chapter{Fixed-points and Induction}

Overview: generating functions, in terms of inference rules

complete lattice -> Knaster-Tarski -> induction -> rule induction

dcp(p)o -> Kleene and continuity -> finite derivations -> inversion

Note for readers not interested in details.


\section{Abstract theory}

\newpar

Let $(P,\leq)$ be a partially ordered set\footnote{That is, $\leq$ is a reflexive, transitive and anti-symmetric binary relation on $P$.}. If $A \subseteq P$, then an element $x \in P$ is called an \keyword{upper bound} of $A$ if $a \leq x$ for all $a \in A$. If there is a least upper bound $x$ (i.e., such that if $y$ is any other upper bound, then $x \leq y$), then $x$ is called the \keyword{join} of $A$. The join is clearly unique if it exists (by anti-symmetry), and we write $\bigjoin A$ for the join of $A$. We similarly define the \keyword{meet} of $A$, denoted $\bigmeet A$, to be the greatest lower bound of $A$, if it exists. If every two-element subset of $P$ has a join and a meet, then $P$ is called a \keyword{lattice}, and we write $x \join y = \bigjoin \{x,y\}$ and $x \meet y = \bigmeet \{x,y\}$. If every subset of $P$ has a join and a meet, then $P$ is a \keyword{complete lattice}.

If $P$ is a partially ordered set, then a subset $D \subseteq P$ is said to be \keyword{directed} if, for every pair of elements $x,y \in D$, there exists a $z \in D$ with $x \leq z$ and $y \leq z$. That is, $D$ is directed if every two-element subset of $D$ has an upper bound in $D$ (but not necessarily a \emph{least} upper bound, i.e. a join). By induction any \emph{finite} subset of $D$ also has an upper bound in $D$.\footnote{As an example of why directedness in interesting, recall that a union of a collection of subspaces of a vector space is not usually a subspace itself, but it is if the collection is directed (with respect to inclusion). Similarly for subgroups and other algebraic structures.} We further note that the image of a directed set under a monotone map is also directed. If $D$ is a directed set for which the join $\bigjoin D$ exists, we often write $\bigdjoin D$ instead. If $\bigdjoin D$ exists for every directed subset $D$ of $P$, then $P$ is called a \keyword{directly complete partial order}, or \keyword{dcpo} for short. If $P$ also has a least element, usually written $\bot$, then $P$ is called a \keyword{dcppo} (the extra \enquote{p} is for \enquote{pointed}). Notice that complete lattices are dcppo's.

Let $F \colon P \to P$ be a monotone\footnote{A map $f \colon P \to Q$ between posets is \keyword{monotone} if $x \leq y$ implies $f(x) \leq f(y)$ for all $x,y \in P$} map. We think of $F$ as a \keyword{generating function}. An element $x \in P$ is said to be \keyword{$F$-closed} if $F(x) \leq x$, \keyword{$F$-consistent} if $x \leq F(x)$, and a \keyword{fixed-point} of $F$ if $F(x) = x$. If $F$ has a least fixed-point, then this is usually denoted $\mu F$. Similarly, the greatest fixed-point, if it exists, is denoted $\nu F$.


\newpar

We begin by studying dcpo's. If $P$ and $Q$ are dcpo's, then a map $f \colon P \to Q$ is \keyword{continuous}\footnote{Also called \keyword{Scott-continuous} after Dana Scott.} if, for every directed $D \subseteq P$, the image $f\image{D}$ is directed and
%
\begin{equation*}
    f \bigl( \bigdjoin D \bigr)
        = \bigdjoin f\image{D}.
\end{equation*}
%
It is easy to show that continuous maps are monotone (notice that if $x \leq y$, then the set $\{x,y\}$ is directed). If conversely $f$ is monotone, then $f\image{D}$ is as mentioned also directed, and the inequality \enquote{$\geq$} always holds.

Now let $P$ be a dcppo and $F \colon P \to P$ a monotone function. We clearly have $\bot \leq F(\bot)$, and since $F$ is monotone we get the chain\footnote{A \keyword{chain} is a totally ordered set.}
%
\begin{equation*}
    \bot
        \leq F(\bot)
        \leq \cdots
        \leq F^n(\bot)
        \leq F^{n+1}(\bot)
        \leq \cdots
\end{equation*}
%
called the \keyword{ascending Kleene chain}. Since it is a chain it is also directed. The main theorem is the following:

\begin{theorem}[Kleene's fixed-point theorem]
    \label{thm:kleene-fixpoint-theorem}
    If $P$ is a dcppo and $F \colon P \to P$ is continuous, then $F$ has a least fixed-point $\mu F$ and
    %
    \begin{equation*}
        \mu F
            = \bigdjoin_{n \in \naturals} F^n(\bot).
    \end{equation*}
\end{theorem}

\begin{proof}
    First notice that, since $F$ is continuous,
    %
    \begin{equation*}
        F \Bigl( \bigdjoin_{n \in \naturals} F^n(\bot) \Bigr)
            = \bigdjoin_{n \in \naturals} F^{n+1}(\bot)
            = \bigdjoin_{n \in \naturals^+} F^n(\bot)
            = \bigdjoin_{n \in \naturals} F^n(\bot),
    \end{equation*}
    %
    where we use that $F^0(\bot) = \bot$. Hence $\bigdjoin_{n \in \naturals} F^n(\bot)$ is indeed a fixed-point of $F$. If $\beta$ is any fixed-point of $F$, then $\bot \leq \beta$, and hence $F^n(\bot) \leq F^n(\beta) = \beta$ since $F$ is monotone. Taking the join on the left-hand side yields $\bigdjoin_{n \in \naturals} F^n(\bot) \leq \beta$ as desired.
\end{proof}


\newpar

Next, let $L$ be a complete lattice, and let $F \colon L \to L$ be a monotone map. Even though $L$ is also a dcppo, if $F$ is not continuous then \cref{thm:kleene-fixpoint-theorem} does not apply. However, $F$ still has fixpoints, as the following theorem shows:

\begin{theorem}[Knaster--Tarski's fixed-point theorem]
    \label{thm:knaster-tarski}
    If $L$ is a complete lattice and $F \colon L \to L$ is monotone, then $F$ has a least and a greatest fixed-point, and these are given by
    %
    \begin{equation*}
        \mu F
            = \bigmeet \set{x \in L}{F(x) \leq x}
        \quad \text{and} \quad
        \nu F
            = \bigjoin \set{x \in L}{x \leq F(x)}.
    \end{equation*}
    %
    In particular, $\mu F$ is the smallest $F$-closed element and $\nu F$ is the greatest $F$-consistent element in $L$.
\end{theorem}

\begin{proof}
    Denote the meet above by $\alpha$. If $x$ is $F$-closed, then $\alpha \leq x$, so $F(\alpha) \leq F(x) \leq x$. Taking the meet of $x$ we get $F(\alpha) \leq \alpha$, so $\alpha$ is closed. It follows that $F(F(\alpha)) \leq F(\alpha)$, so $F(\alpha)$ is also closed, and so $\alpha \leq F(\alpha)$. Hence $\alpha$ is a fixed-point. Since every other fixed-point is in particular closed, $\alpha$ is the least fixed-point.
\end{proof}

\begin{corollarynoproof}[Principle of induction]
    \label{cor:induction-abstract}
    If $y \in L$ is $F$-closed, then $\mu F \leq y$.\footnote{We dually have a \keyword{principle of coinduction}: If $y \in L$ is $F$-consistent, then $y \leq \nu F$.}
\end{corollarynoproof}






% If $x \leq F(x)$ for all $x \in L$, then we say that $F$ is \keyword{extensive}.\footnote{That is, $F$ is extensive if all elements of $x$ are $F$-consistent.} Clearly not all monotone functions are extensive, but it is useful to associate to $F$ an extensive function with the same least fixed-point.

% \begin{proposition}
%     If $F,G \colon L \to L$ are monotone and\footnote{We define the join on the set of functions $L \to L$ pointwise. Notice that the join of two monotone functions is again monotone.} $H = F \join G$, then $\mu H$ is the smallest element that is both $F$-closed and $G$-closed.
% \end{proposition}

% \begin{proof}
%     First notice that
%     %
%     \begin{equation*}
%         \mu H
%             = H(\mu H)
%             = F(\mu H) \join G(\mu H),
%     \end{equation*}
%     %
%     so $\mu H$ is both $F$- and $G$-closed. Conversely, if $y \in L$ is both $F$- and $G$-closed, then
%     %
%     \begin{equation*}
%         H(y)
%             = F(y) \join G(y)
%             \leq y,
%     \end{equation*}
%     %
%     so $y$ is also $H$-closed, and hence $\mu H \leq y$.
% \end{proof}
% %
% Now notice that every element of $L$ is closed with respect to the identity function $\id_L \colon L \to L$, yielding the following:

% \begin{corollarynoproof}
%     If $\overline{F} = F \join \id_L$, then $\overline{F}$ is extensive and $\mu \overline{F} = \mu F$.
% \end{corollarynoproof}


\section{In power sets}

\newpar

Specialising to the case where $L$ is a power set $\powerset{X}$, one way to define a generating function is using inference rules. An \keyword{inference rule} on $X$ is an expression on the form
%
\begin{equation*}
    \inferrule*[left=Rule]{
        x_1 \and x_2 \and \cdots \and x_k
    }{
        y
    }
\end{equation*}
%
where $x_1, \ldots, x_k$ and $y$ are elements of $X$, and $k \in \naturals$.\footnote{We could equivalently view an inference rule as an element of the product $X^k \prod X$.} We allow $k$ to be zero, in which case we call the rule an \keyword{axiom}. We have decorated the expression with the label \enquote{\infrule{Rule}} so that we may refer to it later. Let us call $x_1, \ldots, x_k$ the \keyword{antecedents} of the rule and $y$ the \keyword{consequent}. Given a (possibly infinite) collection of inference rules, we construct a generating function $F \colon \powerset{X} \to \powerset{X}$ by defining $F(A)$ for a subset $A \subseteq X$ as follows: For $y \in X$ we let $y \in F(A)$ if and only if there is an inference rule whose consequent is $y$ and whose antecedents lie in $A$. We say that $F$ is \keyword{represented by} this collection of inference rules.

Clearly $F$ is monotone if it is represented by a collection of inference rules, so it has a least fixed-point $\mu F$ and we get a principle of induction. However, it is useful to restate induction in terms of the inference rules, since we usually have explicit rules in mind when defining $F$. If $\calR$ is a collection of inference rules on $X$, then we say that a subset $A \subseteq X$ is \keyword{$\calR$-closed} if, given any rule $R \in \calR$ with antecedents lying in $A$, the consequent of $R$ also lies in $A$.

\begin{lemma}
    \label{lem:R-closed-F-closed}
    If $F$ is represented by a collection $\calR$ of inference rules, then a subset $A \subseteq X$ is $F$-closed if and only if it is $\calR$-closed.
\end{lemma}

\begin{proof}
    First assume that $A$ is $F$-closed so that $F(A) \subseteq A$, and consider a rule from $\calR$ with antecedents $x_1, \ldots, x_k \in A$ and consequent $y$. Since $F$ is represented by $\calR$, this implies that $y \in F(A) \subseteq A$, so $A$ is $\calR$-closed.

    If $A$ is $\calR$-closed, then let $y \in F(A)$. Hence there is a rule in $\calR$ with consequent $y$ and antecedents $x_1, \ldots, x_k \in A$. But since $A$ is $\calR$-closed, this implies that $y \in A$, and so $F(A) \subseteq A$.
\end{proof}


\begin{theorem}[Principle of rule induction]
    If $F$ is represented by $\calR$ and $A \subseteq X$, then to show that $\mu F \subseteq A$, it suffices to show the following condition: For every inference rule $R \in \calR$ with antecedents lying in $A$, the consequent of $R$ also lies in $A$.
\end{theorem}

\begin{proof}
    The condition says precisely that $A$ is $\calR$-closed, which implies that $A$ is $F$-closed by \cref{lem:R-closed-F-closed}. But then \cref{cor:induction-abstract} implies that $\mu F \subseteq A$.
\end{proof}



% If $G$ is another generating function represented by a collection of inference rules, then we have the following:

% \begin{lemma}
%     If $F$ is represented by inference rules $\calR$ and $G$ is represented by $\calR'$, then $F \union G$ is represented by $\calR \union \calR'$.
% \end{lemma}

% \begin{proof}
%     Let $H = F \union G$, and let $A \subseteq X$. If $y \in H(A)$, then assume without loss of generality that $y \in F(A)$. Then $y$ is the consequent of some inference rule in $\calR$ whose antecedents lie in $A$. But then $y$ is obviously also the consequent of an inference rule in $\calR \union \calR'$.

%     Conversely, if $y$ is a consequent of an inference rule in $\calR \union \calR'$ with antecedents in $A$, then assume without loss of generality that this rule lies in $\calR$. But then $y \in F(A) \subseteq H(A)$.
% \end{proof}
% %
% Now notice that the identity function $\id_{\powerset{X}}$ is represented by the collection of inference rules
% %
% \begin{equation*}
%     \inferrule*[left=$\infrule{ID}_x$]{
%         x
%     }{
%         x
%     }
% \end{equation*}
% %
% for all $x \in X$.\footnote{Notice that this is indeed a \emph{collection} of inference rules, one for each $x \in X$. The rule as written is thus more properly a rule \emph{schema}.} Given a representation $\calR$ of $F$, we thus obtain a representation $\overline{\calR}$ of $\overline{F}$ by augmenting $\calR$ with all identity rules:
% %
% \begin{propositionnoproof}
%     If $F$ is represented by $\calR$, then $\overline{F}$ is represented by
%     %
%     \begin{equation*}
%         \overline{\calR}
%             = \calR \union \set{\infrule{ID}_x}{x \in X}.
%     \end{equation*}
% \end{propositionnoproof}

% Knowing that $F$ is monotone is enough to yield a principle of induction, but sometimes it is necessary (or at least useful) to have an alternative characterisation of $\mu F$, one that makes use of the fact that the number of antecedents of an inference rule is finite. Assume that $F$ is extensive, and define a sequence $(A_n)_{n \in \naturals}$ of subsets of $X$ as follows: Let $A_0 = \emptyset$, and let $A_{n+1} = F(A_n)$ for $n \in \naturals$. Since $F$ is extensive, this is an increasing sequence. Finally let $A_\infty = \bigunion_{n \in \naturals} A_n$. We then have the following:

% \begin{lemma}
%     If $F$ is extensive, then $A_\infty = \mu F$.
% \end{lemma}

% \begin{proof}
%     For the inclusion \enquote{$\subseteq$}, notice that $A_0 = \emptyset \subseteq \mu F$, and that $A_{n+1} = F(A_n) \subseteq F(\mu F) = \mu F$ by induction in $n$ and induction hypothesis $A_n \subseteq \mu F$.

%     To prove the inclusion \enquote{$\supseteq$} it suffices to show that $A_\infty$ is $F$-closed, so consider an element $y \in F(A_\infty)$. Then there is an inference rule with $y$ as consequent and whose antecedents $x_1, \ldots, x_k$ lie in $A_\infty$. But then there exist indices $n_1, \ldots, n_k$ such that $x_i \in A_{n_i}$. Letting\footnote{Here we use that inference rules have finitely many antecedents, and that the sequence $(A_n)$ is increasing.} $N = \max\{n_1, \ldots, n_k\}$ it follows that $x_i \in A_N$ for all $i$. But then $y \in F(A_N) = A_{N+1}$ by definition of $F$, so $y \in A_\infty$. Hence $A_\infty$ is $F$-closed.
% \end{proof}


\newpar

\newcommand{\assum}[1]{\operatorname{asm}#1}
\newcommand{\concl}[1]{\operatorname{con}#1}
\newcommand{\derlen}[1]{l(#1)}


[TODO rewrite] But sometimes it is necessary (or at least useful) to have an alternative characterisation of $\mu F$, one that makes use of the fact that the number of antecedents of an inference rule is finite. This has the following consequence:

\begin{lemma}
    If $F$ is represented by a collection of inference rules, then $F$ is continuous.
\end{lemma}

\begin{proof}
    It suffices to show that if $\calD \subseteq \powerset{X}$ is directed, then $F(\bigdjoin \calD) \subseteq \bigdjoin F\image{\calD}$, so let $y \in F(\bigdjoin \calD)$. Say that $F$ is represented by a collection $\calR$. Then there is a rule in $\calR$ with antecedents $x_1, \ldots, x_k \in \bigdjoin \calD$ and consequent $y$. Since the (directed) join in $\powerset{X}$ is just union, there are sets $A_1, \ldots, A_k \in \calD$ such that $x_i \in A_i$. And since $\calD$ is directed there is a set $A \in \calD$ with $A_i \subseteq A$, so that $x_i \in A$ for all $i$. But then $y \in F(A) \subseteq \bigdjoin F\image{\calD}$ as desired.
\end{proof}
%
Hence, \cref{thm:kleene-fixpoint-theorem} implies that $\mu F = \bigdjoin_{n \in \naturals} F^n(\emptyset)$.



We now make this more concrete. If $\calR$ is a collection of inference rules, then a \keyword{derivation} from $\calR$ is a finite sequence $D = (R_1, \ldots, R_n)$ of inference rules from $\calR$. If $y$ is a consequent of some $R_i$, then we say that $y$ is a \keyword{conclusion} of $D$. The consequent of $R_n$ is called the \keyword{final conclusion} of $D$. We say that $x \in X$ is an \keyword{assumption} of $D$ if $x$ is an antecedent of some $R_i$, and if none of the rules $R_1, \ldots, R_{i-1}$ has $x$ as its antecedent. That is, $x$ is an assumption if it is not implied by any of the previous rules in the derivation. The set of conclusion of $D$ is denoted $\concl{D}$, and the set of assumptions of $D$ is denoted $\assum{D}$. If $\assum{D} = \emptyset$, then we say that $D$ is \keyword{closed}.

\begin{remark}
    Let $D = (R_1, \ldots, R_n)$ be a derivation.
    %
    \begin{enumrem}
        \item If $D$ is closed, then $R_1$ must be an axiom.
    
        \item\label{enum:subderivation} For any $i \in \{1, \ldots, n\}$, $D' = (R_1, \ldots, R_i)$ is called a \keyword{subderivation} of $D$. If $i < n$, then $D'$ is called a \keyword{strict subderivation} of $D$ (so $D'$ is a strict subderivation when $D' \neq D$). It is clear that $\assum{D'} \subseteq \assum{D}$ and $\concl{D'} \subseteq \assum{D}$, so $D'$ is closed if $D$ is closed.
    
        \item\label{enum:derivation-antecedent} If $D$ is closed and $x$ is an antecedent of some $R_i$, then $D$ has a strict subderivation $D'$ whose final consequence is $x$: For $x$ must be the consequent of some $R_1, \ldots, R_{i-1}$, say $R_j$, and $(R_1, \ldots, R_j)$ is closed subderivation of $D$ by \subcref{enum:subderivation}.

        \item\label{enum:derivation-composition} If $E = (S_1, \ldots, S_m)$ is another derivation, then
        %
        \begin{equation*}
            D \circ E
                \defeq (R_1, \ldots, R_n, S_1, \ldots, S_m)
        \end{equation*}
        %
        is another derivation. It is clear that
        %
        \begin{equation*}
            \assum{(D \circ E)}
                = \assum{D} \union (\assum{E} \setminus \concl{D}),
        \end{equation*}
        %
        that
        %
        \begin{equation*}
            \concl{(D \circ E)}
                = \concl{D} \union \concl{E},
        \end{equation*}
        %
        and that the final conclusion of $D \circ E$ is the final conclusion of $E$. In particular, if $D$ is closed and $\assum{E} \subseteq \concl{D}$, then $D \circ E$ is also closed.

        \item The \keyword{length} of a derivation $D = (R_1, \ldots, R_n)$ is $n$, and this is denoted $\derlen{D}$. If $D'$ is a subderivation of $D$, then $\derlen{D'} \leq \derlen{D}$ with equality if and only if $D' = D$. If $E$ is another derivation, then $\derlen{D \circ E} = \derlen{D} + \derlen{E}$.
    \end{enumrem}
\end{remark}


\begin{proposition}
    \label{prop:fixpoint-iff-derivation}
    If $F$ is represented by $\calR$ and $y \in X$, then $y \in \mu F$ if and only if $y$ is the final conclusion of some closed derivation from $\calR$.
\end{proposition}

\begin{proof}
    First assume that $y \in \mu F$, and recall that $\mu F = \bigdjoin_{n \in \naturals} F^n(\emptyset) = \bigunion_{n \in \naturals} F^n(\emptyset)$. We prove by induction in $n$ that every element in $F^n(\emptyset)$ is the final conclusion of a closed derivation. The base case $y \in F^0(\emptyset) = \emptyset$ is vacuous, so let $n \in \naturals$ and assume that the claim holds for every element of $F^n(\emptyset)$. If $y \in F^{n+1}(\emptyset) = F(F^n(\emptyset))$, then there is an inference rule $R$ with consequent $y$ and antecedents $x_1, \ldots, x_k$ that lie in $F^n(\emptyset)$. By induction each $x_i$ is the final conclusion of some derivation $D_i$, and by \cref{enum:derivation-composition} the derivation $D_1 \circ \cdots \circ D_k \circ (R)$ is a closed derivation, and $y$ is its final conclusion.

    % For a general $F$ we apply the above to $\overline{F}$ to obtain a derivation $D$ of $y \in \mu \overline{F}$ from $\overline{\calR}$. We then delete all occurrences of the rules $\infrule{ID}_x$ from $D$, and the result is clearly a derivation of $y$ from $\calR$. Since $\mu F = \mu \overline{F}$ by TODO ref, the first implication follows.

    We prove the converse by (strong) induction on the length of a derivation. If $y$ is the final conclusion of a closed derivation $(R)$ of length $1$, then $R$ must be an axiom. But then $y \in F(\emptyset) \subseteq \mu F$ since $F$ is represented by $\calR$. Next, let $n \in \naturals$ and let $y$ be the final conclusion of a closed derivation $D = (R_1, \ldots, R_{n+1})$. If $x$ is an antecedent of $R_{n+1}$, then by \cref{enum:derivation-antecedent} $D$ must have a strict subderivation $D'$ whose final conclusion is $x$. By \cref{enum:subderivation} $D'$ is also closed, and so $x \in \mu F$ by induction. But then we must have $y \in F(\mu F) = \mu F$ as desired.
\end{proof}
%
It is of course standard to use derivation \emph{trees}, but all that matters is that there is a finite way to obtain elements in $\mu F$. Since it is easier to define and reason about linear derivations -- and since we will not have to do any actual derivations -- we have taken this approach here.\footnote{Compare the role of deductive calculi in first-order logic, where e.g. the compactness theorem can be obtained from the \emph{existence} of a deductive calculus (with finite derivations).} The main consequence we shall use is the following:

\begin{corollary}[Inversion]
    If $F$ is represented by $\calR$ and $y \in \mu F$, then there is a rule $R$ in $\calR$ with $y$ as consequent whose antecedents also lie in $\mu F$.
\end{corollary}

\begin{proof}
    By \cref{prop:fixpoint-iff-derivation} there is a closed derivation $D$ of $y$ from $\calR$, and we denote its last rule by $R$. If $x$ is an antecedent of $R$, then some subderivation of $D$ is a derivation of $x$ by \cref{enum:subderivation}. Another application of \cref{prop:fixpoint-iff-derivation} then implies that $x \in \mu F$.
\end{proof}




\end{document}